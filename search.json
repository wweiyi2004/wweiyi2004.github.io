[{"title":"【学习笔记】最优化方法","url":"/2025/08/15/【学习笔记】最优化方法/","content":"\n# 【学习笔记】最优化方法\n\n###  Armijo型非精确线性搜索求步长\n\n**Armijo 型线性搜索**的数学条件定义为：\n$$\nf(x_k + \\alpha_k d_k) \\leq f(x_k) + \\rho \\alpha_k \\nabla f(x_k)^T d_k\n$$\n\n**参数说明**：\n- $\\rho$ 是预定义常数，且 $\\rho \\in (0,1)$\n- $\\alpha_k$ 表示第 $k$ 次迭代的步长\n- $d_k$ 为搜索方向\n- $\\nabla f(x_k)$ 是目标函数在 $x_k$ 处的梯度\n\n**公式作用**：\n该条件通过限制步长 $\\alpha_k$ 的取值，确保每次迭代中函数值 $f(x)$ 充分下降，常用于梯度下降法等优化算法的步长选择。\n\n**算法2.3（Armijo型线性搜索）步骤**：\n\n**步0**：若 $\\alpha_k=1$ 满足上述数学条件，则取 $\\alpha_k=1$；否则转**步1**  \n**步1**：给定常数 $\\beta>0$，$\\rho \\in (0,1)$，令 $\\alpha_k=\\beta$  \n**步2**：若 $\\alpha_k$ 满足条件（2.6），则得到步长 $\\alpha_k$，终止计算；否则转**步3**  \n**步3**：令 $\\alpha_k=\\rho\\alpha_k$，转**步2**\n\n---\n\n**参数说明**：\n- $\\beta$：初始试探步长（通常取 $\\beta=1$）\n- $\\rho$：步长缩减因子（控制步长收缩速度）\n- $\\alpha_k$：第 $k$ 次迭代的候选步长\n\n**算法注释**：\n1. 先尝试 $\\alpha_k=1$，若满足下降条件（2.6），则直接采用单位步长\n2. 若不满足，则从较大步长 $\\beta$ 开始，通过循环逐步缩小步长（$\\alpha_k \\leftarrow \\rho\\alpha_k$）\n3. 适用于梯度下降法、拟牛顿法等优化算法，保证迭代过程中目标函数值 $f(x_k)$ 充分下降\n\n```python\nimport numpy as np\n\ndef armijo_line_search(f, grad_f, x_k, d_k, beta=1, rho=0.5):\n    \"\"\"\n    严格按照算法2.3实现的Armijo型线性搜索\n    :param f: 目标函数\n    :param grad_f: 目标函数的梯度函数\n    :param x_k: 当前迭代点\n    :param d_k: 当前搜索方向\n    :param beta: 初始试探步长（默认值 1）\n    :param rho: 步长缩减因子（默认值 0.5，范围 (0,1)）\n    :return: 选定的步长 alpha_k\n    \"\"\"\n    # 计算当前点的梯度\n    grad_fk = grad_f(x_k)\n    \n    # 步0：先尝试 α_k = 1\n    alpha_k = 1\n    if f(x_k + alpha_k * d_k) <= f(x_k) + rho * alpha_k * np.dot(grad_fk, d_k):\n        return alpha_k  # 满足 Armijo 条件，直接返回 1\n    \n    # 步1：否则使用 beta 作为初始步长\n    alpha_k = beta\n    \n    # 步2 & 步3：逐步缩小步长，直到满足 Armijo 条件\n    while f(x_k + alpha_k * d_k) > f(x_k) + rho * alpha_k * np.dot(grad_fk, d_k):\n        alpha_k *= rho  # 步长缩小\n    \n    return alpha_k\n\n# 示例目标函数和梯度\ndef f(x):\n    return np.sum(x ** 2)  # 二次函数 f(x) = x^T x\n\ndef grad_f(x):\n    return 2 * x  # 其梯度为 ∇f(x) = 2x\n\n# 测试算法\nx_k = np.array([1.0, 2.0])  # 初始点\nd_k = -grad_f(x_k)  # 负梯度方向（梯度下降法）\nalpha = armijo_line_search(f, grad_f, x_k, d_k)\nprint(f\"选定步长: {alpha}\")\n\n```\n\n### Wolfe-Powell非精确线性搜索求步长\n\n**Wolfe-Powell 线性搜索**的数学条件定义为：\n\n1. **Armijo 条件**（充分下降条件）：\n$$\nf(x_k + \\alpha_k d_k) \\leq f(x_k) + \\rho \\alpha_k \\nabla f(x_k)^T d_k\n$$\n\n2. **曲率条件**：\n$$\n\\nabla f(x_k + \\alpha_k d_k)^T d_k \\geq \\sigma \\nabla f(x_k)^T d_k\n$$\n\n**参数约束**：\n$$ 0 < \\rho < \\sigma < 1 $$\n\n---\n\n**参数说明**：\n\n- $\\rho$：控制下降量的常数（通常取 $\\rho=10^{-4}$）\n- $\\sigma$：曲率条件的松弛系数（通常取 $\\sigma=0.1$ 或 $0.9$）\n- $\\alpha_k$：第 $k$ 次迭代的候选步长\n- $d_k$：搜索方向（如梯度下降方向 $d_k = -\\nabla f(x_k)$）\n- $\\nabla f(x_k)$：目标函数在 $x_k$ 处的梯度\n\n---\n\n**公式作用**：\n1. **Armijo 条件**：确保步长 $\\alpha_k$ 使函数值 $f(x_k)$ 充分下降  \n2. **曲率条件**：保证步长 $\\alpha_k$ 不会过小（避免算法停滞），同时允许梯度模长适当增大  \n3. **参数约束**：$\\rho < \\sigma$ 保证存在满足两条件的步长\n\n---\n\n**补充说明**：\n\n- 与 Armijo 准则相比，Wolfe-Powell 增加了曲率条件，避免步长选取过于保守  \n- 广泛应用于共轭梯度法、拟牛顿法等需要保证“充分下降且合理步长”的优化算法  \n- 条件中梯度内积 $\\nabla f(x_k)^T d_k$ 通常为负数（当 $d_k$ 是下降方向时）\n\n---\n\n**算法步骤**（修正后规范表达）：\n1. **步0**：若 $\\alpha_k=1$ 满足条件 (2.7)，则取 $\\alpha_k=1$；否则转**步1**  \n   - *作用*：优先尝试单位步长，若满足条件则直接采用\n\n2. **步1**：给定参数 $\\beta>0$，$\\rho,\\rho_1 \\in (0,1)$，初始化 $\\alpha_k^{(0)}$ 为集合 $\\{ \\beta \\rho^j \\ | \\ j \\in \\mathbb{Z} \\}$ 中满足第一个不等式的最大候选步长，令 $i=0$  \n   - *数学意义*：在离散化网格 $\\beta \\rho^j$ 中搜索满足条件的最大步长\n\n3. **步2**：若 $\\alpha_k^{(i)}$ 满足第二个条件，则取 $\\alpha_k = \\alpha_k^{(i)}$；否则令 $\\beta_k^{(i)} = \\rho^{-1} \\alpha_k^{(i)}$  \n   - *逻辑说明*：若不满足条件，则放大搜索范围上限（$\\beta_k^{(i)}$ 是原步长的 $\\rho^{-1}$ 倍）\n\n4. **步3**：在区间 $[\\alpha_k^{(i)}, \\beta_k^{(i)}]$ 内，通过集合 $\\{ \\alpha_k^{(i)} + \\rho_1^j (\\beta_k^{(i)} - \\alpha_k^{(i)}) \\ | \\ j=0,1,2,... \\}$ 重新搜索满足第一个不等式的最大步长 $\\alpha_k^{(i+1)}$，令 $i=i+1$，转**步2**  \n   - *几何解释*：在调整后的步长区间内，以 $\\rho_1$ 为细分因子进行二次搜索\n\n---\n\n**参数说明**：\n- $\\beta$：初始试探步长基准值（控制搜索范围）\n- $\\rho$：步长网格收缩因子（用于生成离散候选步长 $\\beta \\rho^j$）\n- $\\rho_1$：区间细分因子（控制二次搜索的精细程度）\n- $\\alpha_k^{(i)}$：第 $i$ 次迭代时的候选步长\n\n---\n\n**解题思路与算法逻辑**：\n1. **核心目标**：通过动态调整步长区间，高效找到同时满足两个条件 (2.7) 的 $\\alpha_k$\n2. **创新点**：\n   - **双重网格搜索**：先用 $\\rho$ 生成粗粒度候选步长，再用 $\\rho_1$ 在调整后的区间内细粒度搜索\n   - **区间动态扩展**：当候选步长不满足条件时，通过 $\\beta_k^{(i)} = \\rho^{-1}\\alpha_k^{(i)}$ 扩大搜索范围\n3. **收敛保证**：$\\rho, \\rho_1 < 1$ 确保步长在有限次迭代内缩小到满足条件\n\n---\n\n**补充说明**：\n1. 条件 (2.7) 应包含两个不等式（推测为类似 Wolfe-Powell 条件的 Armijo 条件和曲率条件）\n2. 该算法比标准 Armijo 搜索更复杂，适用于需要同时满足多个条件的优化问题\n3. 离散化集合 $\\{ \\beta \\rho^j \\}$ 和 $\\{ \\alpha_k^{(i)} + \\rho_1^j (\\beta_k^{(i)} - \\alpha_k^{(i)}) \\}$ 的设计平衡了计算效率与精度\n\n```python\nimport numpy as np\n\ndef wolfe_powell_search(f, grad_f, x_k, d_k, beta=1.0, rho=1e-4, sigma=0.1, rho1=0.5):\n    \"\"\"\n    Wolfe-Powell 线性搜索算法\n    :param f: 目标函数 f(x)\n    :param grad_f: 目标函数的梯度函数 ∇f(x)\n    :param x_k: 当前点 x_k\n    :param d_k: 搜索方向 d_k\n    :param beta: 初始试探步长\n    :param rho: Armijo 条件控制参数（0 < rho < 1）\n    :param sigma: 曲率条件控制参数（rho < sigma < 1）\n    :param rho1: 区间细分因子（0 < rho1 < 1）\n    :return: 满足 Wolfe-Powell 条件的步长 α_k\n    \"\"\"\n    \n    # 计算初始梯度内积 g_k_d = ∇f(x_k)^T d_k\n    g_k = grad_f(x_k)  # 计算梯度\n    g_k_d = np.dot(g_k, d_k)  # 梯度方向上的导数\n    \n    if g_k_d >= 0:\n        raise ValueError(\"搜索方向 d_k 不是下降方向，应满足 ∇f(x_k)^T d_k < 0\")\n    \n    # 先尝试 α_k = 1\n    alpha_k = 1.0\n    x_new = x_k + alpha_k * d_k  # 计算新点\n    \n    # 判断是否满足 Armijo 条件\n    if f(x_new) <= f(x_k) + rho * alpha_k * g_k_d:\n        # 判断是否满足曲率条件\n        if np.dot(grad_f(x_new), d_k) >= sigma * g_k_d:\n            return alpha_k  # 满足条件，直接返回\n    \n    # 步1：初始化 α_k^(0) 在集合 {βρ^j} 中满足 Armijo 条件的最大值\n    alpha_k = beta\n    while f(x_k + alpha_k * d_k) > f(x_k) + rho * alpha_k * g_k_d:\n        alpha_k *= rho  # 缩小步长\n    \n    # 步2：如果 α_k^(0) 也满足曲率条件，则返回\n    if np.dot(grad_f(x_k + alpha_k * d_k), d_k) >= sigma * g_k_d:\n        return alpha_k\n    \n    # 步3：设定 β_k^(0) 并进行区间搜索\n    beta_k = alpha_k / rho  # 设定上界\n    i = 0  # 迭代计数\n    while True:\n        # 在 [α_k^(i), β_k^(i)] 之间搜索满足 Armijo 条件的最大步长\n        alpha_new = alpha_k + rho1 ** i * (beta_k - alpha_k)\n        if f(x_k + alpha_new * d_k) <= f(x_k) + rho * alpha_new * g_k_d:\n            alpha_k = alpha_new\n        else:\n            beta_k = alpha_new\n        \n        # 检查是否满足曲率条件\n        if np.dot(grad_f(x_k + alpha_k * d_k), d_k) >= sigma * g_k_d:\n            return alpha_k\n        \n        i += 1  # 继续搜索\n\n```\n\n### 黄金分割法精确线性搜索求步长\n\n**输入**：初始区间 $[a_1, b_1]$，精度要求 $\\varepsilon > 0$  \n**输出**：极小值点所在区间 $[a_k, b_k]$\n\n1. **初始化**：\n   - 计算初始内点：\n     $$ \\lambda_1 = a_1 + 0.382(b_1 - a_1) $$\n     $$ \\mu_1 = a_1 + 0.618(b_1 - a_1) $$\n   - 计算函数值 $f(\\lambda_1)$ 和 $f(\\mu_1)$，令 $k=1$\n\n2. **停止条件**：\n   - 若 $b_k - a_k < \\varepsilon$，终止计算，输出区间 $[a_k, b_k]$\n\n3. **区间更新**：\n   - **情况1**：若 $f(\\lambda_k) > f(\\mu_k)$  \n     - 更新左端点：$a_{k+1} = \\lambda_k$，$b_{k+1} = b_k$  \n     - 新内点：$\\mu_{k+1} = a_{k+1} + 0.618(b_{k+1} - a_{k+1})$  \n     - 保留原右内点：$\\lambda_{k+1} = \\mu_k$  \n     - 仅需计算 $f(\\mu_{k+1})$\n\n   - **情况2**：若 $f(\\lambda_k) \\leq f(\\mu_k)$  \n     - 更新右端点：$a_{k+1} = a_k$，$b_{k+1} = \\mu_k$  \n     - 新内点：$\\lambda_{k+1} = a_{k+1} + 0.382(b_{k+1} - a_{k+1})$  \n     - 保留原左内点：$\\mu_{k+1} = \\lambda_k$  \n     - 仅需计算 $f(\\lambda_{k+1})$\n\n4. **迭代循环**：\n   - 令 $k = k+1$，返回步骤2\n\n---\n\n**参数说明**：\n\n- $\\lambda_k, \\mu_k$：第 $k$ 次迭代的黄金分割点（比例系数 0.382 和 0.618）  \n- $\\varepsilon$：区间长度精度阈值（控制算法终止条件）  \n- $[a_k, b_k]$：第 $k$ 次迭代时的候选区间\n\n---\n\n**算法原理**：\n\n1. **黄金分割比**：0.618 是 $\\frac{\\sqrt{5}-1}{2}$ 的近似值，确保每次迭代区间长度按固定比例 $0.618$ 收缩  \n2. **单点复用**：在区间更新时保留一个旧内点，只需计算一个新点函数值，减少计算量  \n3. **单调收敛性**：每次迭代排除不可能包含极小值的子区间，保证 $b_k - a_k \\to 0$\n\n---\n\n**解题思路**：\n\n1. **初始区间选择**：需满足 $f(x)$ 在 $[a_1, b_1]$ 内为单峰函数  \n2. **关键步骤验证**：\n  \n   ```python\n   # 伪代码示例：判断如何更新区间\n   if f(lambda_k) > f(mu_k):\n       a_new = lambda_k      # 舍弃左区间\n       b_new = b_k\n   else:\n       a_new = a_k\n       b_new = mu_k          # 舍弃右区间\n\n```python\nimport numpy as np\n\ndef golden_section_search(f, a, b, epsilon=1e-5):\n    \"\"\"\n    黄金分割法求解极小值所在区间\n    :param f: 目标函数\n    :param a: 初始区间左端点\n    :param b: 初始区间右端点\n    :param epsilon: 精度要求，控制停止条件\n    :return: 极小值所在的区间 [a_k, b_k]\n    \"\"\"\n    phi = (np.sqrt(5) - 1) / 2  # 黄金比例 0.618\n    \n    # 计算初始内点\n    lambda_k = a + (1 - phi) * (b - a)\n    mu_k = a + phi * (b - a)\n    f_lambda = f(lambda_k)\n    f_mu = f(mu_k)\n    \n    while (b - a) > epsilon:\n        if f_lambda > f_mu:\n            # 情况1：f(λ_k) > f(μ_k)，收缩左侧区间\n            a = lambda_k\n            lambda_k = mu_k  # 保留原右内点\n            f_lambda = f_mu  # 复用已计算值\n            mu_k = a + phi * (b - a)  # 计算新点\n            f_mu = f(mu_k)\n        else:\n            # 情况2：f(λ_k) <= f(μ_k)，收缩右侧区间\n            b = mu_k\n            mu_k = lambda_k  # 保留原左内点\n            f_mu = f_lambda  # 复用已计算值\n            lambda_k = a + (1 - phi) * (b - a)  # 计算新点\n            f_lambda = f(lambda_k)\n    \n    return a, b  # 返回最终区间\n\n```\n\n### 信赖域算法\n\n**修正后的规范算法步骤**：\n\n**输入**：初始点 $x_0$，最大信赖域半径 $\\overline{\\Delta}$，精度阈值 $\\varepsilon \\geq 0$，阈值参数 $0 < \\eta_1 \\leq \\eta_2 < 1$  \n**输出**：优化解 $x_k$\n\n1. **初始化**：\n   - 设定初始信赖域半径 $\\Delta_0 \\in (0, \\overline{\\Delta})$，迭代计数器 $k=0$\n\n2. **停止条件**：\n   - 计算梯度 $g_k = \\nabla f(x_k)$\n   - 若 $\\|g_k\\| \\leq \\varepsilon$，终止算法，输出 $x_k$\n\n3. **子问题求解**：\n   - 在信赖域 $\\|s\\| \\leq \\Delta_k$ 内，近似求解：\n     $$ \\min_{s} m_k(s) = f(x_k) + g_k^T s + \\frac{1}{2}s^T B_k s $$\n     得到试探步 $s_k$，其中 $B_k$ 是Hessian矩阵或其近似\n\n4. **接受步长**：\n   - 计算实际下降量 $\\delta f = f(x_k) - f(x_k + s_k)$\n   - 计算预测下降量 $\\delta m = m_k(0) - m_k(s_k)$\n   - 计算比值：\n     $$ r_k = \\frac{\\delta f}{\\delta m} $$\n   - 更新迭代点：\n     $$ x_{k+1} = \\begin{cases}\n     x_k + s_k, & \\text{if } r_k > 0 \\\\\n     x_k, & \\text{otherwise}\n     \\end{cases} $$\n\n5. **调整信赖域半径**：\n   $$ \\Delta_{k+1} = \\begin{cases}\n   \\min(2\\Delta_k, \\overline{\\Delta}), & \\text{if } r_k > \\eta_2 \\\\\n   \\Delta_k/4, & \\text{if } r_k < \\eta_1 \\\\\n   \\Delta_k, & \\text{otherwise}\n   \\end{cases} $$\n   \n6. **循环迭代**：\n   - 令 $k = k+1$，返回步骤2\n\n---\n\n**参数说明**：\n\n- $\\overline{\\Delta}$：信赖域半径的最大允许值（防止过度扩张）  \n- $\\eta_1, \\eta_2$：阈值参数，控制半径调整的敏感性（典型值：$\\eta_1=0.1$, $\\eta_2=0.75$）  \n- $B_k$：二阶模型中的Hessian近似矩阵（如BFGS更新、SR1更新等）  \n- $r_k$：实际下降与预测下降的比值（衡量模型可靠性）\n\n---\n\n**算法原理**：\n\n1. **信赖域框架**：通过局部二次模型 $m_k(s)$ 近似目标函数，在半径 $\\Delta_k$ 内寻找最优步长  \n2. **半径动态调整**：\n   - $r_k$ 接近 1：扩大信赖域（模型可信度高）\n   - $r_k$ 接近 0：缩小信赖域（模型不可靠）\n3. **子问题求解**：常用Steihaug-CG方法或狗腿法(Dogleg)近似求解\n4. **全局收敛性**：在适当条件下，算法满足 $\\liminf_{k\\to\\infty} \\|g_k\\| = 0$\n\n---\n\n**解题思路**：\n\n1. **关键公式验证**：\n  \n   ```python\n   # 伪代码示例：计算接受率\n   actual_reduction = f(xk) - f(xk + sk)\n   predicted_reduction = - (g_k.T @ sk + 0.5 * sk.T @ B_k @ sk)\n   rk = actual_reduction / predicted_reduction\n\n```python\nimport numpy as np\n\ndef trust_region_method(f, grad_f, hess_f, x0, delta_max, epsilon=1e-5, eta1=0.1, eta2=0.75):\n    \"\"\"\n    信赖域优化算法\n    :param f: 目标函数\n    :param grad_f: 目标函数梯度函数 ∇f(x)\n    :param hess_f: 目标函数 Hessian 矩阵函数 B_k\n    :param x0: 初始点 x_0\n    :param delta_max: 最大信赖域半径 \\overline{\\Delta}\n    :param epsilon: 终止条件，梯度范数阈值\n    :param eta1: 阈值参数 η_1\n    :param eta2: 阈值参数 η_2\n    :return: 优化解 x_k\n    \"\"\"\n    x_k = np.array(x0, dtype=float)  # 设定初始点，确保数据类型兼容计算\n    delta_k = delta_max / 2  # 设定初始信赖域半径\n    \n    while True:\n        g_k = grad_f(x_k)  # 计算梯度\n        if np.linalg.norm(g_k) <= epsilon:\n            break  # 终止条件\n        \n        B_k = hess_f(x_k)  # 计算 Hessian 矩阵\n        \n        # 处理 Hessian 矩阵奇异性，确保可逆\n        try:\n            s_k = -np.linalg.solve(B_k, g_k)  # 计算 Newton 步长\n        except np.linalg.LinAlgError:\n            s_k = -g_k  # 退化为梯度下降\n        \n        # 限制步长在信赖域半径内\n        s_norm = np.linalg.norm(s_k)\n        if s_norm > delta_k:\n            s_k = (delta_k / s_norm) * s_k\n        \n        # 计算实际下降量\n        delta_f = f(x_k) - f(x_k + s_k)\n        \n        # 计算预测下降量\n        delta_m = g_k.T @ s_k + 0.5 * s_k.T @ B_k @ s_k\n        \n        # 计算 r_k\n        r_k = delta_f / delta_m if delta_m != 0 else 0\n        \n        # 更新 x_k\n        if r_k > 0:\n            x_k = x_k + s_k\n        \n        # 更新信赖域半径\n        if r_k > eta2:\n            delta_k = min(2 * delta_k, delta_max)\n        elif r_k < eta1:\n            delta_k = delta_k / 4\n        \n    return x_k\n\n```\n\n### 最速下降法\n\n**输入**：初始点 $ x_0 \\in \\mathbb{R}^n $，精度阈值 $ \\epsilon > 0 $  \n**输出**：优化解 $ x_k $\n\n1. **初始化**：\n   - 设定 $ k = 0 $，给定初始点 $ x_0 $\n\n2. **停止条件**：\n   - 计算梯度 $ \\nabla f(x_k) $\n   - 若 $ \\|\\nabla f(x_k)\\| \\leq \\epsilon $，终止算法，输出 $ x_k $\n\n3. **方向计算**：\n   - 取搜索方向为负梯度方向：\n     $$ d_k = -\\nabla f(x_k) $$\n\n4. **步长计算**：\n   - 通过一维精确线搜索求解：\n     $$ \\alpha_k = \\arg\\min_{\\alpha > 0} f(x_k + \\alpha d_k) $$\n\n5. **迭代更新**：\n   - 更新迭代点：\n     $$ x_{k+1} = x_k + \\alpha_k d_k $$\n   - 令 $ k = k + 1 $，返回步骤2\n\n---\n\n**关键点解析**：\n\n1. **核心思想**：\n   - 沿目标函数下降最快的方向（负梯度方向）更新迭代点\n   - 通过精确线搜索保证每步下降量最大\n\n2. **方向选择**：\n   - 方向 $ d_k = -\\nabla f(x_k) $ 是当前点的最速下降方向\n   - 梯度方向是函数值局部上升最快的方向，负梯度则为下降最快方向\n\n3. **步长计算**：\n   - 精确线搜索需解一维优化问题，对简单函数可解析求导（如二次函数）\n   - 一般函数可能需要黄金分割法、抛物线插值法等数值方法\n\n---\n\n**收敛性分析**：\n\n1. **收敛条件**：\n   - 在目标函数 $ f(x) $ 为凸且光滑的条件下，算法全局收敛\n   - 线性收敛速度：相邻迭代点误差满足 $ \\|x_{k+1} - x^*\\| \\leq \\rho \\|x_k - x^*\\| $，其中 $ 0 < \\rho < 1 $\n\n2. **局限性**：\n   - 高维问题中可能出现“锯齿现象”，收敛速度显著下降\n   - 对非凸函数可能收敛到局部极小或鞍点\n\n---\n\n**解题思路**（以二次函数为例）：\n\n设目标函数为 $ f(x) = \\frac{1}{2}x^T Q x + b^T x $，其中 $ Q $ 对称正定  \n1. **梯度计算**：\n   $$ \\nabla f(x) = Qx + b $$\n2. **方向更新**：\n   $$ d_k = - (Qx_k + b) $$\n3. **精确步长解析解**：\n   $$ \\alpha_k = \\frac{\\nabla f(x_k)^T \\nabla f(x_k)}{\\nabla f(x_k)^T Q \\nabla f(x_k)} $$\n\n**示例**（二维情况）：\n- 令 $ Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix} $, $ b = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} $, $ x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $\n- 第1次迭代：\n  - $ \\nabla f(x_0) = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} $\n  - $ d_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $\n  - $ \\alpha_0 = \\frac{1}{2} $\n  - $ x_1 = \\begin{bmatrix} 0.5 \\\\ 0 \\end{bmatrix} $\n\n---\n\n**对比与扩展**：\n\n1. **与信赖域算法区别**：\n   - 最速下降法仅依赖梯度，而信赖域法需构建二阶模型\n   - 最速下降法步长由线搜索确定，信赖域法步长受半径约束\n\n2. **改进方向**：\n   - **预处理技术**：通过坐标变换减少条件数，缓解锯齿现象\n   - **非精确线搜索**：使用Armijo准则、Wolfe条件降低计算成本\n\n```python\nimport numpy as np\n\ndef steepest_descent(f, grad_f, x0, epsilon=1e-5, max_iter=1000):\n    \"\"\"\n    最速下降法优化算法\n    :param f: 目标函数\n    :param grad_f: 目标函数的梯度函数 ∇f(x)\n    :param x0: 初始点 x_0\n    :param epsilon: 精度阈值\n    :param max_iter: 最大迭代次数\n    :return: 近似最优解 x_k\n    \"\"\"\n    x_k = np.array(x0, dtype=float)  # 确保初始点为 NumPy 数组\n    k = 0  # 迭代计数\n    \n    while k < max_iter:\n        g_k = grad_f(x_k)  # 计算梯度\n        if np.linalg.norm(g_k) <= epsilon:\n            break  # 满足停止条件，返回结果\n        \n        # 计算步长 α_k 通过精确线搜索（以二次函数为例）\n        d_k = -g_k  # 负梯度方向\n        alpha_k = (g_k.T @ g_k) / (g_k.T @ hess_f(x_k) @ g_k)  # 解析步长计算\n        \n        # 更新 x_k\n        x_k = x_k + alpha_k * d_k\n        k += 1\n    \n    return x_k\n\n```\n\n### 基本Newton法\n\n**输入**：初始点 $ x_0 \\in \\mathbb{R}^n $，精度阈值 $ \\varepsilon > 0 $  \n**输出**：优化解 $ x_k $\n\n1. **初始化**：\n   - 设定 $ k = 0 $，给定初始点 $ x_0 $\n\n2. **停止条件**：\n   - 计算梯度 $ \\nabla f(x_k) $\n   - 若 $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $，终止算法，输出 $ x_k $\n\n3. **方向计算**：\n   - 解线性方程组：\n     $$ \\nabla^2 f(x_k) d_k + \\nabla f(x_k) = 0 \\quad \\text{(公式3.3)} $$\n   - 得到搜索方向 $ d_k $\n\n4. **迭代更新**：\n   - 更新迭代点：\n     $$ x_{k+1} = x_k + d_k $$\n   - 令 $ k = k + 1 $，返回步骤2\n\n---\n\n**关键点解析**：\n\n1. **核心思想**：\n   - 利用目标函数的二阶导数（Hessian矩阵）构造搜索方向，实现局部快速收敛\n   - 通过牛顿方程（公式3.3）直接求解优化方向，默认步长 $ \\alpha_k = 1 $\n\n2. **方向计算**：\n   - 方向 $ d_k $ 是牛顿方向，满足 $ d_k = -[\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k) $\n   - 当Hessian矩阵正定时，方向为下降方向\n\n3. **步长特性**：\n   - 基本Newton法固定步长为1，适用于靠近极值点的情况\n   - 改进方法（如阻尼Newton法）会引入线搜索调整步长\n\n---\n\n**收敛性分析**：\n\n1. **收敛速度**：\n   - 局部二次收敛：若初始点 $ x_0 $ 充分接近极小点且Hessian连续且正定，则\n     $$ \\|x_{k+1} - x^*\\| \\leq C \\|x_k - x^*\\|^2 $$\n   - 相比最速下降法（线性收敛），收敛速度显著提升\n\n2. **局限性**：\n   - Hessian矩阵可能非正定或奇异，导致方向非下降或无法求解\n   - 初始点选择敏感，远离极值点时可能发散\n\n---\n\n**解题示例**（二次函数优化）：\n\n设目标函数为 $ f(x) = \\frac{1}{2}x^T Q x + b^T x $，其中 $ Q $ 对称正定  \n1. **梯度与Hessian**：\n   $$ \\nabla f(x) = Qx + b, \\quad \\nabla^2 f(x) = Q $$\n2. **牛顿方向计算**：\n   $$ Q d_k + (Qx_k + b) = 0 \\Rightarrow d_k = -x_k - Q^{-1}b $$\n3. **迭代更新**：\n   $$ x_{k+1} = x_k + d_k = -Q^{-1}b \\quad \\text{（一步收敛到极小点）} $$\n\n**数值算例**：\n- 令 $ Q = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $, $ b = \\begin{bmatrix} -3 \\\\ -3 \\end{bmatrix} $, $ x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $\n- 第1次迭代：\n  - $ \\nabla f(x_0) = \\begin{bmatrix} -3 \\\\ -3 \\end{bmatrix} $\n  - 解方程 $ Q d_0 = -\\nabla f(x_0) \\Rightarrow d_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $\n  - $ x_1 = x_0 + d_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $（精确解）\n\n---\n\n```python\nimport numpy as np\n\ndef newton_method(f_grad, f_hessian, x0, epsilon):\n    \"\"\"\n    使用牛顿法（Newton's Method）进行优化求解。\n    :param f_grad: 目标函数的梯度函数，输入 x 返回梯度向量。\n    :param f_hessian: 目标函数的 Hessian 矩阵函数，输入 x 返回 Hessian 矩阵。\n    :param x0: 初始点，numpy 数组。\n    :param epsilon: 终止精度阈值。\n    :return: 近似最优解 x_k。\n    \"\"\"\n    k = 0  # 迭代计数\n    x_k = x0  # 初始化 x_k\n    \n    while True:\n        grad = f_grad(x_k)  # 计算梯度 ∇f(x_k)\n        if np.linalg.norm(grad) <= epsilon:  # 停止条件\n            break\n        \n        hessian = f_hessian(x_k)  # 计算 Hessian 矩阵 ∇²f(x_k)\n        \n        # 求解线性方程组 ∇²f(x_k) d_k = -∇f(x_k)\n        d_k = np.linalg.solve(hessian, -grad)\n        \n        # 更新迭代点\n        x_k = x_k + d_k\n        k += 1\n    \n    return x_k\n```\n\n### 阻尼Newton法\n\n**算法（阻尼Newton法）**  \n**输入**：初始点 $ x_0 \\in \\mathbb{R}^n $，精度阈值 $ \\varepsilon > 0 $  \n**输出**：优化解 $ x_k $\n\n1. **初始化**：\n   - 设定 $ k = 0 $，给定初始点 $ x_0 $\n\n2. **停止条件**：\n   - 计算梯度 $ \\nabla f(x_k) $\n   - 若 $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $，终止算法，输出 $ x_k $\n\n3. **方向计算**：\n   - 解线性方程组：\n     $$ \\nabla^2 f(x_k) d_k + \\nabla f(x_k) = 0 $$\n   - 得到搜索方向 $ d_k $\n\n4. **步长计算**：\n   - 通过线搜索（如Armijo准则、Wolfe条件）确定步长 $ \\alpha_k $，确保目标函数值下降\n\n5. **迭代更新**：\n   - 更新迭代点：\n     $$ x_{k+1} = x_k + \\alpha_k d_k $$\n   - 令 $ k = k + 1 $，返回步骤2\n\n---\n\n**关键点解析**：\n\n1. **核心改进**：\n   - 相比基本Newton法（固定步长 $ \\alpha_k = 1 $），阻尼法引入线搜索动态调整步长，提升全局收敛性。\n   - 适用于初始点远离极值点或Hessian矩阵非正定的情况。\n\n2. **方向与步长的协同**：\n   - **方向**：仍为牛顿方向 $ d_k = -[\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k) $，需Hessian正定以保证下降性。\n   - **步长**：通过线搜索找到 $ \\alpha_k \\in (0, 1] $，使 $ f(x_k + \\alpha_k d_k) < f(x_k) $。\n\n3. **适用场景**：\n   - 目标函数局部凸性较弱时，步长调整可避免迭代发散。\n   - Hessian矩阵条件数较差时（如接近奇异），阻尼法更稳定。\n\n---\n\n**对比基本Newton法**：\n\n| 特性             | 基本Newton法               | 阻尼Newton法                     |\n| ---------------- | -------------------------- | -------------------------------- |\n| **步长策略**     | 固定步长 $ \\alpha_k = 1 $  | 动态步长 $ \\alpha_k \\in (0, 1] $ |\n| **收敛性**       | 局部二次收敛               | 全局收敛（需合理线搜索）         |\n| **计算成本**     | 低（无需步长搜索）         | 较高（需多次函数值计算）         |\n| **初始点敏感性** | 高（依赖初始点接近极值点） | 低（适应性更强）                 |\n\n---\n\n**收敛性分析**：\n\n1. **全局收敛**：\n   - 在适当线搜索条件下（如Armijo准则），算法对任意初始点 $ x_0 $ 均收敛到局部极小点。\n2. **局部收敛速度**：\n   - 若初始点充分接近极小点且Hessian连续正定，仍保持局部二次收敛速度。\n\n---\n\n- **解题示例**（二次函数优化）：\n  \n  设目标函数 $ f(x) = \\frac{1}{2}x^T Q x + b^T x $，其中 $ Q $ 对称正定  \n  1. **梯度与Hessian**：\n     $$ \\nabla f(x) = Qx + b, \\quad \\nabla^2 f(x) = Q $$\n  2. **牛顿方向**：\n     $$ Q d_k + (Qx_k + b) = 0 \\Rightarrow d_k = -x_k - Q^{-1}b $$\n  3. **步长计算**：\n     - 二次函数下，线搜索直接得 $ \\alpha_k = 1 $（一步收敛）\n  4. **迭代更新**：\n     $$ x_{k+1} = x_k + d_k = -Q^{-1}b \\quad \\text{（精确极小点）} $$\n  \n  **数值算例**（同基本Newton法）：\n  \n  - $ Q = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $, $ b = \\begin{bmatrix} -3 \\\\ -3 \\end{bmatrix} $, $ x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $\n  - 第1次迭代：\n    - $ \\nabla f(x_0) = \\begin{bmatrix} -3 \\\\ -3 \\end{bmatrix} $\n    - 解方程 $ Q d_0 = -\\nabla f(x_0) \\Rightarrow d_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $\n    - 线搜索得 $ \\alpha_0 = 1 $，故 $ x_1 = x_0 + d_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $（精确解）\n  \n\n```python\nimport numpy as np\n\ndef newton_method(f, grad_f, hess_f, x0, epsilon=1e-6, max_iter=100):\n    \"\"\"\n    牛顿法优化算法\n    :param f: 目标函数\n    :param grad_f: 目标函数的梯度函数\n    :param hess_f: 目标函数的 Hessian 矩阵函数\n    :param x0: 初始点 (n 维向量)\n    :param epsilon: 收敛阈值\n    :param max_iter: 最大迭代次数\n    :return: 近似优化解 x_k\n    \"\"\"\n    x_k = np.array(x0, dtype=np.float64)  # 确保是浮点类型\n    k = 0  # 初始化迭代次数\n    \n    while k < max_iter:\n        grad = grad_f(x_k)  # 计算梯度 ∇f(x_k)\n        \n        # 检查终止条件 ||∇f(x_k)|| ≤ ε\n        if np.linalg.norm(grad) <= epsilon:\n            break\n        \n        hess = hess_f(x_k)  # 计算 Hessian 矩阵 ∇²f(x_k)\n        \n        # 解线性方程组 ∇²f(x_k) * d_k = -∇f(x_k)\n        try:\n            d_k = np.linalg.solve(hess, -grad)\n        except np.linalg.LinAlgError:\n            print(\"Hessian 矩阵不可逆，终止优化。\")\n            break\n        \n        # 线搜索确定步长 α_k（使用 Armijo 准则）\n        alpha = armijo_line_search(f, grad_f, x_k, d_k)\n        \n        # 迭代更新\n        x_k = x_k + alpha * d_k\n        k += 1\n    \n    return x_k\n\ndef armijo_line_search(f, grad_f, x_k, d_k, beta=1, rho=0.5):\n    \"\"\"\n    严格按照算法2.3实现的Armijo型线性搜索\n    :param f: 目标函数\n    :param grad_f: 目标函数的梯度函数\n    :param x_k: 当前迭代点\n    :param d_k: 当前搜索方向\n    :param beta: 初始试探步长（默认值 1）\n    :param rho: 步长缩减因子（默认值 0.5，范围 (0,1)）\n    :return: 选定的步长 alpha_k\n    \"\"\"\n    # 计算当前点的梯度\n    grad_fk = grad_f(x_k)\n    \n    # 步0：先尝试 α_k = 1\n    alpha_k = 1\n    if f(x_k + alpha_k * d_k) <= f(x_k) + rho * alpha_k * np.dot(grad_fk, d_k):\n        return alpha_k  # 满足 Armijo 条件，直接返回 1\n    \n    # 步1：否则使用 beta 作为初始步长\n    alpha_k = beta\n    \n    # 步2 & 步3：逐步缩小步长，直到满足 Armijo 条件\n    while f(x_k + alpha_k * d_k) > f(x_k) + rho * alpha_k * np.dot(grad_fk, d_k):\n        alpha_k *= rho  # 步长缩小\n    \n    return alpha_k\n\n```\n\n### Newton-最速下降混合法\n\n步骤1：给定初始点 $x_0 \\in \\mathbb{R}^n$，精度 $\\epsilon > 0$。令 $k = 0$；\n\n步骤2：若 $\\| \\nabla f(x_k) \\| \\leq \\epsilon$，则得解 $x_k$，算法终止。否则，解线性方程组  \n$$\n\\nabla^2 f(x_k)d + \\nabla f(x_k) = 0 \n$$\n若有解 $d_k$ 且满足 $\\nabla f(x_k)^\\top d_k < 0$，转步骤3；否则取 $d_k = -\\nabla f(x_k)$；\n\n步骤3：由线性搜索计算步长 $\\alpha_k$；\n\n步骤4：令 $x_{k+1} = x_k + \\alpha_k d_k$，$k := k + 1$，转步骤2。\n\n```python\nimport numpy as np\n\ndef newton_method(f, grad_f, hess_f, x0, epsilon=1e-6, max_iter=100):\n    \"\"\"\n    牛顿法优化算法\n    :param f: 目标函数\n    :param grad_f: 目标函数的梯度函数\n    :param hess_f: 目标函数的 Hessian 矩阵函数\n    :param x0: 初始点 (n 维向量)\n    :param epsilon: 收敛阈值\n    :param max_iter: 最大迭代次数\n    :return: 近似优化解 x_k\n    \"\"\"\n    x_k = np.array(x0, dtype=np.float64)  # 确保是浮点类型\n    k = 0  # 初始化迭代次数\n    \n    while k < max_iter:\n        grad = grad_f(x_k)  # 计算梯度 ∇f(x_k)\n        \n        # 检查终止条件 ||∇f(x_k)|| ≤ ε\n        if np.linalg.norm(grad) <= epsilon:\n            break\n        \n        hess = hess_f(x_k)  # 计算 Hessian 矩阵 ∇²f(x_k)\n        \n        # 解线性方程组 ∇²f(x_k) * d_k = -∇f(x_k)\n        try:\n            d_k = np.linalg.solve(hess, -grad)\n            if np.dot(grad, d_k) >= 0:\n                d_k = -grad  # 若解不满足下降方向，则取 d_k = -∇f(x_k)\n        except np.linalg.LinAlgError:\n            d_k = -grad  # 若 Hessian 不可逆，使用梯度下降方向\n        \n        # 线搜索确定步长 α_k（使用 Armijo 准则）\n        alpha = armijo_line_search(f, grad_f, x_k, d_k)\n        \n        # 迭代更新\n        x_k = x_k + alpha * d_k\n        k += 1\n    \n    return x_k\n\ndef armijo_line_search(f, grad_f, x_k, d_k, beta=1, rho=0.5):\n    \"\"\"\n    严格按照算法2.3实现的Armijo型线性搜索\n    :param f: 目标函数\n    :param grad_f: 目标函数的梯度函数\n    :param x_k: 当前迭代点\n    :param d_k: 当前搜索方向\n    :param beta: 初始试探步长（默认值 1）\n    :param rho: 步长缩减因子（默认值 0.5，范围 (0,1)）\n    :return: 选定的步长 alpha_k\n    \"\"\"\n    # 计算当前点的梯度\n    grad_fk = grad_f(x_k)\n    \n    # 步0：先尝试 α_k = 1\n    alpha_k = 1\n    if f(x_k + alpha_k * d_k) <= f(x_k) + rho * alpha_k * np.dot(grad_fk, d_k):\n        return alpha_k  # 满足 Armijo 条件，直接返回 1\n    \n    # 步1：否则使用 beta 作为初始步长\n    alpha_k = beta\n    \n    # 步2 & 步3：逐步缩小步长，直到满足 Armijo 条件\n    while f(x_k + alpha_k * d_k) > f(x_k) + rho * alpha_k * np.dot(grad_fk, d_k):\n        alpha_k *= rho  # 步长缩小\n    \n    return alpha_k\n\n```\n\n### 修正Newton-LM法\n\n**步骤1**  给定初始点 $ x_0 \\in \\mathbb{R}^n $，精度 $ \\varepsilon > 0 $。令 $ k = 0 $。\n\n**步骤2**  若 $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $，则得解 $ x_k $，算法终止；否则，解线性方程组  \n$$\nA_k d + \\nabla f(x_k) = 0 \\quad (3.3)\n$$\n得解 $ d_k $，其中 $ A_k = \\nabla^2 f(x_k) + v_k I $，$ v_k > 0 $ 使得 $ A_k $ 正定。\n\n**步骤3**  通过线性搜索计算步长 $ \\alpha_k $。\n\n**步骤4**  令 $ x_{k+1} = x_k + \\alpha_k d_k $，$ k := k + 1 $，转至步骤2。\n\n参数说明\n\n在修正Newton法中：\n1. 为确保 $ A_k $ 的正定性，需选择足够大的 $ v_k > 0 $；\n2. 为确保算法收敛性，需限制 $ v_k $ 的上界，即要求  \n   $$\n   v_k \\leq C \\|\\nabla f(x_k)\\|\n   $$\n   其中 $ C $ 为常数，但其具体确定较为困难。\n\n修正形式的收敛性\n\n- Newton法的两种修正形式**在较弱条件下**具有：\n  - 超线性收敛性\n  - 二次收敛性\n- 存在多种其他修正形式\n\n鞍点情况的特殊处理\n\n当 $ x $ 为鞍点时（即 $ \\nabla f(x) = 0 $ 但 $ \\nabla^2 f(x) $ 不定）：\n1. 所有修正方法均失效\n2. 可取负曲率方向 $ d $，需满足：\n   $$\n   d^T \\nabla^2 f(x) d < 0\n   $$\n   > 沿此方向搜索时，目标函数值必然下降\n\n方法特性对比\n\n| 优点       | 缺点           |\n| ---------- | -------------- |\n| 收敛速度快 | 对初始点要求高 |\n| -          | 计算量大       |\n\n算法改进方向\n\n通过修改Newton法：\n1. **保留优势**：利用快速收敛特性\n2. **克服缺陷**：降低对初始点的敏感性，减少计算量\n3. 已衍生出多种高效新算法\n\n### 拟Newton法\n\n**步骤1**  给定初始点 $ x_0 \\in \\mathbb{R}^n $，初始化矩阵 $ B_0 $ 或 $ H_0 $，精度阈值 $ \\varepsilon > 0 $。令迭代计数器 $ k = 0 $。\n\n**步骤2**  若满足终止准则 $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $，输出解 $ x_k $ 并终止算法。\n\n**步骤3**  求解线性方程组：\n$$\nB_k d + \\nabla f(x_k) = 0\n$$\n得搜索方向 $ d_k $，或通过逆矩阵公式直接计算：\n$$\nd_k = -H_k g_k \\quad (\\text{其中 } g_k = \\nabla f(x_k))\n$$\n\n**步骤4**  通过线搜索计算步长 $ \\alpha_k $，更新迭代点：\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n\n**步骤5**  按拟Newton条件（DFP/SRP1等）修正矩阵：\n$$\nB_{k+1} = B_k + \\Delta B \\quad \\text{或} \\quad H_{k+1} = H_k + \\Delta H\n$$\n使其满足拟Newton方程 $B_{k+1}s_k$ 或 $H_{k+1}y_k=s_k$ 。令 $ k := k + 1 $，返回步骤2。\n\n#### 修正矩阵\n\n##### 对称秩1\n\nSR修正公式形式1\n$$\nB_{k+1}=B_k+\\dfrac {(y_k-B_ks_k)(y_k-B_ks_k)^T} {(y_k-B_ks_k)^Ts_k}\n$$\n\n\nSR修正公式形式2\n$$\nH_{k+1}=H_k+ \\dfrac {(s_k-H_ky_k)(s_k-H_ky_k)^T} {(s_k-H_ky_k)^Ty_k}\n$$\n\n##### 对称秩2\n\n$BFGS$ 修正公式(迄今最好的拟牛顿修正公式)\n$$\nB_{k+1}=B_k-\\dfrac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}+\\dfrac {y_ky_k^T}{y_k^Ts_k}\n$$\n$BFGS$ 的逆修正公式\n$$\nH_{k+1}=(I-\\dfrac {s_ky_k^T}{y_k^Ts_k})H_k(I-\\dfrac {s_ky_k^T}{y_k^Ts_k})^T+\\dfrac {s_ks_k^T}{y_k^Ts_k}\n$$\n\n\n$DFP$ 修正公式\n$$\nH_{k+1}=H_k-\\dfrac {H_ky_ky_k^TH_k}{y_k^TH_ky_k}+\\dfrac {s_ks_k^T}{s_k^Ty_k}\n$$\n$DFP$ 的逆修正公式\n$$\nB_{k+1}=(I-\\dfrac {y_ks_k^T}{s_k^Ty_k})B_k(I-\\dfrac {y_ks_k^T}{s_k^Ty_k})+\\dfrac {y_ky_k^T}{s_k^Ty_k}\n$$\n\n\n### BFGS算法\n\n**步骤1**：给定初始点 $x_0 \\in \\mathbb{R}^n$，初始对称正定矩阵 $B_0$，精度 $\\epsilon > 0$。令 $k = 0$。\n\n**步骤2**：若 $\\| \\nabla f(x_k) \\| \\leq \\epsilon$，则得解 $x_k$，算法终止。否则转下一步。\n\n**步骤3**：解线性方程组\n$$B_k d + \\nabla f(x_k) = 0 \\qquad $$\n得解 $d_k$。\n\n**步骤4**：由线性搜索计算步长 $\\alpha_k$。\n\n**步骤5**：令 $x_{k+1} = x_k + \\alpha_k d_k$，若 $\\| \\nabla f(x_{k+1}) \\| \\leq \\epsilon$，则得解 $x_{k+1}$，算法终止。否则由公式 $B_{k+1}=B_k-\\dfrac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}+\\dfrac {y_ky_k^T}{y_k^Ts_k}$ 计算 $B_{k+1}$。\n\n**步骤6**：令 $k := k + 1$，转步骤3。\n\n```python\nimport numpy as np\n\ndef function(x):\n    \"\"\" 目标函数 f(x) \"\"\"\n    return np.sum(x**2)  # 这里使用 f(x) = x^T x 作为示例\n\ndef gradient(x):\n    \"\"\" 计算目标函数的梯度 \\nabla f(x) \"\"\"\n    return 2 * x  # 目标函数 f(x) = x^T x 的梯度为 2x\n\n#这里也可以用Wolfe-Powell搜索\ndef line_search(f, grad, xk, dk, alpha=1, rho=0.8, c=1e-4):\n    \"\"\"\n    线性搜索计算步长 \\alpha_k\n    使用 Armijo 条件进行搜索，确保下降足够快。\n    \"\"\"\n    while f(xk + alpha * dk) > f(xk) + c * alpha * np.dot(grad(xk).T, dk):\n        alpha *= rho  # 按比例缩小步长\n    return alpha\n\ndef bfgs(x0, B0, epsilon=1e-6, max_iter=100):\n    \"\"\" BFGS 算法实现 \"\"\"\n    xk = x0  # 初始点\n    Bk = B0  # 初始对称正定矩阵\n    k = 0\n    \n    while np.linalg.norm(gradient(xk)) > epsilon and k < max_iter:\n        # 计算搜索方向 d_k\n        dk = -np.linalg.solve(Bk, gradient(xk))\n        \n        # 线性搜索确定步长 alpha_k\n        alpha_k = line_search(function, gradient, xk, dk)\n        \n        # 更新 x\n        xk_new = xk + alpha_k * dk\n        \n        # 计算梯度变化 y_k 和步长向量 s_k\n        sk = xk_new - xk\n        yk = gradient(xk_new) - gradient(xk)\n        \n        # 终止条件检查\n        if np.linalg.norm(gradient(xk_new)) <= epsilon:\n            return xk_new\n        \n        # 计算 BFGS 公式中的矩阵更新项\n        Bk_sk = Bk @ sk\n        sk_Bk_sk = sk.T @ Bk_sk\n        yk_ykT = np.outer(yk, yk)\n        yk_sk = yk.T @ sk\n        \n        # BFGS 更新公式\n        Bk = Bk - np.outer(Bk_sk, Bk_sk) / sk_Bk_sk + yk_ykT / yk_sk\n        \n        # 进入下一轮迭代\n        xk = xk_new\n        k += 1\n    \n    return xk\n\n# 设置初始值\nn = 2  # 变量维数\nx0 = np.array([1.0, 1.5])  # 初始点\nB0 = np.eye(n)  # 初始正定矩阵，单位矩阵\n\n# 运行 BFGS 算法\noptimal_x = bfgs(x0, B0)\nprint(\"最优解:\", optimal_x)\n\n```\n\n### DFP算法\n\n**步骤1**：给定初始点 $x_0 \\in \\mathbb{R}^n$，初始对称正定矩阵 $H_0$，精度 $\\epsilon > 0$。令 $k = 0$。\n\n**步骤2**：若 $\\| \\nabla f(x_k) \\| \\leq \\epsilon$，则输出解 $x_k$，算法终止；否则继续。\n\n**步骤3**：计算搜索方向：\n$$ d_k = -H_k \\nabla f(x_k) $$\n（或通过解线性方程组 $B_k d + \\nabla f(x_k) = 0$，其中 $B_k$ 由公式 $B_{k+1}=(I-\\dfrac {y_ks_k^T}{s_k^Ty_k})B_k(I-\\dfrac {y_ks_k^T}{s_k^Ty_k})+\\dfrac {y_ky_k^T}{s_k^Ty_k}$ 计算）\n\n**步骤4**：通过线性搜索确定步长 $\\alpha_k$。\n\n**步骤5**：更新迭代点：\n$$ x_{k+1} = x_k + \\alpha_k d_k $$\n若 $\\| \\nabla f(x_{k+1}) \\| \\leq \\epsilon$，输出解 $x_{k+1}$ 并终止；否则按公式 $H_{k+1}=H_k-\\dfrac {H_ky_ky_k^TH_k}{y_k^TH_ky_k}+\\dfrac {s_ks_k^T}{s_k^Ty_k}$ 计算 $H_{k+1}$。\n\n**步骤6**：令 $k := k + 1$，转步骤3。\n\n```python\nimport numpy as np\n\ndef line_search(f, x, d, grad_x, alpha_init=1.0, rho=0.8, c=1e-4, max_backtrack=20):\n    \"\"\"\n    回溯线搜索算法寻找满足Armijo条件的步长\n    \n    参数:\n    f -- 目标函数\n    x -- 当前点\n    d -- 搜索方向\n    grad_x -- 当前点的梯度\n    alpha_init -- 初始步长\n    rho -- 步长衰减因子\n    c -- Armijo条件常数\n    max_backtrack -- 最大回溯次数\n    \n    返回:\n    alpha -- 选择的步长\n    \"\"\"\n    alpha = alpha_init\n    f_x = f(x)\n    grad_dot_d = np.dot(grad_x, d)\n    \n    # 确保方向是下降方向\n    if grad_dot_d >= 0:\n        return 0.0\n    \n    for _ in range(max_backtrack):\n        x_new = x + alpha * d\n        f_new = f(x_new)\n        if f_new <= f_x + c * alpha * grad_dot_d:\n            return alpha\n        alpha *= rho\n    return alpha  # 返回最小找到的alpha\n\ndef dfp_algorithm(f, grad_f, x0, H0, epsilon, max_iter=1000, line_search_params=None):\n    \"\"\"\n    DFP拟牛顿法实现\n    \n    参数:\n    f -- 目标函数\n    grad_f -- 梯度函数\n    x0 -- 初始点 (numpy数组)\n    H0 -- 初始正定矩阵 (numpy数组)\n    epsilon -- 收敛精度\n    max_iter -- 最大迭代次数\n    line_search_params -- 线搜索参数字典\n    \n    返回:\n    x_k -- 近似最优解\n    \"\"\"\n    # 步骤1: 初始化参数\n    x_k = x0.copy()\n    H_k = H0.copy()\n    grad_k = grad_f(x_k)\n    n = x_k.shape[0]\n    \n    # 线搜索参数设置\n    if line_search_params is None:\n        line_search_params = {'alpha_init': 1.0, 'rho': 0.5, 'c': 1e-4}\n    alpha_init = line_search_params['alpha_init']\n    rho = line_search_params['rho']\n    c = line_search_params['c']\n    \n    for k in range(max_iter):\n        # 步骤2: 检查梯度收敛条件\n        if np.linalg.norm(grad_k) <= epsilon:\n            print(f\"在迭代 {k} 次后收敛\")\n            return x_k\n        \n        # 步骤3: 计算搜索方向\n        d_k = -H_k @ grad_k  # 计算搜索方向\n        \n        # 步骤4: 执行线搜索\n        alpha_k = line_search(f, x_k, d_k, grad_k, alpha_init, rho, c)\n        if alpha_k <= 1e-12:\n            print(\"步长过小，终止计算\")\n            return x_k\n        \n        # 步骤5: 更新迭代点\n        x_next = x_k + alpha_k * d_k\n        grad_next = grad_f(x_next)\n        \n        # 检查新点的收敛条件\n        if np.linalg.norm(grad_next) <= epsilon:\n            print(f\"在迭代 {k+1} 次后收敛\")\n            return x_next\n        \n        # 计算向量s_k和y_k\n        s_k = x_next - x_k\n        y_k = grad_next - grad_k\n        sy_dot = np.dot(s_k, y_k)  # 计算s_k^T y_k\n        \n        # 步骤5续: 更新H矩阵\n        if abs(sy_dot) < 1e-8:  # 防止数值不稳定\n            H_next = H_k.copy()\n        else:\n            Hy = H_k @ y_k\n            yHy = y_k @ Hy\n            if abs(yHy) < 1e-8:\n                term1 = 0.0\n            else:\n                term1 = np.outer(Hy, Hy) / yHy  # 第一修正项\n            \n            term2 = np.outer(s_k, s_k) / sy_dot  # 第二修正项\n            H_next = H_k - term1 + term2  # 更新H矩阵\n        \n        # 步骤6: 迭代变量更新\n        x_k = x_next\n        grad_k = grad_next\n        H_k = H_next.copy()\n    \n    print(f\"达到最大迭代次数 {max_iter} 次未收敛\")\n    return x_k\n\n# 示例使用（二次函数优化）\ndef quadratic(x):\n    return x[0]**2 + 2*x[1]**2 + x[0]*x[1]\n\ndef quadratic_grad(x):\n    return np.array([2*x[0] + x[1], 4*x[1] + x[0]])\n\n# 初始参数设置\nx0 = np.array([2.0, 1.0])\nH0 = np.eye(2)\nepsilon = 1e-6\n\n# 执行DFP算法\nsolution = dfp_algorithm(quadratic, quadratic_grad, x0, H0, epsilon)\nprint(\"优化解:\", solution)\nprint(\"最终梯度:\", quadratic_grad(solution))\n```\n\n### Broyden族\n\n对互为对偶的BFGS公式和DFP公式进行加权组合得到一类重要的拟Newton矩阵修正公式：Broden族\n$$\nB_{k+1}^{\\theta_k}=\\theta_kB_{k+1}^{BFGS}+(1-\\theta_k)B_{k+1}^{DFP}\n$$\n\n$$\nH_{k+1}^{\\phi_k}=\\phi_kH_{k+1}^{BFGS}+(1-\\phi_k)H_{k+1}^{DFP}\n$$\n\n其中 $\\theta_k=1-\\dfrac {1-\\phi_k}{1-\\phi_k+\\phi_k \\mu_k}$ ， $\\mu_k=\\dfrac{y_k^TH_ky_ks_k^TB_ks_k}{s_k^Ty_k}$\n\n### 由人工神经网络方法解微分方程导出的最优化问题\n\n思维学一般把人类的大脑思维分为抽象（逻辑）思维，形象（直观）思维和灵感（顿悟）思维三种基本方式，人工神经网络就是模拟人类思维的形象思维方式去解决问题的。神经网络的基础是神经元，神经元是以生物神经系统的神经细胞为基础的生物模型。人们对生物神经系统进行研究以探讨人工智能的机制时，把神经元数学化，从而产生了神经元数学模型，大量相同形式的神经元连接在一起就组成了神经网络\n\n神经网络是一个高度非线性动力学系统，虽然每个神经元的结构和功能都不复杂，但是神经网络的动态行为却是十分复杂的，因此，用神经网络可以表达实际物理世界的各种现象。\n\n我们可以用有向图表示神经网络，这里有向图是节点与节点之间的有向连接。一般来说，神经网络有两种基本结构：前馈神经网络和递归神经网络。这里我们仅考虑前馈神经网络，它包括：\n\n输入层：源节点构成输入层，它向网络提供输入信号\n\n隐藏层：前馈神经网络有一层或多层隐藏节点，相应的节点称为隐藏神经元\n\n输出层：该层给出相对于源节点的激活模式的网络输出\n\n一个 $m-n_1-n_2-q$ 前馈神经网络，表示该网络有 $m$ 个源节点， 第一个隐藏层有 $n_1$ 个神经元，第二个隐藏层有 $n2$ 个神经元，输出层有 $q$ 个神经元\n\n求解微分方程的方法是多种多样的，如下有一种用神经网络的方法求解微分方程的方法\n\n考虑一阶常微分方程\n$$\n\\dfrac {dy(x)}{dx}=f(x,y),x\\in [0,1]\n$$\n其初始条件为\n$$\ny(0)=y_0\n$$\n利用神经网络的方法我们可以得到一个近似解\n$$\ny_t(x,p)\n$$\n可以用它去近似方程的精准解 $y(x)$ ，其中实验解中的参数 $p$ 为神经网络方法所产生的参数，下面我们讨论神经网络实验解的表示\n\n考虑一个 $1-n-1$ 的前馈神经网络，一阶微分方程的神经网络实验解可以表示为 \n$$\ny_t(x,p)=y_0+xN(x,p)\n$$\n其中 $N(x,p)$ 为前馈神经网络的输出， $x$ 是单一输入， $p$ 是神经网络参数向量。前馈神经网络的输出为 \n$$\nN(x,p)= \\sum_{j=1}^{n} v_jφ(z_j),z_j=w_jx-\\theta_j\n$$\n其中 $w_j$ 是从源节点到隐藏节点 $j$ 的权重 ， $v_j$ 是从隐藏节点 $j$ 到输出节点的权重， $\\theta_j$ 是隐藏节点 $j$ 的阈值， $p=(w,v,\\theta)^T$ 是一个 $3n$ 维向量 ， $φ(z)$ 是 Sigmoid型激活函数\n$$\nφ(z)=\\dfrac {1}{1+e^{-z}}\n$$\n为了使得实验解能尽可能好地近似方程的解，我们要用最优化的方法去确定实验解中的参数 $p$ 。对于一阶微分方程，我们要将 $x$ 在 $[0,1]$ 离散化，得到 $m$ 个值 $0 \\leq x_1<x_2……\\leq x_m=1 $ 以及 $\\left. \\dfrac {dy_t(x,p)}{dx}\\right|_{x=x_i}$ ，$f(x_i,y_t(x_i,p))$ .其中 $x_1$ 到 $x_m$ 称为训练集元素.然后求解最优化问题\n$$\n\\min\\sum_{i=1}^m \\{\\left.\\dfrac{dy_t(x,p)}{dx}\\right|_{x=x_i}-f(x_i,y_t(x_i,p))\\}^2\n$$\n\n\n来确定 $p$ 的值\n\n### 对正定二次型的共轭梯度法\n\n**步0**（初始化）:  \n给定线性无关向量组 $ \\mathbf{g} \\in \\mathbb{R}^n $，令  \n$$\n\\mathbf{d}_0 = -\\mathbf{g}_0, \\quad k := 0\n$$\n\n**步1**（方向更新）:  \n计算搜索方向：  \n$$\n\\mathbf{d}_k = -\\mathbf{g}_k + \\sum_{j=0}^{k-1} \\left( \\frac{\\mathbf{d}_j^T \\mathbf{G} \\mathbf{g}_k}{\\mathbf{d}_j^T \\mathbf{G} \\mathbf{d}_j} \\right) \\mathbf{d}_j\n$$\n\n**步2**（终止条件）:  \n若 $ k = n-1 $，则停止；否则继续。\n\n**步3**（线搜索）:  \n计算步长 $ \\alpha_k $，使得：  \n$$\n\\alpha_k = \\arg\\min_{\\alpha} f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k)\n$$\n\n**步4**（迭代更新）:  \n1. 计算梯度 $ \\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{d}_k) $  \n\n2. 令 $ k := k + 1 $，返回步1\n\n```python\nimport numpy as np\n\ndef conjugate_direction_method(f, grad_f, G, x0):\n    \"\"\"\n    共轭方向法求解无约束优化问题。\n    \n    参数：\n    f : function - 目标函数 f(x)\n    grad_f : function - 目标函数的梯度 ∇f(x)\n    G : ndarray -  正定矩阵，表示二次型 f(x) 的 Hessian 矩阵\n    x0 : ndarray - 初始点\n    \n    返回：\n    x_opt : ndarray - 最优解\n    \"\"\"\n    n = len(x0)  # 变量维度\n    g = grad_f(x0)  # 计算初始梯度\n    d = [-g]  # 初始化搜索方向 d_0 = -g_0\n    x = x0  # 当前点\n    \n    for k in range(n):\n        if k > 0:\n            # 计算共轭方向\n            d_k = -g\n            for j in range(k):\n                beta = (d[j].T @ G @ g) / (d[j].T @ G @ d[j])\n                d_k += beta * d[j]\n            d.append(d_k)\n        \n        # 线搜索，找到最优步长 α_k\n        alpha_k = - (g.T @ d[k]) / (d[k].T @ G @ d[k])\n        \n        # 迭代更新\n        x = x + alpha_k * d[k]\n        g = grad_f(x)  # 计算新的梯度\n        \n        if k == n - 1:\n            break  # 终止条件\n    \n    return x\n\n# 示例使用\nif __name__ == \"__main__\":\n    # 目标函数 f(x) = 0.5 * x^T G x - b^T x\n    G = np.array([[4, 1], [1, 3]])  # 正定矩阵\n    b = np.array([1, 2])\n    \n    def f(x):\n        return 0.5 * x.T @ G @ x - b.T @ x\n    \n    def grad_f(x):\n        return G @ x - b\n    \n    x0 = np.array([2, 1])  # 初始点\n    x_opt = conjugate_direction_method(f, grad_f, G, x0)\n    print(\"最优解:\", x_opt)\n\n```\n\n   \n\n### （FR）对正定二次型的共轭梯度法\n\n**算法步骤：**\n\n**步0**：给定线性无关向量组：$ g \\in \\mathbb{R}^n $，令 $ d_0 = -g_0 $，$ k = 0 $。  \n**步1**：计算  \n$$\nd_k = -g_k + \\frac{d_{k-1}^T G g_k}{d_{k-1}^T G d_{k-1}} d_{k-1}.\n$$\n**步2**：若 $ k = n - 1 $，则停止。  \n**步3**：计算线搜索 $ f(x_k + \\alpha d_k) $ 的最小值点 $ \\alpha_k $。  \n**步4**：求梯度 $ \\nabla f(x_k + \\alpha_k d_k) $，令 $ k = k + 1 $，转**步1**。 \n\n```python\nimport numpy as np\n\ndef conjugate_gradient_method(f, grad_f, G, x0):\n    \"\"\"\n    共轭梯度法求解无约束优化问题。\n    \n    参数：\n    f : function - 目标函数 f(x)\n    grad_f : function - 目标函数的梯度 ∇f(x)\n    G : ndarray -  正定矩阵，表示二次型 f(x) 的 Hessian 矩阵\n    x0 : ndarray - 初始点\n    \n    返回：\n    x_opt : ndarray - 最优解\n    \"\"\"\n    n = len(x0)  # 变量维度\n    g = grad_f(x0)  # 计算初始梯度\n    d = -g  # 初始化搜索方向 d_0 = -g_0\n    x = x0  # 当前点\n    \n    for k in range(n):\n        # 线搜索，找到最优步长 α_k\n        alpha_k = - (g.T @ d) / (d.T @ G @ d)\n        \n        # 迭代更新\n        x = x + alpha_k * d\n        g_new = grad_f(x)  # 计算新的梯度\n        \n        if k == n - 1:\n            break  # 终止条件\n        \n        # 计算新的搜索方向\n        beta_k = (d.T @ G @ g_new) / (d.T @ G @ d)\n        d = -g_new + beta_k * d\n        g = g_new  # 更新梯度\n    \n    return x\n\n# 示例使用\nif __name__ == \"__main__\":\n    # 目标函数 f(x) = 0.5 * x^T G x - b^T x\n    G = np.array([[4, 1], [1, 3]])  # 正定矩阵\n    b = np.array([1, 2])\n    \n    def f(x):\n        return 0.5 * x.T @ G @ x - b.T @ x\n    \n    def grad_f(x):\n        return G @ x - b\n    \n    x0 = np.array([2, 1])  # 初始点\n    x_opt = conjugate_gradient_method(f, grad_f, G, x0)\n    print(\"最优解:\", x_opt)\n\n```\n\n\n\n### HS共轭梯度法\n\n**步骤1**：  \n给定初始点 $ x_0 \\in \\mathbb{R}^n $，精度 $ \\varepsilon > 0 $。  \n令 $ d_0 = -\\nabla f(x_0) $，令 $ k := 0 $。\n\n**步骤2**：  \n若 $ \\|\\nabla f(x_k)\\| \\leq \\varepsilon $，则得解 $ x_k $，算法终止。否则转步骤3。\n\n**步骤3**：  \n由线性搜索计算步长 $ \\alpha_k $。\n\n**步骤4**：  \n令 $ x_{k+1} = x_k + \\alpha_k d_k $。\n\n**步骤5**：  \n由式 $$\nd_k = \\begin{cases} \n-\\nabla f(x_0), & k=0 \\\\\n-\\nabla f(x_k) + \\beta_k d_{k-1}, & k \\geq 1 \n\\end{cases}\n$$ 确定 $ d_{k+1} $，其中 $ \\beta_{k+1} = \\beta_{k+1}^{HS} $\n\n其中 $\n\\beta_{k}^{\\text{HS}} = \\frac{\n    \\nabla f(x_k)^T [\n    \\nabla f(x_k) - \n    \\nabla f(x_{k-1})]\n}{d_{k-1}^T [\n    \\nabla f(x_k) - \n    \\nabla f(x_{k-1})]\n}, \\quad k=1, 2, \\ldots\n$ \n\n令 $ k := k + 1 $，转步骤2。\n\n```python\nimport numpy as np\n\ndef hs_conjugate_gradient(f, grad_f, x0, epsilon=1e-6):\n    \"\"\"\n    HS 共轭梯度法求解无约束优化问题。\n    \n    参数：\n    f : function - 目标函数 f(x)\n    grad_f : function - 目标函数的梯度 ∇f(x)\n    x0 : ndarray - 初始点\n    epsilon : float - 终止精度\n    \n    返回：\n    x_opt : ndarray - 最优解\n    \"\"\"\n    x = x0\n    g = grad_f(x)  # 计算初始梯度\n    d = -g  # 初始化搜索方向\n    k = 0  # 迭代步数\n    \n    while np.linalg.norm(g) > epsilon:\n        # 线搜索计算步长 α_k\n        alpha_k = - (g.T @ d) / (d.T @ grad_f(x + d))\n        \n        # 更新 x\n        x_new = x + alpha_k * d\n        g_new = grad_f(x_new)  # 计算新的梯度\n        \n        # 计算 β_{k+1}^{HS} (Hestenes-Stiefel 公式)\n        beta_hs = (g_new.T @ (g_new - g)) / (d.T @ (g_new - g))\n        \n        # 更新搜索方向\n        d = -g_new + beta_hs * d\n        \n        # 更新 x 和梯度\n        x, g = x_new, g_new\n        k += 1\n    \n    return x\n\n# 示例使用\nif __name__ == \"__main__\":\n    # 目标函数 f(x) = 0.5 * x^T G x - b^T x\n    G = np.array([[4, 1], [1, 3]])  # 正定矩阵\n    b = np.array([1, 2])\n    \n    def f(x):\n        return 0.5 * x.T @ G @ x - b.T @ x\n    \n    def grad_f(x):\n        return G @ x - b\n    \n    x0 = np.array([2, 1])  # 初始点\n    x_opt = hs_conjugate_gradient(f, grad_f, x0)\n    print(\"最优解:\", x_opt)\n```\n\n1.HS公式\n$$\n\\beta_k^{HS} = \\dfrac {[∇f(x_k)^T (∇f(x_k) - ∇f(x_{k-1}))]} {[d_{k-1}^T (∇f(x_k) - ∇f(x_{k-1}))]}\n$$\n2.PRP公式\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\n∇ f(x_k)^T [\n∇ f(x_k) - \n∇ f(x_{k-1})]}{|\n∇ f(x_{k-1})|^2}\n$$\n3.FR公式\n$$\n\\beta_k^{FR}=\\dfrac {|∇f(x_k)|^2}{|∇f(x_{k-1})|^2}\n$$\n4.CD公式\n$$\n\\beta_k^{CD}=-\\dfrac {|∇f(x_k)|^2}{d_{k-1}^T∇f(x_{k-1})}\n$$\n5.DY公式\n$$\n\\beta_k^{DY}=\\dfrac {|∇f(x_{k})|^2}{d_{k-1}^T[∇f(x_k)-∇f(x_{k-1})]}\n$$\n\n### 非线性共轭梯度法\n\n1. **步1** 给定初始点 $x_0 \\in \\mathbb{R}^n$，精度 $\\varepsilon > 0$。令 $d_0 = -\\nabla f(x_0)$，令 $k := 0$。\n2. **步2** 若 $\\|\\nabla f(x_k)\\| \\leq \\varepsilon$，则得解 $x_k$，算法终止。否则转步3。\n3. **步3** 由线性搜索计算步长 $\\alpha_k$。\n4. **步4** 令 $x_{k+1} = x_k + \\alpha_k d_k$。\n5. **步5** 确定 $d_{k+1}$ 和 $\\beta_k$，其中 $d_{k+1} = -\\nabla f(x_{k+1}) + \\beta_k d_k$。令 $k := k + 1$，转步2。\n\n```python\nimport numpy as np\nfrom typing import Callable, Tuple\n\ndef gradient_descent(\n    f: Callable[[np.ndarray], float],          # 目标函数，输入x，返回标量函数值f(x)\n    grad_f: Callable[[np.ndarray], np.ndarray], # 目标函数的梯度函数，输入x，返回梯度向量∇f(x)\n    x0: np.ndarray,                            # 初始点x_0∈R^n\n    epsilon: float = 1e-6,                     # 精度ε>0，用于判断梯度是否足够小以终止算法\n    max_iter: int = 1000,                      # 最大迭代次数，防止无限循环\n    line_search: Callable = None               # 线性搜索函数，用于计算步长α_k\n) -> Tuple[np.ndarray, int]:\n    \"\"\"\n    最速下降法（梯度下降法）实现\n    \n    参数:\n        f: 目标函数\n        grad_f: 目标函数的梯度\n        x0: 初始点\n        epsilon: 收敛精度\n        max_iter: 最大迭代次数\n        line_search: 步长搜索函数\n        \n    返回:\n        x_k: 找到的解\n        k: 迭代次数\n    \"\"\"\n    # 步1：初始化\n    x_k = x0.copy()                           # 初始点x_0\n    d_k = -grad_f(x_k)                        # 初始搜索方向d_0 = -∇f(x_0)\n    k = 0                                     # 迭代次数k初始化为0\n    \n    # 步2：迭代直到满足终止条件\n    while k < max_iter:\n        grad_norm = np.linalg.norm(grad_f(x_k))  # 计算当前梯度的范数||∇f(x_k)||\n        \n        # 检查是否满足终止条件：梯度范数是否小于等于ε\n        if grad_norm <= epsilon:\n            print(f\"算法在 {k} 次迭代后收敛\")\n            return x_k, k\n        \n        # 步3：由线性搜索计算步长α_k\n        # 如果没有提供line_search函数，则使用固定步长（这里简化为0.01，实际应用中需要调整）\n        if line_search is None:\n            alpha_k = 0.01  # 固定步长，实际应用应使用线搜索或其他自适应方法\n        else:\n            alpha_k = line_search(f, grad_f, x_k, d_k)  # 使用线搜索计算最优步长\n        \n        # 步4：更新x_{k+1} = x_k + α_k * d_k\n        x_k_plus_1 = x_k + alpha_k * d_k\n        \n        # 步5：计算新的搜索方向d_{k+1} = -∇f(x_{k+1}) + β_k * d_k\n        # 在最速下降法中，β_k = 0，即每次都是负梯度方向\n        # 这里实现的是最速下降法（β_k=0），但保留了β_k的接口以便扩展为共轭梯度法\n        beta_k = 0.0  # 对于最速下降法，β_k=0\n        d_k_plus_1 = -grad_f(x_k_plus_1) + beta_k * d_k\n        \n        # 更新变量，准备下一次迭代\n        x_k = x_k_plus_1\n        d_k = d_k_plus_1\n        k += 1\n    \n    print(f\"达到最大迭代次数 {max_iter}，未收敛到所需精度\")\n    return x_k, k\n\n# 示例使用：优化二次函数 f(x) = (x-1)^2 + (y-2)^2\nif __name__ == \"__main__\":\n    # 定义目标函数和梯度函数\n    def f(x):\n        return (x[0]-1)**2 + (x[1]-2)**2\n    \n    def grad_f(x):\n        return np.array([2*(x[0]-1), 2*(x[1]-2)])\n    \n    # 初始点\n    x0 = np.array([0.0, 0.0])\n    # 调用梯度下降法\n    solution, iterations = gradient_descent(f, grad_f, x0, epsilon=1e-6)\n    \n    print(f\"找到的解: {solution}\")\n    print(f\"实际最小值点: [1.0, 2.0]\")\n    print(f\"迭代次数: {iterations}\")\n```\n\n### Gauss-Newton法\n\n**目的:**解决最小二乘法的问题\n\n**步骤：**\n\n1. 给定解的初始估计 $ x_0 $，$ \\varepsilon > 0 $，$ k := 0 $；\n2. 如果 $ x^{(k)} $ 满足精度要求，停止迭代；\n3. 解方程组 $ J_k^T J_k d_k = -J_k^T r_k $；\n4. 更新解：  \n   $ x_k := x_k + \\alpha_k d_k $，  \n   其中 $ \\alpha_k $ 为一维线搜索结果，$ k := k + 1 $，转步2。\n\n**注释：**\n- **基本Gauss-Newton法**：步长固定为 $ \\alpha_k = 1 $；\n- **阻尼Gauss-Newton法**：步长 $ \\alpha_k $ 通过一维线搜索动态确定。\n\n**Gauss-Newton法的优缺点**\n\n**优点**\n\n1. **零残量问题**  \n   当 $ S^* = 0 $ 时，具有**局部二阶收敛速度**；\n2. **小残量问题**  \n   当 $ S^* $ 较小或问题接近线性时，具有**快速的局部收敛速度**；\n3. **线性最小二乘问题**  \n   可通过一步迭代达到极小点。\n\n**缺点**\n\n1. **大残量问题**  \n   对于非严重的大残量问题，收敛速度较慢；\n2. **强非线性或极大残量问题**  \n   当残量极大或 $ f(x) $ 非线性程度过高时，算法可能**不收敛**；\n3. **Jacobian矩阵不满秩**  \n   若 $ J_k $ 不满秩，方法无法定义；\n4. **全局收敛性**  \n   算法**不一定保证总体收敛**。\n\n**公式说明：**\n\n- $ S^* $: 残量平方和的最小值  \n- $ J_k $: 第 $ k $ 次迭代的 Jacobian 矩阵\n\n```python\nimport numpy as np\n\ndef gauss_newton(residual, jacobian, x0, epsilon, max_iter, damped=False):\n    \"\"\"Gauss-Newton法实现（支持阻尼选项）\n\n    参数:\n    residual -- 残差函数，输入参数向量返回残差向量\n    jacobian -- 雅可比矩阵函数，输入参数向量返回雅可比矩阵\n    x0 -- 初始估计值向量\n    epsilon -- 收敛精度（残差范数阈值）\n    max_iter -- 最大迭代次数\n    damped -- 是否启用阻尼（默认False）\n\n    返回:\n    x -- 最终迭代得到的最优解向量\n    \"\"\"\n    \n    x = np.array(x0, dtype=float)  # 将初始估计值转换为numpy数组\n    \n    for k in range(max_iter):\n        # Step 2: 检查当前解是否满足精度要求\n        r = residual(x)  # 计算当前参数的残差向量\n        residual_norm = np.linalg.norm(r)  # 计算残差的欧氏范数\n        if residual_norm < epsilon:  # 残差足够小则停止迭代\n            break\n        \n        # Step 3: 解线性方程组 J^T J d = -J^T r\n        J = jacobian(x)  # 计算当前参数的雅可比矩阵\n        JtJ = J.T @ J  # 计算J的转置与J的乘积\n        Jtr = J.T @ r  # 计算J的转置与残差向量的乘积\n        \n        try:\n            # 尝试直接解线性方程组\n            d = np.linalg.solve(JtJ, -Jtr)\n        except np.linalg.LinAlgError:  # 若矩阵不可逆则使用最小二乘解\n            d, *_ = np.linalg.lstsq(JtJ, -Jtr, rcond=None)\n        \n        # Step 4: 计算步长alpha（阻尼时需进行一维搜索）\n        if damped:  # 阻尼Gauss-Newton模式\n            # 定义目标函数（残差平方和的一半）\n            def cost(alpha):\n                return 0.5 * np.linalg.norm(residual(x + alpha * d))**2\n            \n            alpha = 1.0  # 初始步长设为1\n            c = 0.1  # Armijo条件中的充分下降系数\n            rho = 0.5  # 步长缩减因子\n            grad = J.T @ r  # 计算梯度∇f(x) = J^T r\n            grad_dot_d = grad.dot(d)  # 梯度与方向d的点积\n            \n            # 检查Armijo条件：cost(alpha) ≤ cost(0) + c*alpha*grad_dot_d\n            sufficient_decrease = cost(alpha) <= cost(0) + c * alpha * grad_dot_d\n            while not sufficient_decrease and alpha > 1e-6:  # 当步长过小时停止搜索\n                alpha *= rho  # 步长减半\n                sufficient_decrease = cost(alpha) <= cost(0) + c * alpha * grad_dot_d\n        else:  # 基本Gauss-Newton模式\n            alpha = 1.0  # 固定步长\n            \n        # 更新当前解向量\n        x += alpha * d  # 按步长alpha进行更新\n    \n    return x  # 返回最终迭代结果\n```\n\n### LMF方法\n\n**步骤：**\n\n1. **初始化**  \n   给定 $ x_0 \\in \\mathbb{R}^n $，$ v_0 > 0 $，$ \\varepsilon > 0 $，$ k := 0 $；\n2. **终止条件**  \n   若满足终止条件，输出结果并停止迭代；\n3. **求解方程**  \n   计算 $ (J_k^T J_k + v_k I) d_k = -J_k^T r_k $，得到步长 $ d_k $；\n4. **计算步长质量**  \n   计算 $ \\rho_k = \\dfrac {\\triangle f_k}{\\triangle q_k} $；\n5. **更新阻尼因子**  \n   - 若 $ \\rho_k < 0.25 $，则 $ v_{k+1} = 4v_k $；\n   - 若 $ \\rho_k > 0.75 $，则 $ v_{k+1} = v_k / 2 $；\n   - 否则 $ v_{k+1} = v_k $；\n6. **迭代更新**  \n   - 若 $ \\rho_k \\leq 0 $，则 $ x_{k+1} = x_k $；\n   - 否则 $ x_{k+1} = x_k + d_k $；\n   - 令 $ k := k + 1 $，转步骤2。\n\n**公式说明**：\n\n- $ J_k $: 第 $ k $ 次迭代的 Jacobian 矩阵  \n- $ r_k $: 第 $ k $ 次迭代的残差向量  \n- $ v_k $: 阻尼因子（控制步长可靠性）  \n- $ \\rho_k $: 步长质量指标（评估步长接受程度）\n\n```python\nimport numpy as np\n\ndef levenberg_marquardt(residual, jacobian, x0, epsilon, max_iter, v0=0.01):\n    \"\"\"Levenberg-Marquardt算法实现（非线性最小二乘问题）\n\n    参数:\n    residual -- 残差函数，输入参数向量返回残差向量\n    jacobian -- 雅可比矩阵函数，输入参数向量返回雅可比矩阵\n    x0 -- 初始估计值向量\n    epsilon -- 收敛精度（残差范数阈值）\n    max_iter -- 最大迭代次数\n    v0 -- 初始阻尼因子（默认0.01）\n\n    返回:\n    x -- 最终迭代得到的最优解向量\n    \"\"\"\n    \n    x_prev = np.array(x0, dtype=float)  # 初始参数估计\n    v = v0  # 初始阻尼因子\n    k = 0  # 迭代计数器\n    \n    while k < max_iter:\n        # Step 2: 检查终止条件\n        r = residual(x_prev)  # 计算当前参数的残差向量\n        residual_norm = np.linalg.norm(r)  # 计算残差的欧氏范数\n        if residual_norm < epsilon:  # 残差足够小则停止迭代\n            break\n        \n        # Step3: 解方程 (J^T J + vI) d = -J^T r\n        J = jacobian(x_prev)  # 当前参数的雅可比矩阵\n        JtJ = J.T @ J  # 计算J的转置与J的乘积\n        Jtr = J.T @ r  # 计算J的转置与残差的乘积\n        \n        # 构建矩阵A = JtJ + vI\n        A = JtJ + v * np.eye(len(x_prev))\n        \n        try:\n            # 尝试直接解线性方程组\n            d = np.linalg.solve(A, -Jtr)\n        except np.linalg.LinAlgError:  # 若矩阵奇异则使用最小二乘解\n            d, *_ = np.linalg.lstsq(A, -Jtr, rcond=None)\n        \n        # 计算候选解x_candidate和新残差\n        x_candidate = x_prev + d\n        r_new = residual(x_candidate)\n        \n        # Step4: 计算步长质量指标ρ\n        # 计算实际减少量Δf = 0.5*(||r||^2 - ||r_new||^2)\n        delta_f = 0.5 * (np.linalg.norm(r)**2 - np.linalg.norm(r_new)**2)\n        \n        # 计算预测减少量Δq = d^T (JtJ d) + v*(d^T d)\n        JtJd = JtJ @ d\n        delta_q = d.dot(JtJd) + v * (d.dot(d))\n        \n        # 当预测减少量为负时强制拒绝步长\n        if delta_q <= 0:\n            rho = -np.inf  # 强制ρ为负无穷\n        else:\n            rho = delta_f / delta_q  # 计算步长质量\n        \n        # Step5: 更新阻尼因子v\n        if rho < 0.25:  # 步长质量差，增大阻尼因子\n            v_new = v * 4.0\n        elif rho > 0.75:  # 步长质量好，减小阻尼因子\n            v_new = v / 2.0\n        else:  # 保持当前阻尼因子\n            v_new = v\n        \n        # Step6: 根据ρ值决定是否接受步长\n        if rho <= 0:  # 步长导致残差增大，不接受更新\n            x_next = x_prev.copy()  # 保持当前参数\n            v = v_new  # 更新阻尼因子\n        else:  # 接受步长，更新参数和阻尼因子\n            x_next = x_candidate\n            v = v_new\n        \n        # 更新迭代变量\n        x_prev = x_next\n        k += 1\n    \n    return x_prev  # 返回最终解\n```\n\n### 外点罚函数法\n\n**步0**：选定初始点 $x_0 \\in \\mathbb{R}^n$；初始罚因子 $\\mu_1 > 0$，放大系数 $\\sigma > 1$，精度 $\\varepsilon > 0$。置 $k=1$。\n\n**步1**：构造增广目标函数  \n$$ F_{\\mu_k}(x) = f(x) + \\dfrac 1 2\\mu_k P(x) $$\n\n**步2**：以 $x_{k-1}$ 为初始点求解无约束问题：  \n$$ \\min F_{\\mu_k}(x), \\quad \\text{得解} \\ x_k $$\n\n**步3**：若 $\\mu_k P(x_k) < \\varepsilon$，停止迭代，输出 $x_k$；否则转步4。\n\n**步4**：更新罚因子 $\\mu_{k+1} = \\sigma \\mu_k$，置 $k = k + 1$，转步1。\n\n```python\nimport numpy as np\n\n# 外点罚函数法\ndef exterior_penalty_method(f, P, x0, mu1, sigma, epsilon, solver):\n    \"\"\"\n    参数说明：\n    f: 原目标函数 f(x)\n    P: 罚函数 P(x)\n    x0: 初始点，numpy数组\n    mu1: 初始罚因子，正数\n    sigma: 放大系数，sigma > 1\n    epsilon: 精度要求\n    solver: 无约束优化子程序，输入一个目标函数和初始点，输出局部极小点\n    \"\"\"\n\n    # 步0：初始化\n    xk = np.array(x0)\n    muk = mu1\n    k = 1\n\n    while True:\n        # 步1：构造增广目标函数\n        def F_mu(x):\n            x = np.array(x)\n            return f(x) + (1 / (2 * muk)) * P(x)\n\n        # 步2：以 x_{k-1} 为初始点求解无约束问题\n        xk = solver(F_mu, xk)\n\n        # 步3：检查停止条件\n        if muk * P(xk) < epsilon:\n            return xk\n\n        # 步4：更新罚因子和迭代次数\n        muk = sigma * muk\n        k += 1\n\n```\n\n### 内点罚函数法\n\n**步0**：选定初始点 $x_0 \\in \\mathbb{R}^n$；初始罚因子 $\\mu_1 > 0$，放大系数 $\\sigma > 1$，精度 $\\varepsilon > 0$。置 $k=1$。\n\n**步1**：构造增广目标函数  \n$$ F_{\\mu_k}(x) = f(x) + \\mu_k^{-1} S(x) $$\n\n**步2**：以 $x_{k-1}$ 为初始点求解无约束问题：  \n$$ \\min F_{\\mu_k}(x), \\quad \\text{得解} \\ x_k $$\n\n**步3**：若 $\\mu_k S(x_k) < \\varepsilon$，停止迭代，输出 $x_k$；否则转步4。\n\n**步4**：更新罚因子 $\\mu_{k+1} = \\sigma \\mu_k$，置 $k = k + 1$，转步1。\n\n```python\nimport numpy as np\n\n# 内点罚函数法\ndef interior_penalty_method(f, S, x0, mu1, sigma, epsilon, solver):\n    \"\"\"\n    参数说明：\n    f: 原目标函数 f(x)\n    S: 罚函数 S(x)（比如 -log形式）\n    x0: 初始点，numpy数组\n    mu1: 初始罚因子，正数\n    sigma: 放大系数，sigma > 1\n    epsilon: 精度要求\n    solver: 无约束优化子程序，输入一个目标函数和初始点，输出局部极小点\n    \"\"\"\n\n    # 步0：初始化\n    xk = np.array(x0)\n    muk = mu1\n    k = 1\n\n    while True:\n        # 步1：构造增广目标函数\n        def F_mu(x):\n            x = np.array(x)\n            return f(x) + (1 / muk) * S(x)\n\n        # 步2：以 x_{k-1} 为初始点求解无约束问题\n        xk = solver(F_mu, xk)\n\n        # 步3：检查停止条件\n        if muk * S(xk) < epsilon:\n            return xk\n\n        # 步4：更新罚因子和迭代次数\n        muk = sigma * muk\n        k += 1\n\n```\n\n### 等式约束问题的乘子法\n\n**步0**：选定初始点 $x_0 \\in \\mathbb{R}^n$，初始乘子估计 $\\lambda_1 \\in \\mathbb{R}^m$；初始罚因子 $\\mu_1 > 0$，常数 $\\sigma > 1$，$\\beta \\in (0,1)$；精度 $\\varepsilon > 0$。置 $k := 1$。\n\n**步1**：构造增广目标函数  \n$$\nL_{\\mu_k}(x, \\lambda_k) = L(x, \\lambda_k) + \\frac{1}{2} \\mu_k S(x)\n$$\n其中 $L(x, \\lambda_k) = f(x) - \\lambda_k^T h(x)$。\n\n**步2**：以 $x_{k-1}$ 为初始点求解无约束问题：  \n$$\n\\min_x L_{\\mu_k}(x, \\lambda_k)\n$$\n设其解为 $x_k$。\n\n**步3**：若 $\\|h(x_k)\\| \\leq \\varepsilon$，则得解 $x_k$，**STOP**；否则转步4。\n\n**步4**：若 $\\frac{\\|h(x_k)\\|}{\\|h(x_{k-1})\\|} \\geq \\beta$ 成立，则令 $\\mu_{k+1} = \\mu_k$；否则 $\\mu_{k+1} = \\sigma \\mu_k$。\n\n**步5**：令 $\\lambda_{k+1} = \\lambda_k - \\mu_k h(x_k)$，置 $k := k+1$，转步1。\n\n```python\nimport numpy as np\n\n# 等式约束问题的乘子法\ndef multiplier_method(f, h, x0, lambda0, mu1, sigma, beta, epsilon, solver):\n    \"\"\"\n    参数说明：\n    f: 目标函数 f(x)\n    h: 等式约束函数 h(x)，返回一个向量\n    x0: 初始点，numpy数组\n    lambda0: 初始乘子估计，numpy数组\n    mu1: 初始罚因子，正数\n    sigma: 放大系数，sigma > 1\n    beta: 收敛判别系数，0 < beta < 1\n    epsilon: 精度要求\n    solver: 无约束优化子程序，输入一个目标函数和初始点，输出局部极小点\n    \"\"\"\n\n    # 步0：初始化\n    xk = np.array(x0)\n    lambdak = np.array(lambda0)\n    muk = mu1\n    k = 1\n\n    while True:\n        # 步1：构造增广拉格朗日函数\n        def L_mu(x):\n            x = np.array(x)\n            L = f(x) - np.dot(lambdak, h(x))\n            penalty = (1/2) * muk * np.linalg.norm(h(x))**2\n            return L + penalty\n\n        # 步2：以 x_{k-1} 为初始点求解无约束极小化问题\n        x_prev = xk.copy()\n        xk = solver(L_mu, xk)\n\n        # 步3：检查停止条件\n        if np.linalg.norm(h(xk)) <= epsilon:\n            return xk\n\n        # 步4：更新罚因子\n        if np.linalg.norm(h(xk)) / np.linalg.norm(h(x_prev)) >= beta:\n            muk = muk  # 保持不变\n        else:\n            muk = sigma * muk\n\n        # 步5：更新拉格朗日乘子并迭代\n        lambdak = lambdak - muk * h(xk)\n        k += 1\n\n```\n\n### 一般约束问题的乘子法\n\n**步0**：初始化参数  \n\n- 初始点 $x_0 \\in \\mathbb{R}^n$  \n- 乘子估计 $\\lambda^{(1)} \\in \\mathbb{R}^m$（需满足 $\\lambda_i^{(1)} \\geq 0,\\ \\forall i \\in I$）  \n- 初始罚因子 $\\mu_1 > 0$，放大系数 $\\sigma > 1$，收缩率 $\\beta \\in (0,1)$  \n- 精度阈值 $\\varepsilon > 0$  \n- 置迭代计数器 $k = 1$\n\n**步1**：构造增广Lagrange函数  \n$$\nL_{\\mu}(x,\\lambda) = f(x) \n+ \\frac{1}{2\\mu} \\sum_{i \\in I} \\left( \\max^2 \\{0, \\lambda_i - \\mu g_i(x)\\} - \\lambda_i^2 \\right) \n- \\sum_{j \\in E} \\lambda_j h_j(x) \n+ \\frac{\\mu}{2} \\sum_{j \\in E} h_j^2(x)\n$$\n**步2**：求解无约束子问题  \n以 $x_{k-1}$ 为初始点，求解：  \n$$ \\min_x L_{\\mu_k}(x, \\lambda^{(k)}) $$  \n得到解 $x_k$\n\n**步3**：终止判定  \n若 $\\|h(x_k)\\| + \\|\\min(g(x_k), 0)\\| \\leq \\varepsilon$，输出 $x_k$ 并终止；否则转步4\n\n**步4**：自适应罚因子更新  \n$$ \\text{若}\\ \\frac{\\|h(x_k)\\| + \\|\\min(g(x_k),0)\\|}{\\|h(x_{k-1})\\| + \\|\\min(g(x_{k-1}),0)\\|} \\geq \\beta,\\ \\text{则}\\ \\mu_{k+1} = \\mu_k $$  \n否则 $\\mu_{k+1} = \\sigma \\mu_k$\n\n**步5**：乘子更新  \n$$\n\\begin{cases}\n\\lambda_i^{(k+1)} = \\max \\left\\{ 0,\\ \\lambda_i^{(k)} - \\mu g_i(x_k) \\right\\}, & \\forall i \\in I \\\\\n\\lambda_j^{(k+1)} = \\lambda_j^{(k)} - \\mu h_j(x_k), & \\forall j \\in E\n\\end{cases}\n$$\n置 $k := k+1$，返回步1\n\n```python\nimport numpy as np\n\n# 一般约束问题的乘子法\ndef general_multiplier_method(f, g, h, x0, lambda0, mu1, sigma, beta, epsilon, solver):\n    \"\"\"\n    参数说明：\n    f: 目标函数 f(x)\n    g: 不等式约束函数组 g(x)，返回一个numpy数组，g_i(x) <= 0\n    h: 等式约束函数组 h(x)，返回一个numpy数组，h_j(x) = 0\n    x0: 初始点，numpy数组\n    lambda0: 初始乘子估计（先 g 后 h 排列），numpy数组\n    mu1: 初始罚因子，正数\n    sigma: 放大系数，sigma > 1\n    beta: 收缩率，0 < beta < 1\n    epsilon: 精度阈值\n    solver: 无约束优化子程序，输入一个目标函数和初始点，输出局部极小点\n    \"\"\"\n\n    # 步0：初始化\n    xk = np.array(x0)\n    lambdak = np.array(lambda0)\n    muk = mu1\n    k = 1\n\n    # 记录不等式与等式的个数\n    m_g = len(g(xk))  # 不等式约束数目\n    m_h = len(h(xk))  # 等式约束数目\n\n    while True:\n        # 步1：构造增广Lagrange函数\n        def L_mu(x):\n            x = np.array(x)\n            gx = g(x)\n            hx = h(x)\n            term1 = f(x)\n            term2 = (1 / (2 * muk)) * np.sum((np.maximum(0, lambdak[:m_g] - muk * gx))**2 - lambdak[:m_g]**2)\n            term3 = - np.dot(lambdak[m_g:], hx)\n            term4 = (muk / 2) * np.sum(hx**2)\n            return term1 + term2 + term3 + term4\n\n        # 步2：求解无约束子问题\n        x_prev = xk.copy()\n        xk = solver(L_mu, xk)\n\n        # 步3：终止判定\n        violation = np.linalg.norm(h(xk)) + np.linalg.norm(np.minimum(g(xk), 0))\n        if violation <= epsilon:\n            return xk\n\n        # 步4：自适应罚因子更新\n        prev_violation = np.linalg.norm(h(x_prev)) + np.linalg.norm(np.minimum(g(x_prev), 0))\n        if violation / prev_violation >= beta:\n            muk = muk  # 保持罚因子不变\n        else:\n            muk = sigma * muk\n\n        # 步5：乘子更新\n        gx = g(xk)\n        hx = h(xk)\n        lambda_g_new = np.maximum(0, lambdak[:m_g] - muk * gx)\n        lambda_h_new = lambdak[m_g:] - muk * hx\n        lambdak = np.concatenate([lambda_g_new, lambda_h_new])\n\n        # 迭代次数加一\n        k += 1\n\n```\n","tags":["学习笔记"]},{"title":"【游记】2025年电子设计竞赛国赛","url":"/2025/08/06/【游记】2025年电子设计竞赛国赛/","content":"\n# 【游记】2025年电子设计竞赛国赛\n\n### Day0 7.29\n\n是电子设计竞赛的国赛前一天，经过了一年多的准备终于等到了这一天。不管是什么样的题目都已经做过了，面对电子设计竞赛的国赛也是没有了心理负担，准备进行下去了。早上先是补了补觉，为国赛的熬夜做准备，然后到实验室准备明天的比赛。\n\n早上先是调了调自己的M0，把之前调过的工程都改了改看了看，确保没有问题之后就和队友一起收拾自己的东西，方便为自己之后的移位置做准备。收拾东西大多是硬件汤进行，软件没有多少收拾的，我就和软件李开始玩文明6。文明6是刚下载没有多久的，因此很多情况下都是我发展缓慢而软件李科技和文化值发展得非常快，让我目不暇接。很快，我们把东西收拾完之后就回寝室了，准备早睡，迎接第二天早上六点的起床，七点半的题目发布。\n\n### Day1 7.30\n\n起了一个大早，早上很快就拿到了题目，仪器组一共三道题目，分别是以太网线的检测仪、接收机还有电路模型探究装置。那两道题目我们是没有选的。因为以太网线的检测仪是测量题，需要标定等一系列麻烦的事情，对于我们来说并不友好。那么接收机呢？这个涉及射频，我们还没有学会把非常小的信号放大，因此也不选。那么留在我们眼前的只剩下了一道题目，电路模型探究装置。\n\n这道题目看起来非常简单，于是我们也就选了这道题目。基础部分非常简单，直接用AD9833进行波形的输出，但是因为电压要求，对于输出级还需要一些处理，所以基础部分留给了硬件手去搭建，开始思考软件的问题。对于发挥部分，第一问判断滤波器类型，也是比较简单的，我们使用正弦波进行扫频，然后通过ADC读取通过滤波网络和AD637有效值检波器的电压大小，就可以算出在特定频率下该滤波器对于信号的增益。然后对于低频与高频分别扫频，在题目要求的范围内，我们可以根据两点法判断滤波器的类型，这样发挥部分第一问也解决了。\n\n现在于我们而言，比较困难的是发挥部分的第二问，我们如何学习模型电路，并且复刻出模型电路呢？我们先是想到了一种方法，因为我们可以对未知电路进行扫频学习，那么我们就可以知道位置电路在各个频点下的幅频响应和相频响应。然后在输出的时候，我们对于输入的信号，就进行FFT操作，分析出其含有的各个频率的分量，根据之前的幅频响应和相频响应进行累加，继而使用DMA+DAC输出波形。这个时候，我们看到了发挥部分后面的输出波形要求。步进为200Hz，这让我们意识到无法使用DMA+DAC进行求解，另外，后面要求输出的波形和未知电路的波形要求同频显示，这让我们更加确信了频率控制字没有那么精准的时候不能选择这个方法。\n\n那么我们该怎么办呢？我们该使用什么方法才能做到同频显示呢？这个时候我们想而到了使用FPGA的方法，使用FPGA写一个二阶IIR真实滤波器，使用单片机传入参数。\n\n目标已然确定，开始行动了。李开始写滤波器，汤搭硬件，我去寻找拟合曲线的方法。然而拟合曲线并非很容易，最先想到的是最小二乘法，先去尝试，然而最小二乘法在单片机中运行不动，只能放弃，但是又没有别的方法，我想到了python。但是python并不能在单片机中使用，怎么办，只有天知道。这个时候已经接近第二天的凌晨，我听说了别的组使用树莓派的做法，于是我也去找周老师拿了一块树莓派，进行开始写树莓派。但是我不太会用树莓派，于是我就开始从0开始安装驱动等之类的，然后，也是成功安装好了系统。回去睡觉了。\n\n### Day2 7.31\n\n第二天也是早早过来，寻找如何解决曲线拟合的问题，我搞了第一天的树莓派，一直搞到晚上七点多才解决所有的驱动，然后开始尝试。然而这个时候李发现了一种可以在单片机中计算出来系数的方法，于是他就开始写，我们将树莓派解决作为备选方案。\n\n直到凌晨，我们也没有弄出些什么，只知道单片机拟合出来的数据始终不是那么准确，我们不知道为什么，我们陷入了长时间的疑惑，这个时候已经到达了第三天的凌晨。\n\n### Day3 8.1\n\n小睡了一会儿，我们还是没有解决，我们开始寻找方法，但是没有成功。真的要放弃了吗？后来发现有一部分原因是DE10的时钟虽然设置的是1MHz，但是实际上是1.17Mhz，当我们改过来之后就好了一点，我们感觉非常兴奋。\n","categories":["游记"]},{"title":"为美好的世界献上祝福","url":"/2025/07/10/为美好的世界献上祝福/","content":"\n为美好的世界献上祝福\n","categories":["漫评"]},{"title":"【回忆录】到信息学一游","url":"/2025/07/09/到信息学一游/","content":"\n# 【回忆录】到信息学一游\n\n## 序章 夜来幽梦忽还乡\n\n现在是2025年7月份的一个晚上，是大二的暑假，大学生活已然过半，我终于通过一个自己参照教程的方式配置好了属于自己的博客，兴致满满。在添加友情链接的时候，我想起了我在初三的时候就已经了解到的学长 $Dew$ 和 $wjyyy$ ，于是找到他们的博客。在他们的博客中，其中置顶的就是他们的回忆录，也就是参加信息学竞赛的整个高中的心路历程，让我不由自主想起了自己的信息学竞赛的生活，有感而发，于是我也准备写一个自己的回忆录。如果说沙子写字，风轻轻一吹就能吹散，纸上面写字，稍微的烈火焚烧就能让踪迹悄然而逝，也许，写在网上，这份记忆才不会消失，成为一种永生的纪念（大概是有区块链？）。\n\n那时的生活，说累也累，说苦也苦，但是不知为什么，我总是不由自主回忆起高中、初中的生活，回忆起那段信息学竞赛的日子，这大概是我人生中最特殊的经历之一。我不知道它对我的生活产生了多少的影响，然而它的确是不可缺少的。\n\n## 第一章 阿克琉斯的脚跟\n\n我并不类似于 $Dew$ 和 $wjyyy$ 学长一样，是从高中开始学的信息学竞赛，我开始学信息学竞赛是在初一的时候。那个时候的我刚从小学升上来，小学执行的是素质教育，因此每周都会有社团活动时间，我对计算机有兴趣，便参加了计算机相关社团，学习了许多计算机的基础知识，比如word、ppt、Excel、VB以及Scratch的使用，算是对编程有了初步的了解。其实我一开始并不打算学习信息学竞赛，而是想学VB，做着自己喜欢的程序，做出一些好玩的东西，用来取悦自己。然而，当时的电脑并不支持我去这么做。想要安装VB完整版本的我在10年代的电脑上根本没有什么操作空间，只有一步一卡的漫长等待。而六年级的暑假我又是在没有网络的老家度过，感受这淳朴的乡土情怀，只能玩着MC度日。比较有意思的是，在没有网络的日子里，我靠着局域网联机，竟然在游戏中搭建出一个不小的天地，后来我又通过命令方块建造了一个可以自由传送的城镇，也许编程的思想来自于那里。后来升上了初中，开始了第一次摸底测试，在全校学生中拿到了前五十名，于是乎不知道为何发了6000元作为奖励，至此，我的信息学奥赛之路似乎被开启了。\n\n当时的电脑不行，我的家长于是就用这6000元给我买了一台台式机，这台台式机在现在看来配置不可谓不低，而且现在已经用起来很卡顿，但是对于当时的我来说，已经是一台非常流畅的机器了。拿到电脑了之后，我安装了VB完整版本，准备左手参考书，右手鼠标，两只手一起打键盘的时候，家长拿着两本厚厚的书出现了。\n\n这两本书就是至今也脍炙人口的《信息学奥赛一本通》和《信息学奥赛一本通习题》，相信大多数自己入门，或者弱校起步的同学，竞赛启航的第一站便是这里。两本书放在了我的桌面上，家长跟我说，编程不仅仅可以使用VB编程，也可以使用C++对程序进行编写，相对于VB，C++在应用面上更广。而且，现在有信息学奥赛，省一就能获得不小的高考优惠，因此学这个竞赛也许是有好处的。小小的我有着大大的梦，像所有小朋友一样，年少时候的梦想很多都是当科学家，而当上首屈一指的科学家，就必须要考上清华大学、北京大学这两所中国的TOP2，小学时候成绩还算优异的我理所当然也有着这样的梦想。\n\n因为初中是一所寄宿学校，在学校并没有机会接触电脑，所以追求的只有那文化课的好成绩，每到周末的时候，我一回家就会快速写完作业，然后开始那信息学竞赛的进程。因为是自主学习，而且每周都会有遗忘的时候，在初一上学期我便重复在那顺序分支循环等逻辑结构中循环。我当然也知道每周过去都会忘记一些东西，所以我每周也反复练习，渴求达到熟练。\n\n然而初一上的期末，因为考试题目过于简单，自己马虎过多，考出了一个自己在高中时候也不怎么见过的排名。那是我第一次考出这么差的名次，然而当时的班主任倒是非常友善，”偶尔考不好无所谓的，找清楚原因”。在初中的第一个学期，我含着泪离开了学校，这时候信息学也一再耽搁，仅仅止步于顺序分支循环了。\n\n在寒假的时候，也许是临近过年了，气氛逐渐喜庆起来，我也将期末考试的悲伤抛之脑后，在电脑上开始了新一轮的信息学竞赛练习。父母非常看好我，我从小便喜欢计算机，在小学获得了大大小小的有关计算机的各种奖项。特别有印象的是全国中小学生计算机表演赛，如果没记错的话，小学课文里那篇启发我计算机重要性的文章的事件也是同一个事件。邓主席说过：“计算机的普及要从娃娃抓起”。在这寒假中，我便去认真学习循环，一维数组，二维数组等，直到达到熟练为止。\n\n时间恍然而过，白驹过隙一般寒假已经过去，这个时候意想不到的事情发生了：我们进行了重新分班！从素质教育的小学升上初中的我并不明白这样的含义，也觉得这样非常残酷。为什么我们这些学生就不能和一开始认识的同学安安稳稳共同陪伴三年呢，为什么非要把各个成绩好的学生集中在一起，而不是分开各领风骚呢。我第一次经历分班，我并不理解。但是我所做的，我能做的只有接受，那天晚上，原先的班主任跟我的父母电话打了好久，我却处在一种不自觉的震撼之中，没有情绪波动，内心仍然认为下个学期的周边还是那群同学，而我会在熟悉的环境中启程我的新的一个学期。\n\n终于开学了，我来到了新的班级，尽管非常想念原来的同学和老师，但这也是我无可奈何的事情。我来到了初中最好的班级，这里高手如云，每个人似乎都跟自己一样厉害，甚至还有不少人比自己厉害的多。在原先的班级中待惯了，现在就会感受到不小的落差。幸亏在班级中又认识了不少新朋友，让我怀古的心情好了许多。为了不丢原先班级的脸，我基本上努力学习，本想着能够保持中正的排名，但是意料之外，第一次考了一个非常好的成绩，是我初中的最好成绩，年级14名，和我的好朋友 $tjs$ 的成绩仅差0.5分，这是我第一次分数接近他，也是最后一次，从此之后我就再也没有考过他，我也非常钦佩他的努力。在这个学期中，对于信息学我是疏于练习的，现在想来原因可能是当时刚分入一个新的班级，心情还没有调整过来，我一心投入到文化课上，也正是因此，这个学期我文化课基本没有掉过队。\n\n在这个学期除了 $tjs$ 是我认识的成为好朋友的同学，另外一个一直深刻影响我直到大一的同学也正是跟我同一个班级，为了个人隐私，这里我暂且把她叫做 $w$ 小姐。当时我的身高在班上的身高并不能算高，于是在有排队的情况下，我排到了 $w$ 小姐的旁边，排队走路时，有时她谈论着周末父母做小龙虾吃的快乐，有时她诉说着原来班级班主任的好。我想，原来我们都差不多，都是从一个普通班通过成绩考上来，而最终被分到这个班级，我们都念着原先的班级，不可谓不是身在曹营心在汉。我跟她总是去和自己原先的班主任聊天，一来一去也是多次偶遇，就这样熟悉了起来。\n\n当然，一个学期又很快的过去了，这个暑假我先是前往了美国游学，参观了各种美国著名的景点，了解了美国的学校然然，然而我觉得其实大城市美国都跟中国差不多，而中小城市更是中国好一些，主要是因为我觉得美国的网络太差了。回来之后，我暑假面临着我即将要参加的第一次正经的联赛即 $NOIP2018$ 。\n\n然而我已经一个学期没有怎么碰过信息学了，那个时候的我因为要参加这样的可以对考大学有好处的比赛而奋发昂扬，重新拿起了微微落了一层灰的《信息学奥赛一本通》，对着电脑敲打起来。不管是分支，还是顺序，还是循环，我都逐渐重新熟悉起来，甚至到当时还在的一个题目网站刷题打榜单（这个OJ似乎现在已经消失了），我剩下的暑假便花费在这个上面。\n\n初二上学期以一种及其戏剧的形式来到了我的眼前，又是在放假的最后一天，我们被通知了又一次分班，这次是自招班。众所周知，襄阳四中五中为了防止每年华师一从襄阳的中小学掐走大量的尖子生，便在华师一的招生考试前面举办了自主招生考试，这一批240个同学可以不用中考直接进入四中五中，招生一般是每年的四月份，也就是说，可以比同一届的同学早4-5个月进入高中，省去那繁琐的重复的无所意义的中考备考，而是直接学习高中课程，并有在学习高中知识的基础上学习竞赛的资格。我侥幸进入了自招班，同时来到自招班的当然有 $tjs$ ，$w$ 小姐也不遑多让，更令我兴奋的是我原先在初一上还在一起的两位室友 $ljh$ 和 $wyx$ 也是进入了自招班。\n\n在初一下的时候，曾有一位疑似喜欢我的女生，每次遇见的时候都会给我打招呼，然而我并无感觉，每次她跟我说话的时候到最后都会加一句喜欢我，毕竟是第一次在对话中被女生直言喜欢，我也是非常害羞，再一个我当时是一个书呆子，提纲掣林得来说，我因为道德与法治课本七年级下册的课程，知道了“青春期的心理萌动是正常的现象，并不是真正的喜欢，而是一种错觉”。我一直坚持这个原则到快到中考的时候。\n\n初二上学期的返校是在一个下午，我们的语文老师没有更换，而其他老师却换了个便，我早早地准备好学习用品和生活用品，从家中出发，来到了新的寝室，整理好床上用品我便前往教室。\n\n作为初二学生，我们换了一个教学楼，我走下宿舍楼，穿过操场，来到了教学楼的楼下，就在我准备上楼的时候，有人突然从我的后方出现，从后面拍了我一下，我一向对外界不是很关注，所以我一回头，听到一阵清澈的声音，一阵沁人心脾的化妆品香气冲进了我的鼻子，定睛一看，原来是 $w$ 小姐，她化妆化得不够成熟，甚至有些过度，然而我不知道为什么她看起来这么好看，我发愣着，思绪在我的脑中打转，思考着种种，但是找不到正确答案的出口。\n\n一句话结束了我的思考。\n\n「这个学期，我们又分到了一个班啊！」\n\n她笑着说着。\n\n我也前言不搭后语的回着。\n\n「是啊，不知道这次的老师怎么样呢」\n\n那我们一起去教室吧。这句话当然没有说出来。但是我们保持着似近非近的距离，一起走到了四层的教室，那是我们八年级共同的班级，八十二班。\n\n## 第二章 埋下种子的因果\n\n再次的分班让我的不适减少了许多，因为是把各个班成绩好的同学都分到一起组建自招班，我初一下班上的大部分同学自然而然地跟我进入了同一个班级，让我有些欣慰，而又多了两个同学，也就是 $ljh$ 和 $wyx$ ，让我的初二生活充满了更多期待。\n\n和 $w$ 小姐，走上楼梯，一进入教室，因为老师还没有正式排座位，于是来得早的我们就坐在了教室的前排，也就是最好听课、最好学习的地方。我坐在进门第一排的中间位置，而 $w$ 小姐就坐在我的左边。\n\n收拾了一会儿自己的暑假作业等等，把书包里的东西整理到了桌子内部，一切都井然有条了，人也舒坦了起来。这个时候 $ljh$ 进入了教室，和他半年没有在同一个班级上的我急忙把他招呼到我的右边坐下，和他开始了闲聊时刻，叙叙旧，并展望着未来。$wyx$ 也随之进入了教室，三人小团体便很快形成了。左边的 $w$ 小姐也也没有参与进话题，随后我就抛之脑后了。\n\n初二的班主任，我暂且称为 $White$ 老师，她走进了教室，在所有人都到齐了之后，为我们开了一次小小的开学班会，大概是有关暑假的过去，以前的班级和未来的目标云云，同学们也是认真听着，想象着自己在班级里大放异彩的样子，我也摩拳擦掌，想象着自己通过自招之后可以提前学习信息学竞赛，完成每个 $OIer$ 的梦想——夺得 $NOI$ 金牌，~~进入集训队，保送清华北大！~~\n\n当然，在寄宿制学校，为学习 $OI$ 所能做的只有平日里好好学习，放假的时候进行短期的培训，从而获得奖项的途径。随着 $NOIP2018$ 的一天天逼近，我在国庆的时候前往了武汉进行集训，是在一个不知名的小机构，名叫深学汇，老师是一个因为获得联赛一等奖而从河北保送到武汉大学计算机学院的人。我们在那里进行了七天的集训，通过这个集训，我看到了很多之前没有学过的东西，对于初二的我是一种莫大的震撼。在这里，我了解到并查集，dfs，bfs，动态规划等等这些我从来都还没有听说过的东西，而在最后一次的模拟测试中，我也完全答对了几道题目，信心立马就上来了。在最后，吃着武汉的火锅，看着华科的光谷体育馆，我坐着家里的车离开了武汉，回到了初中学校继续上课。\n\n通过这次集训，我对信息学更加感兴趣了，与之而来的是对自己的自信过度，这将会在很久以后得到体现。\n\n回到了初中，时间照常流动着，我依旧按照学校认真学习文化课，放假努力学习信息学的劲头进行着每日的安排，这个时候突如其来的月考再一次打断了我的思绪。在组成自招班之后的第一次月考，我考出了继初一上期末考试的第二差成绩。我原本以为那是我最后一次会考那么差，然而第二次还是发生了。因为班级只有40多人，这个名次显然会在每个学期的分班中对我的去留产生影响。已经和熟悉的同学待了那么久了，我不想分开！自己安慰着自己，控制着情绪不去消极，然而成绩单发下来张贴在教室前面的时候，我的内心还是咯噔了一下。这个我的名次从熟悉的中上方看不到了，名字跑到了成绩单下面去，几个一百多名次的黑字在白色的纸张下显得更加突兀。\n\n这个时候一个同学走过我的旁边，似乎是一个认识我但是我不熟悉的同学，看着成绩单。\n\n「xx，你不是很厉害吗，这次怎么没考好？」\n\n同学的疑问更是把我的内心压到了地底下。我好想逃离这个地方，我暂时不想出现在这个教室，我不想在上课的时候听老师讲卷子，我不想，我不想，我不想！\n\n「xx他只是发挥失误了，偶尔没考好罢了，马有失蹄，人有失手罢了。」\n\n这个时候，$w$ 小姐不知道为什么出现在了成绩单旁边。\n\n「没事，下次加油吧！」\n\n她说着这句话对我笑了一下，不知为什么，我似乎不再那么想要逃离这个教室，我的心跳似乎加快了几拍，感觉到有些害羞，转向自己的座位，头也不会地小声说了句谢谢，回到了座位上。内心的天气也从万里乌云变成了万里无云，或许我真的要加油了，~~为了还跟她一个班级~~，不对，是为了最终考过自主招生，提前进入四中五中，通过信息学完成自己的梦想，~~逐梦清北，名震鄂西北~~！\n\n此后，神奇般的，在初中，我就没有再考过这么差的成绩。\n\n## 第三章 守株待兔第一撞\n\n不过该来的还是要来的，随着日期的临近，我在襄阳五中大成楼比完了 $NOIP2018$ 普及组的初赛，并顺利进入了复赛。当年的复赛是和 $wuxianucw$ 等一众学长一起进行的，不过那个时候我还不认识他们。我坐上了前往武汉的火车，因为是第一次参加这样的奥林匹克信息学联赛，所以我还是非常兴奋的。在火车上，虽然心理上很紧张，但是第一次参赛还是故作镇定，仿佛自己已经胸有成竹，优势在握。经过一个多小时的高铁和四十多分钟的地铁，我们也许到达了光谷站，但是一下车，看到的不是现在的光谷大转盘等云云，而是一个又一个工程栅栏遮遮掩掩着，似乎在进行着非常宏大的工程。走过一道道沟壑纵横的路，我们终于走到了先前预定的酒店，也是襄阳五中的信息学教练 $f$ 老师带学生来比赛常来的酒店。不过走出铁栅栏之后，我才看到，墙上写着工工整整的几个大字“**奋发向上，迎接2019武汉军运会**！”，那时的我，还有没有意识到这个事情将会是我从初中到高中人生的转折点，也是一个百年未有之大变局。\n\n来到酒店，我和家长住一个房间，由于是比赛的前一天，所以我拿出了笔记本电脑，打开了一些考前注意以及考前必掌握知识点的贴子看了起来，特别注意的是深学汇的老师在集训时候特别强调的要会写文件的输入输出，这样才能在这个 $OI$ 赛制中成功测评，并拿到自己应有的分数，在复习完这些之后，我拿起了《信息学奥赛一本通》，以求新的突破，从而应对可能会出现的题型。就这样，我翻到了这样一道题目开始做。\n\n> 给你 `k` 枚相同的鸡蛋，并可以使用一栋从第 `1` 层到第 `n` 层共有 `n` 层楼的建筑。\n>\n> 已知存在楼层 `f` ，满足 `0 <= f <= n` ，任何从 **高于** `f` 的楼层落下的鸡蛋都会碎，从 `f` 楼层或比它低的楼层落下的鸡蛋都不会破。\n>\n> 每次操作，你可以取一枚没有碎的鸡蛋并把它从任一楼层 `x` 扔下（满足 `1 <= x <= n`）。如果鸡蛋碎了，你就不能再次使用它。如果某枚鸡蛋扔下后没有摔碎，则可以在之后的操作中 **重复使用** 这枚鸡蛋。\n>\n> 请你计算并返回要确定 `f` **确切的值** 的 **最小操作次数** 是多少？\n\n 现在来看，这道题目就是一个典型的使用二分加上动态规划的算法，但是对于当时的我而言怎么也看不明白，只能强行对着代码进行理解，祈求得到一丝慰藉，然而，我没有继续看下去，把这道题目抛之脑后，去吃饭了。\n\n除了高中的学长学姐，与我一起同行的还有一个跟我同年级的初中生，她也是学习信息学竞赛的，这里暂且叫她 $l$ 同学。 $l$ 同学和她家长也随着 $f$ 老师的带领前来，住进了酒店，现在也是吃饭时间了。论资历来讲，她似乎比我早参加一年比赛，在 $NOIP2017$ 也取得了不错的成绩，不过还是吃饭比较重要。下楼吃饭，是一个吃小碗菜的餐厅，吃的多少全凭你拿了多少，也并不会像那种专门的炒菜一样很贵，我大抵当时吃了一些鸡蛋羹，虽然记忆已经模糊，但是当时第一次吃这种小碗菜的新奇感觉还回荡在我的记忆中，虽然我现在不喜欢吃小碗菜了就是说。吃完饭之后，我便回到了酒店，看了几道题目和注意事项，以及对拍的方法之后我就去睡觉了，长夜虽漫漫，仍然不如第二天早上的闹钟来得快。\n\n第二天早上，为了准时参加比赛，我早早地起了床，前往了湖北省信息学联赛复赛的指定比赛地点的华中科技大学，走进南大门，迎面而来的是太阳照的雪亮的毛主席像，一只手举起来，面向着前面的一条大道，似乎正在招手迎接着更加美好的未来，明天会更好。\n\n往前走有一个大条幅，上面写着这次比赛的名称，内心也逐渐激动起来。首先在名单上找到自己的名字和机房，然后背着包走上楼，找到自己坐的位置。其实在前一天， $f$ 老师已经带我们来看过机房了，这里的环境跟我想象中的大学很相似，一排排的电脑坐着一排排的人，在激烈地打着代码，有一个似乎是保送华科的学长来到了旁边，和老师欢快的聊着，也许到时候我如果能保送的话，我是不是也会像他们那样呢。想着想着，回到现实，我坐在比赛的赛场上，老师走进来，开始公布密码，我有条不紊的按照老师说的密码一个一个输入。\n\n好！顺利解压！\n\n打开文件之后，点开pdf，开始读题！\n\n认真读题，第一题是标题统计，是一个很基础的题目，直接秒了，不过要注意一下输入输出的问题。好的\n\n第二题是龙虎斗，似乎是一个模拟的题目，只需要考一考模拟，那就模拟一通吧。\n\n第三题是摆渡车，似乎有点动态规划的感觉，但是我当时又没有把动态规划学会，那就继续模拟一通吧。\n\n到第四道题目，我才是完全不会了，因为当时集训和我自己学到的内容都还没有到树的时候，那没办法了，只能去找一找特殊性质，看看能不能特判掉算了。\n\n最后，随着暴力的进行，不会的题目的性质分析中，四个小时结束了，这也意味着 $NOIP2018$ 结束了。出考场的时候，我自认为我所做的题目都做了出来，暴力也都打满了，嗯这样就满意了吧。在一层往外看，武汉的天空下着哗啦啦的大雨，最终坐上了地铁二号线，走进前往汉口的车，最终回到了襄阳，因为第二天还要继续上学，那初中生活。\n\n细说这次比赛，我大概率是准备不充分的，不仅仅是因为初一下基本什么都没有学，还因为自己对一些有关信息学的问题不求甚解，只是关注于某道题目的解法而没有达到举一反三的效果。更重要的是，我开始意识到，信息学并非像我想象的那么简单，前面到底有多少艰难与困苦呢，我不知道，我也不想知道，我只知道我参加这个比赛似乎就能在高考中取得什么加分优势，或者是一条实现我的梦想的捷径。我向来是那种喜欢耍小聪明的人，除非有必要，否则我不会去费劲心力去搞一个自己或许没有那么感兴趣的东西，到底是因为困难导致了我的困扰呢，还是困扰造就了我的困难，一切都不太明朗，赛后等待成绩的一个多月总算是过去了，到了揭榜的时候。\n\n奇迹般的，我的暴力能打满的都打满了，不会写的也稍微混了点分数。最终我获得了一个不高也不低的分数，也是成功拿到了普及组的一等奖。这是必然呢，还是偶然呢，不管怎么样，这一次获奖给当时的我了一针强心剂，也许我是有天赋的，也许我能成功拿到那个万众瞩目的金牌。也就是这第一次获奖，让后面的我继续学习信息学竞赛，也正是这一次获奖，让我学信息学竞赛学到了最后的时刻。\n\n## 第四章 无才扶我凌云志\n\n获得了普及组一等奖，按理来说并非非常困难，但是我却有些小小的自得酝酿在心中。然而，回到第一次月考的打击仍然在后续呈现缓慢恢复的状态，当然，没有再考过那么差了，考得越来越好了，但是仍然不足以留在这个班级。 $White$ 老师的特点是考试之后换座位，然后换座位的原则是按照每次月考排名分数的先后顺序选座位，然后再按照一个月为周期的轮换制来让所有的组都有坐在前面和坐在后面的机会。第三次月考结束后，一扫第一次月考的不景气，我，终于也要往前一点选座位了。选座位靠前的总是那些成绩比较好的同学，他们稳稳地排在前面，丝毫不动摇，当然也包括 $w$ 小姐，时常出现在年级前十的位置。位置一个个被选走，就像是用于缓冲的泡沫的泡泡一个一个被捏破，剩下的座位就像是不好捏的了，我考得中间，这个时候前面的座位就已经所剩无几了，于是我看到了 $w$ 小姐后方还有两个位置。这两个位置，我到底选不选！我想选，这样的话或许能和她的座位近一点，但我不知道为什么想跟她的座位近一点，等到我反应过来的时候我已经把我自己的名字写在她名字的右下方了，为什么是右下方呢，我想可能是害羞吧。当座位开始调动的时候，我左边的同学应该是 $bc$ 同学，如果是这样的话，我不知道当时用什么说服了他跟我换位置，但最终的结果是我换到了 $w$ 小姐座位的后方，倒数第一排的位置。这样的日子也许给我每日的生活带来了一点新奇，我初中的时候英语很好，时常背诵前列背完，因为她成绩好，她是组长，组长需要听组员背书，因此我每次都迅速背完，然后在她面前兴奋得背书，或许我背的书是她听我当面说的最多的话。不过她每次都要听背书的时候似乎总有些不耐烦，也许是她自己也需要在有限的时间内背下来，为了节省时间的目的吧。时间过得很快，凛冽的冬日随着太阳直射点的南移更加寒冷，我们校内举办了元旦联欢晚会，这也意味着，新的一年到来了。我们班级的节目是一段暗光舞，人身上贴着的亮条可以控制亮灭，然后把环境的灯全部关掉的话，就可以通过控制亮灭的方式实现类似于人形瞬移的效果。我们班级另外一个节目是有关校园的小品，我的好朋友 $ljh$ 和 $wyx$ 也参与了小品的表演，让现场的观众们难以忘怀，初中的校长也点名表扬这个节目。在节目表演期间，常有排练的时候，这个时候同学们一般有自习，我突发奇想，想到自己的校园卡里面还有一些买零食的钱，而我想请 $w$ 小姐吃零食，我就把校园卡给她说，你要不要一些零食呢？我可以请你哦！然而最后她并没有要，把卡还给了我，不知为何，我突发的奇想不禁让我感到失落。\n\n在元旦晚会之后，很快就面临着期末考试了，对于这次期末考试，我很认真的复习，但是因为题目有些简单，大家的分数都差不多，区区几分就可以相差很多名次。而我与众不同的是，我在这次考试的英语中拿到了樊城区那套卷子唯二的满分，不知道是不是因为题目太难了，其他人的分数都考得不是很高，然而一向都考得很好的 $w$ 小姐似乎考得不太好，于是我想着，想用她安慰我的方式安慰她，但是当我想说话的时候，不知道什么噎住了我的喉咙，让我说不出一个字，她和我之间，只有令人窒息的沉默。期末考试结束，紧紧张张担心着分班被分走，此时此刻，我们的年级主任 $h$ 老师为我们下了一颗定心丸，这个学期不分班，也就意味着下个学期会分班。但是我管不了那么多了，期末考试的结束意味着寒假的到来，意味着我即将开始下一段信息学奥赛的集训。\n\n2019年的北京没有雪，有的只是那瑟瑟的寒风，太阳如医院的冷光灯一样照在头上，令人做涕。这个寒假，来到的集训地点是清北学堂，而培训地点是位于北京的华北电力大学计算机房间。我因为是普及组1等奖，因此家长为我选择的是普及升提高的训练营，当时刚入营的我还想着自己一定可以全部学会，大杀四方，然而我错了。在当时的培训中，老师讲知识点的速度非常快，让我来不及听，只有初中数学基础的我根本听不明白。往上是最近公共祖先LCA的倍增做法和Tarjan做法，往下是中国剩余定理赖以生存的基础最小公约数，虽然有少数听得懂，但是大部分听不懂已然似乎是定局了，第四天和最后一天的模拟比赛便说明了这一点。也许最后，我获得的知识只是我不会和我要学以及那里的饭很难吃等云云了。\n\n至今记忆深刻的只剩下当时休息时候玩国际象棋，看西游记续篇以及当时模拟赛的时候做的一道题目了。\n\n题目如下：\n\n> **排队的小鸟**\n>\n> **题目背景**\n>\n> 有 n 位小鸟曾在排队，可是她们走散了。\n>\n> 每个小鸟有一个编号，她们想要恢复原先的队列，可是她们几乎忘了她们之前站在哪里了，甚至忘了自己的编号！\n>\n> 小鸟们并不聪明，但是她们还记得前面的小鸟和后面的小鸟的编号，如果前面或后面没有小鸟了，她们会记录一个 0。\n>\n> 请你帮助小鸟们重新把队伍排起来吧！\n>\n> **输入格式**\n>\n> 第一行一个正整数 N。\n>\n> 接下来 N 行，每行 2 个整数,表示某一位小鸟前面的小鸟和后面的小鸟的编号。\n>\n> **输出格式**\n>\n> 一行 N 个数，是小鸟的编号，表示小鸟的排列。\n>\n> **说明/提示**\n>\n> 对于 30%的数据， 1 ≤n≤ 10。\n>\n> 对于 50%的数据， 1 ≤n≤ 100。\n>\n> 对于 100%的数据， 1 ≤n≤ 2 × 10e5。\n>\n> 小鸟的编号保证不重复，在 int 范围之内。\n\n当时的我还不太会打 $Markdown$ 的语法，但是这道题目我当时着实是没有做不出来，现在来看，只要使用链表的方式就可以把所有小鸟的排序排出来，从而获得满分的成绩了。而我旁边的一个同学倒是实打实做出来了。从这里，我看到了差距，旁边的同学我似乎已经记不住他的名字了，但是我知道他是一个东北的同学，因为他的初中是四年制。后面的几道题目看了看回顾，也总归是动态规划的做法和线段树的做法，然而虽然现在看起来简单，当时的我确做不出来。\n\n这次培训给我带来的最大收获是什么呢，我不太知道，但是我看到了很多我没有见过的算法，我看到了很多厉害的人，我也见识到了我的知识是多么的浅薄。虽然侥幸得了普及组一等奖，但也只是因为在湖北这个弱省的结果，如果是在浙江之类的强省呢？或许不知道结果，但是这个成绩确确实实给了我一种自己有机会的假象，让我不断努力下去，但是这个东西也确实看天赋，仅仅是努力是没有作用的，也许一直干下去并非一种好的结果。那么，抱着这样的信念，我该继续下去吗？\n\n拿着手上的笔记本电脑，是父母因为我获得了普及组一等奖给我买的，也许我应该坚持下去，但是就我的实力而言，我是否能够追寻到那个所有 $OIer$ 的梦想呢？时间容不得我继续想，我能做的只有前进，无论前面到底是一片黑暗还是一片光明，我或许都该燃烧着内心的火焰英勇向前吧。过完了寒假，父母联系到了一位老师，是北京的老师，他比较熟悉北京的信息学竞赛，而且北京的教育资源当然比襄阳更好一些，于是我就每个周末和一个浙江的同学开始和他一起学习信息学了。\n\n对方听说了我获得过普及组的一等奖，因此先入为主认为我有一定的水准，其实湖北省的一等奖是有点水的，仅仅是做对了一道题目的多一点就拿到了一等奖。周末开始学习信息学，老师一开始就开始讲DP的专题，DP即动态规划，无后效性，对于当时的我来说，我不怎么听得明白，但是我也是看着题目一点点做，对于0/1背包、多重背包等简单问题当然是能够信手拈来，熟能生巧，但是越到后面越是困难。比如，为什么0/1背包可以把二维的状态压缩成一维的啊，因为数组更新的先后可以不需要一些数据，让空间利用率更高，但是如此简单的理论对于当时的我来说就是想不出来为什么。再比如环形DP等等，到底是把环直接拆成双倍长度的链呢，还是直接取余呢，两种做法也迟迟没有弄清楚，只弄了个模棱两可，把例题做对了，再往后我就知其然而不知其所以然了。\n\n初二下当然是一个比较好的学期，没有发生什么比较重大的事情，成绩也比较稳定，破除了初二上的定律，物理数学之类的学科也学得越来越好。然而正是因为寄宿制学校，信息学学的东西倒是常常遗忘，又因为每个月一次要进行的月考，信息学便常常耽搁。我也不知道几月几日，在今天的一次函数和明天的不等式中徘徊，我也不知道为什么要这样，只知道学着学着也许就会有美好的将来。在大众的赛道上越走越远，在小众的独木桥上踽踽独行，到底哪个才能更快的到罗马？\n\n","categories":["游记"]},{"title":"wweiyi-first-blog","url":"/2025/07/08/wweiyi-first-blog/","content":"\n今天我重新创建了我的hexo驱动的blog，希望以后这个blog能够时常更新吧\n\n"},{"title":"NOI Online2021游记","url":"/2021/03/30/NOI-Online2021游记/","content":"\n# NOI Online2021第一场游记 \n\n## 0 准备\n\n2021年3月27日，一个美妙的早上，我从我温暖的被子里滚了出来，走向了襄阳五中的大门，内心踌躇：这可能是我最后一次参加NOI Online了，但是不慌，因为此时学校俨然已经被封锁起来，变成了公务员考试的考场。我从旁边的宁静湖穿过，走了比平时多了大约一倍的路程直达我的竞赛场地——大成楼，有诗云：\n\n> 大成楼里集大成，大成楼旁小成生，小小大大大小成，小小大大状元名\n\n好不费力，我从一层的电梯起步坐到五层，然后从五层的楼梯爬到六层，这里就是我所在的机房了——只有我一个人的机房。\n\n拿出钥匙，我将钥匙插进门锁中，前后倒腾了几下，门开了，里面散发出一股灰尘的味道，我知道，是我上周月考，没有进去，已经布满灰尘了。进门之后向右转，打开电闸，快到墙的时候，又及时来了一个急转弯，走到倒数第二排的电脑面前，放下书包，踱到窗旁，打开窗户，迎接这一天的新鲜空气。\n\n电脑缓缓地开了，滴滴的声音和窗外公务员考试的广播声加大了比赛的真实感觉，让我身临其境，\n\n时间一分一秒地过去，终于到了8：30这个时间，题目发下来了\n\n## 1 提高组\n\n### T1 愤怒的小N\n\n鸽\n\n### T2 积木小赛\n\n这道题目开始看的时候没有思路，以为是动态规划，但是怎么找状态转移方程就是找不到。\n\n下来之后，参考了别人的题解之后，这道题用hash最方便！\n\n~~然而我没学~~\n\n接下来来介绍一下哈希\n\n>Hash算法可以将一个数据转换为一个标志，这个标志和源数据的每一个字节都有十分紧密的关系。Hash算法还具有一个特点，就是很难找到逆向规律。\n>\n>Hash算法是一个广义的算法，也可以认为是一种思想，使用Hash算法可以提高存储空间的利用率，可以提高数据的查询效率，也可以做[数字签名](https://baike.baidu.com/item/数字签名/212550)来保障数据传递的安全性。所以Hash算法被广泛地应用在互联网应用中。\n>\n>Hash算法也被称为散列算法，Hash算法虽然被称为算法，但实际上它更像是一种思想。Hash算法没有一个固定的公式，只要符合散列思想的算法都可以被称为是Hash算法。\n>\n>——摘自《百度百科》\n\n我们常见情况下我们可以使用一种简单的方法做哈希，就是对于一个数据，我们可以对它的某一位进行一个**误差非常大**的处理，然后使这个值与相距很近的数的差值非常大，以区分开，这样哈希也是比较好理解的\n\n对于这道题目，$n\\leq 3005$，我们需要一个$O(n^3)$以内的算法\n\n观察字符串s，t，实际上就是：\n\n对于所有的s的子序列，t的子串，求他们子序列和子串相同的个数\n\n对于s的子序列，我们有$2^n-1$种情况，但是对于t的子串，我们的情况就少了许多，所以我们从t开始下手\n\n我们可以枚举一个t为$i到j$的子串，然后查找s串是否具有相同的子序列，如果具有的话，就可以将这个子串变成一个数存进去，作为一个hash值\n\n最后，运用C++中的函数unique统计一下不同元素的个数，就是本道题的答案了\n\nPS：不同元素的个数=unique(a+1,a+1+n)-(a+1);\n\n参考自：[**syksykCCC**](https://www.luogu.com.cn/user/51971)\n\n#### 代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\n#include <string>\n#include <vector>\n#include <map>\n#include <queue>\nusing namespace std;\nconst int HASH=52717;\nconst long long M=2004090320050915;\nint n;\nchar s[3005],t[3005];\nlong long hash[3005*3005],tot;\nint main()\n{\n\tcin>>n;\n\tscanf(\"%s%s\",s+1,t+1);\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tlong long v=0;\n\t\tint p=1;\n\t\tfor(int j=i;j<=n;j++)\n\t\t{\n\t\t\twhile(p<=n&&s[p]!=t[j])\n\t\t\tp++;\n\t\t\tif(p>n)\n\t\t\tbreak;\n\t\t\tp++;\n\t\t\tv=(1LL*v*HASH+t[j]-'a'+1)%M;\n\t\t\thash[++tot]=v;\n\t\t}\n\t}\n\tsort(hash+1,hash+tot+1);\n\tcout<<(long long)(unique(hash+1,hash+tot+1)-(hash+1))<<endl;\n\treturn 0;\n}/\n```\n\n### T3 岛屿探险\n\n鸽\n\n## 2 停顿\n\n提高组的考试考完了，我收拾了一下东西就去食堂吃饭了。由于公务员考试的原因，学生都放假了，食堂自然没有很多的菜品。去食堂的路上，来公务员考试的考生们都在坐在树荫下，坐在台阶上，自己复习自己的东西，还有一些人在食堂里学习，所有人都是如此努力，所有人都是那么精神，所有人都是充满激情。\n\n鲁迅说过：“人类的悲欢并不相通。”我走进食堂，看见其他学科竞赛的同学们已经坐在那里了，我赶忙打了饭，然后坐了过去，讨论着月考的事情，你一言我一语地说着，我开始抱怨自己在月考中的生物，我只考了65分，但是对着答案我却不止，我错误地将考差的原因归咎于批卷老师了，后来我才想通，最主要是自己的问题，导致了总排名的落后。\n\n吃完饭，我们回到了寝室，有几个人说要玩狼人杀，我就在看着，但是他们许久没玩，看着手表，已经来到了12：35分，他们快要静校了，我也赶紧回到了机房。机房还是那个味道，我一通熟练操作，拿起了电脑上的耳机放入耳朵，然后从我最喜欢的音乐《Luv Letter》开始播放起来，音乐忽然刺痛到我心灵的某个地方。却说不出来，又一曲《风吹过的街》，唤起了我内心中美好的回忆，却现在已然悄然逝去。\n\n听着听着，看着看着电脑，打着打着模板，时间很快就到了14:30，是考入门组试题的时间了，我照例下载了试题，打开了试题。\n\n## 3 入门组\n\n### T1 切蛋糕\n\n这道题开始的时候一看，没有思路，阿这，这不是入门组的题目吗，我已经菜到这个程度了吗，我仔细一看，原来是找规律的题目，蛋糕最多切，不可能超过三下，但是怎么分类呢？我们可以看一下样例数据。\n\n#### 输入样例\n\n```\n6\n0 0 8\n0 5 3\n9 9 0\n6 2 4\n1 7 4\n5 8 5\n```\n\n#### 输出样例\n\n```\n0\n2\n1\n2\n3\n2\n```\n\n**首先分析一个人的情况**\n\n从第一组数据，同时我们根据一个数据的特殊性质\n\n30%的数据满足$a=b=0$\n\n也就是第一组数据，因为其他两个人是0，所以只有一个人，自然也就不需要切分蛋糕了，所以特殊判断一下，如果三个人中有两个人是0，就直接输出0\n\n**接下来我们来分析两个人的情况**\n\n从第二组和第三组数据，同时我们根据数据的另一个特殊性质\n\n60%的数据满足$a=0$\n\n也就是说，只有两个人参与分蛋糕，这肯定好分啊，不就切一下就好了吗？\n\n不不不，不要着急，这里还需要分为两种情况：\n\n第一种情况：两个人的不同\n\n如果两个人的不同的话，我们肯定不能只切一次，所以切两次就一定可以成功了，因为第二次我们可以把蛋糕切成任意比例的二分，所以当两个人的不同的话，就直接输出2\n\n如果两个人的相同的话，那就更好办了，只用输出1\n\n**接下来我们来分析三个人的情况**\n\n根据三、四、五组数据，我们发现：\n\n1.如果其中两个加起来等于第三个的话，我们就可以只用先切一下，再切一下，因为实际上就是将蛋糕二分之后，再分一次，这样的情况，我们可以直接输出2\n\n2.如果其中两个相同的话，也只用切两次，第一次将蛋糕分成两个部分，第二次再切的时候，我们无论怎么切（除了重复切一个地方），总是可以在四块蛋糕中找到两份相同的蛋糕，然后另外的部分就可以随意控制大小了\n\n3.最后，如果这个数据没有以上的所有性质的话，也就是说，我们切两次已经不能完成了（想一想，为什么），切完两次之后，再切一次，就可以把蛋糕分成任意的三比形式了\n\n#### 代码\n\n```cpp\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\n#include <string>\nusing namespace std;\nint T;\nint main()\n{\n\t//freopen(\"cake.in\",\"r\",stdin);\n\t//freopen(\"cake.out\",\"w\",stdout);\n\tcin>>T;\n\twhile(T--)\n\t{\n\t\tint a,b,c;\n\t\tcin>>a>>b>>c;\n\t\tif(a==0&&b==0||b==0&&c==0||a==0&&c==0)\n\t\tcout<<0<<endl;\n\t\telse if(a==0)\n\t\t{\n\t\t\tif(b==c)\n\t\t\tcout<<1<<endl;\n\t\t\telse\n\t\t\tcout<<2<<endl;\n\t\t}\n\t\telse if(b==0)\n\t\t{\n\t\t\tif(a==c)\n\t\t\tcout<<1<<endl;\n\t\t\telse\n\t\t\tcout<<2<<endl; \n\t\t}\n\t\telse if(c==0)\n\t\t{\n\t\t\tif(a==b)\n\t\t\tcout<<1<<endl;\n\t\t\telse\n\t\t\tcout<<2<<endl;\n\t\t}\n\t\telse if(a+b==c||a+c==b||b+c==a)\n\t\t{\n\t\t\tcout<<2<<endl;\n\t\t}\n\t\telse if(a==b||a==c||b==c)\n\t\t{\n\t\t\tcout<<2<<endl;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tcout<<3<<endl;\n\t\t}\n\t}\n\t//fclose(stdin);\n\t//fclose(stdout);\n\treturn 0;\n}\n```\n\n### T2 吃豆人\n\n这道题目实际上跟T1很类似，也有一些找规律的问题和数据范围的查看\n\n数据范围$n\\leq 1000$，我们可以想到$O(n^3)$就可以解决这道题目了，因为电脑一秒钟可以计算大约$10^9$的数量级\n\n我们所要做的就是从两个点开始，他们所经过的路径权值最大和，我们经过试验可以发现，无论选取左上，左下，右上还是右下无论哪一个方向，只要在某一个矩形上，这个矩形永远就是那个矩形，这个矩形的权值也永远不会变，所以我们设置一个数$c_i$，表示从第一行的第i个出发，所经过矩阵的权值和\n\n如何计算$c_i$呢，**我们可以建立两个方向数组**，dx，dy，分别储存方向，这个在二维的方向问题上非常好用，然后按照游戏的规则，设立边界，如果大于边界就反弹，也就是说，横坐标和纵坐标分别满足$x\\in [0,n) $,$y\\in [0,n)$,所以，我们遇到了边界，方向标志dir就加一，这样就可以做到反弹的效果了\n\n将经过的每个点$a_{ij}$加起来，这样就是c所对应矩形的权值和.\n\n算出来矩形对应的权值和之后有什么作用呢，这样我们就可以枚举啦，枚举两个点所对应的矩阵，我们将他们的权值和加起来，再把他们重复的减去，这样就可以通过设立一个答案变量$ans$来记录枚举的最大值。但是重复的怎么办呢？\n\n我们通过尝试以及找规律，我们可以找到如下结果：\n\n1.当$j-i$为奇数的时候，,这个时候两个矩形不会有公共的交点，$ans=c_i+c_j$\n\n2.当$j-i$为偶数的时候\n\n（1）$i=1且j=n$时,两个矩形是线，正好是对角线的交叉，公共交点在矩阵的中心，此时$ans=c_i+c_j-a_{\\frac{n+1}{2}\\frac{n+1}{2}}$\n\n  (2)当且仅当$i=1或j=n$时，一个矩形是线，另一个是矩形，这个时候$ans=c_i+c_j-a_{1+\\frac{j-i}{2}\\frac{i+j}{2}}-a_{n-\\frac{j-i}{2},n-\\frac{i+j}{2}+1}$\n\n  (3)$i不等于1且j不等于1$时，是两个矩形，此时\n\n$ans=c_i+c_j-a_{1+\\frac{j-i}{2}\\frac{i+j}{2}}-a_{n-\\frac{j-i}{2},n-\\frac{i+j}{2}+1}-a_{\\frac{i+j}{2},1+\\frac{j-i}{2}}-a_{n-\\frac{i+j}{2}+1,n-\\frac{j-i}{2}}$\n\nPS：以上参考自@[**水の殤璃**](https://www.luogu.com.cn/user/365107)\n\n这样我们所有的情况都讨论完毕了\n\n预处理一个c，然后枚举一下，时间复杂度$O(n^2)$\n\n#### 代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\n#include <string>\nusing namespace std;\nint n;\nint a[1005][1005];\nint c[1005];\nint dx[]={1,1,-1,-1},dy[]={-1,1,1,-1};\nbool v[1005][1005];\nint main()\n{\n\tint n;\n\tcin>>n;\n\tfor(int i=1;i<=n;i++)\n\tfor(int j=1;j<=n;j++)\n\tcin>>a[i][j];\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tc[1]+=a[i][i];\n\t\tc[n]+=a[i][n-i+1];\n\t}\n\tfor(int i=2;i<n;i++)\n\t{\n\t\tint x=1,y=i;\n\t\tint dir=0,sum=0;\n\t\tbool flag=false;\n\t\twhile(1)\n\t\t{\n\t\t\tif(x==1&&y==i&&flag)\n\t\t\t{\n\t\t\t\tc[i]=sum;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsum+=a[x][y];\n\t\t\tflag=true;\n\t\t\tif(x+dx[dir]<=0||x+dx[dir]>n||y+dy[dir]<=0||y+dy[dir]>n)\n\t\t\tdir++;\n\t\t\tx+=dx[dir],y+=dy[dir];\n\t\t}\n\t}\n\tint ans=0;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\t{\n\t\t\tif(i==j)\n\t\t\tcontinue;\n\t\t\tint maxn=max(i,j),minn=min(i,j);\n\t\t\tif((maxn-minn)%2==1)\n\t\t\tans=max(ans,c[i]+c[j]);\n\t\t\telse if(minn==1&&maxn==n)\n\t\t\tans=max(ans,c[i]+c[j]-a[(i+j)>>1][(i+j)>>1]);\n\t\t\telse\n\t\t\t{\n\t\t\t\tint t=c[i]+c[j];\n\t\t\t\tt-=a[1+(maxn-minn)/2][(i+j)/2];\n\t\t\t\tt-=a[n-(maxn-minn)/2][n-(i+j)/2+1];\n\t\t\t\tif(minn==1||maxn==n)\n\t\t\t\tans=max(ans,t);\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tt-=a[(i+j)/2][1+(maxn-minn)/2];\n\t\t\t\t\tt-=a[n-(i+j)/2+1][n-(maxn-minn)/2];\n\t\t\t\t\tans=max(ans,t);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n\n### T3 重力球\n\n鸽\n\n## 4 尾声\n\n题目终于考完了，简直就是头脑风暴，大脑简直累死了，只会打暴力的我默默地留下了眼泪，看着一无所有的代码，看着一无所有的电脑，看着一无所有的月考成绩，一切似乎都变得昏暗起来。所以我学信息学竞赛的意义在哪里，为什么要学呢，如果不会的话，如果没有天赋的话，学它又有什么意义？\n\n看着代码，看是倒计时走到零，我不禁想起了我的小学同学，是他将我带入了学习计算机的这一条道路，想起了我的小学计算机老师，这是我的计算机启蒙老师，她非常耐心，认为我也非常有天赋，也是她的教学给了我对于信息学的极大兴趣，一路上我参加了YNIT，北京海淀区的知识技能竞赛，北京海淀区的“世纪杯”竞赛，全国青少年计算机表演赛，均获得了很好的成绩。但是在这里，我的天赋似乎被压抑，所有的成绩在这里似乎变得不值一提，也许信息学竞赛和之前的竞赛完全不是一个难度吧，值得肯定的是，我这么去做了，我也努力去做了。\n\n总是有人问我，“你为什么要学习信息学啊”，“为什么不学习其他的竞赛呢”，“信息学竞赛是不是只是玩电脑就可以了”，从广义上说，信息学竞赛对于有兴趣的人来说，的确是一种娱乐，而且使人沉迷其中，无法自拔，同时增长人的知识。为什么不学其他的竞赛啊，因为我喜欢信息学啊，喜欢是没有理由的。\n\n“一旦选择了这条道路。就算哭着走，也要自己走下去。”没错，这次我要走到底。从小时候开始，我似乎都不是一个非常有毅力的人：学前班的时候，我跟着学前班兴趣班的围棋老师学习围棋，没学多久，我便展现出我的天赋，很快就考过了围棋十段，父母老师也觉得我很有天赋，于是准备继续学下去，但是我却在一次又一次与高手对决中，在老师的讲解非常难的情况下退缩了，以自己不想学结束了围棋的学习；又是小学一年级，我在小学报名了奥数兴趣班，我在一次“巨人杯”比赛中拿到了不错的成绩，于是就在某巨人学校的数学尖子班学习奥数，开始的时候父母跟着我学，每次也催促我完成作业，到后来，我就在和爷爷一起去巨人学校学习，没有父母的催促，我也渐渐怠惰了下来，每次去上兴趣班唯一的目的便是去吃楼下飘香的手抓饼——每次加很多鸡蛋，很多火腿肠。渐渐地，我和一位父亲的朋友的孩子渐行渐远，我们原本在一个尖子班，我进入时考试成绩比他高，但是最后我却没有留在尖子班。那天晚上，听着父母的谈话，“没有必要，这些什么数列都是高中的知识，现在学太早了”，在这样的话语中，我在四年级的时候结束了奥数的学习，那位尖子班的孩子通过自己在各个竞赛中的优异成绩进入了清华附中的，而我通过这三年的学习却一无所得。\n\n这是我的问题吗，是的，在某种意义上，也不是我的问题，因为我那时候小，不懂事。但是从二年级开始，我的朋友带着我走的路——信息之路，我却坚持到了现在，既然往事历历已经年，已经快8年了，还在坚持，我又有什么理由放弃呢？\n\n站在人生的交界处，面对越考越差的月考，面对一无所获的竞赛，却有了一个更加屡败屡战的我，这是我的幸运吗，通过NOI Online，也算是成长了一步啊。\n\n公务员考试考完了，同学们从各自的家回到学校，学校又恢复了生机。我背起书包，收拾东西回到教室。路上，每个同学都洋溢着笑容，既然如此，何必每天愁眉苦脸呢？我抬了一下书包，拍拍身上的灰尘，加快了脚步，望向久违的明德楼，望向那个梦想中的远方。\n","categories":["游记"]},{"title":"【解题报告】 Stick","url":"/2019/08/25/【解题报告】-Stick/","content":"\n# 【解题报告】 Stick\n\n## 题目：[木棍](https://www.acwing.com/problem/content/169/)\n\n### 解题思路：\n\n深度优先搜索\n\n我们可以用深度优先搜索简单地做出来，没错，是很简单\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\nusing namespace std;\nint a[100],v[100],n,len,cnt;\nbool cmp(int a,int b)\n{\n\treturn a>b;\n}\nbool dfs(int stick,int cab,int last)\n{\n\tif(stick>cnt)\n\treturn true;\n\tif(cab==len)\n\treturn dfs(stick+1,0,1);\n\tint fail=0;\n\tfor(int i=last;i<=n;i++)\n\t{\n\t\tif(!v[i]&&cab+a[i]<=len&&fail!=a[i])\n\t\t{\n\t\t\tv[i]=1;\n\t\t\tif(dfs(stick,cab+a[i],i+1))\n\t\t\treturn true;\n\t\t\tfail=a[i];\n\t\t\tv[i]=0; \n\t\t\tif(cab==0||cab+a[i]==len)\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn false;\n}\nint main()\n{\n\twhile(cin>>n&&n)\n\t{\n\t\tint sum=0,val=0,m=0;\n\t\tfor(int i=1;i<=n;i++)\n\t\t{\n\t\t\tint x;\n\t\t\tcin>>x;\n\t\t\tif(x<=50)\n\t\t\t{\n\t\t\t\ta[++m]=x;\n\t\t\t\tsum+=a[m];\n\t\t\t\tval=max(val,a[m]);\n\t\t\t}\n\t\t}\n\t\tn=m;\n\t\tsort(a+1,a+1+n,cmp);\n\t\tfor(len=val;len<=sum;len++)\n\t\t{\n\t\t\tif(sum%len)\n\t\t\tcontinue;\n\t\t\tcnt=sum/len;\n\t\t\tmemset(v,0,sizeof(v));\n\t\t\tif(dfs(1,0,1))\n\t\t\tbreak;\n\t\t}\n\t\tcout<<len<<endl;\n\t}\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 小猫爬山","url":"/2019/08/23/【解题报告】-小猫爬山/","content":"\n# 【解题报告】 小猫爬山\n\n## 题目：[小猫爬山](https://www.acwing.com/problem/content/167/)\n\n### 解题思路：\n\n哪家会养这么重又这么多猫，只能说他们两个比较闲的无聊\n\n当然这道题就像猫一样，特别狡猾\n\n开始的时候我看到这个，想到了一个贪心做法，但是不管怎么改，答案就是改不对，经过前思后想，左顾右盼，我知道了真正的算法：搜索+剪枝\n\n恰好就是我不最擅长的算法！\n\n我就按照思路和书中所给的部分代码打出来了，当我提交上去的时候，一个大大的A字亮在我的眼前，没错\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\nusing namespace std;\nint c[20],cab[20],n,w,ans;\nvoid dfs(int now,int cnt)\n{\n\tif(cnt>=ans)\n\treturn ;\n\tif(now==n+1)\n\t{\n\t\tans=min(ans,cnt);\n\t\treturn ;\n\t}\n\tfor(int i=1;i<=cnt;i++)\n\t{\n\t\tif(cab[i]+c[now]<=w)\n\t\t{\n\t\t\tcab[i]+=c[now];\n\t\t\tdfs(now+1,cnt);\n\t\t\tcab[i]-=c[now];\n\t\t}\n\t}\n\tcab[cnt+1]=c[now];\n\tdfs(now+1,cnt+1);\n\tcab[cnt+1]=0;\n}\nbool cmp(int a,int b)\n{\n\treturn a>b;\n}\nint main()\n{\n\tcin>>n>>w;\n\tfor(int i=1;i<=n;i++)\n\tcin>>c[i];\n\tsort(c+1,c+1+n,cmp);\n\tans=n;\n\tdfs(1,0);\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 防线","url":"/2019/08/23/【解题报告】-防线/","content":"\n# 【解题报告】 防线\n\n## 题目：[防线](https://www.acwing.com/problem/content/122/)\n\n### 解题思路：\n\n虽然说是解题报告，但是也是有一部分曲折在其中的，因为这道题开始的时候实在想不到思路，知道看到了某大佬的题解之后才豁然开朗，明白了这道题目的做法，和神奇的思想\n\n这道题的思想很简单，就是用前缀和\n\n你要想，奇数加奇数等于偶数，但是题目中只有一个是奇数的，所以这种情况不成立，那么就只有偶数加奇数或偶数加偶数两种情况\n\n因此我们就做一个前缀和加上一个神奇而又简单的二分，就可以得出正确而又完美有缺的答案了（有缺我也不知道那里缺）\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\nusing namespace std;\nconst int maxn=200005;\nint n,t;\nstruct ar\n{\n\tlong long s;\n\tlong long e;\n\tlong long d;\n}a[maxn];\nlong long judge(long long x)\n{\n\tlong long ans=0;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tif(a[i].s<=x)\n\t\tans+=(min(x,a[i].e)-a[i].s)/a[i].d+1;\n\t}\n\treturn ans;\n}\nint main()\n{\n\tcin>>t;\n\twhile(t--)\n\t{\n\t\tcin>>n;\n\t\tfor(int i=1;i<=n;i++)\n\t\tcin>>a[i].s>>a[i].e>>a[i].d;\n\t\tlong long l=0,r=((long long)1<<31)-1;\n\t\twhile(l<r)\n\t\t{\n\t\t\tlong long mid=(l+r)/2;\n\t\t\tif(!(judge(mid)&1))//如果这个min之前的是偶数 \n\t\t\tl=mid+1;\n\t\t\telse\n\t\t\tr=mid;\n\t\t}\n\t\tint ans=judge(r)-judge(r-1);\n\t\tif(ans)\n\t\tcout<<l<<\" \"<<ans<<endl;\n\t\telse\n\t\tcout<<\"There's no weakness.\"<<endl;\n\t}\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 耍杂技的牛","url":"/2019/08/23/【解题报告】-耍杂技的牛/","content":"\n# 【解题报告】 耍杂技的牛\n\n## 题目：[耍杂技的牛](https://www.acwing.com/problem/content/127/)\n\n### 解题思路：\n\n排序+贪心\n\n这个牛啊实际上就是大臣，约翰那就是国王，你有没有想到一道题目，没错，那就是《国王游戏》，但是没错，那道题需要高精度，这道题目不需要高精度，所以这道题目就简单多了，这道题就是按（w+s）把牛牛们从小到大排序一下，然后计算出它们的风险值，找出最大的就可以了，还是比较简单的\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\nusing namespace std;\nconst int maxn=50010;\nlong long n;\nlong long d[maxn];\nlong long res=-0x3f3f3f3f,sum=0;\nstruct cow\n{\n\tlong long w;\n\tlong long s;\n}a[maxn];\nbool cmp(cow a,cow b)\n{\n\treturn a.s+a.w<b.s+b.w;\n}\nint main()\n{\n\tcin>>n;\n\tfor(int i=1;i<=n;i++)\n\tcin>>a[i].w>>a[i].s;\n\tsort(a+1,a+n+1,cmp);\n    for(int i=1;i<=n;i++)\n\t{\n        res=max(res,sum-a[i].s);\n        sum+=a[i].w;\n    }\n    cout<<res<<endl;\n    return 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 占卜DIY","url":"/2019/08/18/【解题报告】-占卜DIY/","content":"\n# 【解题报告】 占卜DIY\n\n## 题目：[占卜DIY](https://www.acwing.com/problem/content/119/)\n\n### 解题思路：\n\n简简单单的模拟加上dfs（简单的dfs）\n\n但是这个游戏还是挺有意思的，可以看一下\n\n> 一副去掉大小王的扑克共52张，打乱后均分为13堆，编号1~13，每堆4张，其中第13堆称作“生命牌”，也就是说你有4条命。\n>\n> 这里边，4张K被称作死神。\n>\n> 初始状态下，所有的牌背面朝上扣下。\n>\n> 流程如下：\n>\n> 1.抽取生命牌中的最上面一张(第一张)。\n>\n> 2.把这张牌翻开，正面朝上，放到牌上的数字所对应编号的堆的最上边。(例如抽到2，正面朝上放到第2堆牌最上面，又比如抽到J，放到第11堆牌最上边，注意是正面朝上放)\n>\n> 3.从刚放了牌的那一堆最底下(最后一张)抽取一张牌，重复第2步。（例如你上次抽了2，放到了第二堆顶部，现在抽第二堆最后一张发现是8，又放到第8堆顶部.........）\n>\n> 4.在抽牌过程中如果抽到K，则称死了一条命，就扔掉K再从第1步开始。\n>\n> 5.当发现四条命都死了以后，统计现在每堆牌上边正面朝上的牌的数目，只要同一数字的牌出现4张正面朝上的牌(比如4个A)，则称“开了一对”，当然4个K是不算的。\n>\n> 6.统计一共开了多少对，开了0对称作”极凶”，1~2对为“大凶”，3对为“凶”，4~5对为“小凶”，6对为“中庸”，7~8对“小吉”，9对为“吉”，10~11为“大吉”，12为“满堂开花，极吉”。\n\n这个游戏还是比较高级的\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\nusing namespace std;\nint po[60][60];\nint d[60][60];\nint s[60];\nvoid init()\n{\n\tfor(int i=1;i<=13;i++)\n\t{\n\t\tfor(int j=1;j<=4;j++)\n\t\t{\n\t\t\tchar p=getchar();\n\t\t\tif(p>='2'&&p<='9')\n\t\t\tpo[i][j]=p-'0';\n\t\t\tif(p=='0')\n\t\t\tpo[i][j]=10;\n\t\t\tif(p=='J')\n\t\t\tpo[i][j]=11;\n\t\t\tif(p=='Q')\n\t\t\tpo[i][j]=12;\n\t\t\tif(p=='K')\n\t\t\tpo[i][j]=13;\n\t\t\tif(p=='A')\n\t\t\tpo[i][j]=1;\n\t\t\tgetchar();\n\t\t}\n\t\td[i][5]=5;\n\t}\n}\nvoid dfs(int x)\n{\n\tif(x==13)\n\treturn ;\n\td[x][++d[x][0]]=x;\n\tint cx=po[x][--d[x][5]];\n\tdfs(cx);\n}\nvoid sum()\n{\n\tfor(int i=1;i<=13;i++)\n\t{\n\t\tfor(int j=1;j<=4;j++)\n\t\t{\n\t\t\tif(d[i][j])\n\t\t\ts[d[i][j]]++;\n\t\t}\n\t}\n\tint ans=0;\n\tfor(int i=1;i<=13;i++)\n\t{\n\t\tif(s[i]==4)\n\t\tans++;\n\t}\n\tcout<<ans<<endl;\n}\nint main()\n{\n\tinit();\n\tfor(int i=1;i<=4;i++)\n\tdfs(po[13][i]);\n\tsum();\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 士兵","url":"/2019/08/18/【解题报告】-士兵/","content":"\n# 【解题报告】 士兵\n\n## 题目：[士兵](https://www.acwing.com/problem/content/125/)\n\n### 解题思路：\n\n排序+离散化\n\n我们可以对每个士兵的纵坐标和横坐标进行排序，然后算它们的中位数，就可以得到他们最短的路线，然后就可以对每个士兵和中位数做差，用一个变量记录它们的和，即为答案\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cmath>\nusing namespace std;\nconst int maxn=10005;\nint n;\nlong long x[maxn],y[maxn],mx,my,ans;\nint main()\n{\n\tcin>>n;\n\tfor(int i=1;i<=n;i++)\n\tcin>>x[i]>>y[i];\n\tsort(x+1,x+1+n);\n\tsort(y+1,y+1+n);\n\tfor(int i=1;i<=n;i++)\n\tx[i]-=i;\n\tsort(x+1,x+1+n);\n\tmx=x[(n+1)>>1];\n\tmy=y[(n+1)>>1];\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tans+=abs(x[i]-mx);\n\t\tans+=abs(y[i]-my);\n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 Task","url":"/2019/08/18/【解题报告】-Task/","content":"\n# 【解题报告】 Task\n\n## 题目：[任务](https://www.acwing.com/problem/content/129/)\n\n### 解题思路：\n\n贪心\n\n我们可以贪心每个任务的等级，再贪心每个任务的时间，我们这样排一下序，再循环一下，就可以得到正确的答案了\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\nusing namespace std;\nconst long long maxn=100010;\nlong long n,m;\nstruct task\n{\n\tlong long x;\n\tlong long y;\n}; \ntask f[maxn];\ntask e[maxn];\nlong long cnt[105],ans,num;\nlong long cmp(task a,task b)\n{\n\tif(a.x==b.x)\n\treturn a.y>b.y;\n\treturn a.x>b.x;\n}\nint main()\n{\n\tcin>>n>>m;\n\tfor(long long i=1;i<=n;i++)\n\tcin>>e[i].x>>e[i].y;\n\tfor(long long i=1;i<=m;i++)\n\tcin>>f[i].x>>f[i].y;\n\tsort(e+1,e+1+n,cmp);\n\tsort(f+1,f+1+m,cmp);\n\tlong long j=1;\n\tfor(long long i=1;i<=m;i++)\n\t{\n\t\twhile(j<=n&&e[j].x>=f[i].x)\n\t\t{\n\t\t\tcnt[e[j].y]++;\n\t\t\tj++;\n\t\t}\n\t\tfor(long long k=f[i].y;k<=100;k++)\n\t\t{\n\t\t\tif(cnt[k])\n\t\t\t{\n\t\t\t\tnum++;\n\t\t\t\tcnt[k]--;\n\t\t\t\tans+=500*f[i].x+2*f[i].y;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tcout<<num<<\" \"<<ans<<endl;\n\treturn 0; \n}\n```\n\nPS：实在不懂为什么别人的代码要用pair\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 POJ1050 To the Max","url":"/2019/08/18/【解题报告】-POJ1050-To-the-Max/","content":"\n# 【解题报告】 POJ1050 To the Max\n\n## 题目：[最大的和](https://www.acwing.com/problem/content/128/)（已翻译）\n\n### 题意简述：\n\n就是一个矩阵中找一个和最大的子矩阵\n\n### 解题思路：\n\n贪心，我们可以首先在输入的时候贪一遍心，然后我们可以一行一行地贪心，最终得到最后的正确结果\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\nusing namespace std;\nconst int maxn=105;\nint n;\nint a[maxn][maxn],s[maxn][maxn];\nint ans=-0x3f,nans;\nbool check=false;\nint max(int a,int b)\n{\n\treturn a>b? a:b;\n}\nint main()\n{\n\tcin>>n;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\t{\n\t\t\tcin>>a[i][j];\n\t\t\tif(a[i][j]>=0)\n\t\t\tcheck=true;\n\t\t\tans=max(ans,a[i][j]);\n\t\t}\n\t}\n\tif(!check)\n\t{\n\t\tcout<<ans<<endl;\n\t\treturn 0;\n\t}\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\ts[i][j]=s[i-1][j]+a[i][j];\n\t}\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=i;j<=n;j++)\n\t\t{\n\t\t\tnans=0;\n\t\t\tfor(int k=1;k<=n;k++)\n\t\t\t{\n\t\t\t\tnans+=s[j][k]-s[i-1][k];\n\t\t\t\tif(nans<0)\n\t\t\t\tnans=0;\n\t\t\t\tif(nans>ans)\n\t\t\t\tans=nans;\n\t\t\t}\n\t\t}\n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 POJ2054 给树染色","url":"/2019/08/10/【解题报告】-POJ2054-给树染色/","content":"\n# 【解题报告】 POJ2054 给树染色\n\n## 题目：[Color a tree](https://www.acwing.com/problem/content/117/)(已翻译)\n\n### 解题思路：\n\n贪心\n\n我们很容易想到从第一层开始，每次染权值最大的一个节点，但是我们可以构造出一个数，让一个权值很小的点下面有权值很大的节点，所以我们考虑的贪心思路是树中除了根节点外的权值最大的节点，它的父节点染色之后一定会被马上染色，所以我们可以将权值最大的点和它的父节点进行合并，合并得到的新点的权值是这两个点之和的平均值，这样就一直合并，直至合并到一个点的时候，我们就按照这个点合并的顺序染色就是正确的答案了。\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\nusing namespace std;\nlong long r,n;\nlong long i,j,now,ans,a,b,father;\nstruct node\n{\n\tlong long f,c,t;\n\tdouble w;\n}num[1010];\nlong long find()\n{\n\tlong long ans=0;\n\tdouble maxn=0;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tif(i!=r&&num[i].w>maxn)\n\t\t{\n\t\t\tmaxn=num[i].w;\n\t\t\tans=i;\n\t\t}\n\t } \n\t return ans;\n}\nint main()\n{\n\twhile(cin>>n>>r&&(n||r))\n\t{\n\t\tans=0;\n\t\tfor(int i=1;i<=n;i++)\n\t\t{\n\t\t\tcin>>num[i].c;\n\t\t\tnum[i].w=num[i].c;\n\t\t\tnum[i].t=1;\n\t\t\tans+=num[i].c;\n\t\t}\n\t\tfor(int i=1;i<=n-1;i++)\n\t\t{\n\t\t\tcin>>a>>b;\n\t\t\tnum[b].f=a;\n\t\t}\n\t\tfor(int i=1;i<=n-1;i++)\n\t\t{\n\t\t\tnow=find();\n\t\t\tnum[now].w=0;\n\t\t\tfather=num[now].f;\n\t\t\tans+=num[now].c*num[father].t;\n\t\t\tfor(int j=1;j<=n;j++)\n\t\t\t{\n\t\t\t\tif(num[j].f==now)\n\t\t\t\tnum[j].f=father;\n\t\t\t}\n\t\t\tnum[father].c+=num[now].c;\n\t\t\tnum[father].t+=num[now].t;\n\t\t\tnum[father].w=(double)(num[father].c)/num[father].t;\n\t\t}\n\t\tcout<<ans<<endl;\n\t}\n\treturn 0;\n}\n```\n\nPS:LYD的代码有毒，在洛谷上过得了，但是在AcWing上过不了，所以我进行了摸爬滚打，终于找到了正确的答案，哈哈哈，我太开心了\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 [NOIP2012] 国王游戏","url":"/2019/08/10/【解题报告】-NOIP2012-国王游戏/","content":"\n# 【解题报告】 [NOIP2012]国王游戏\n\n## 题目：[国王游戏](https://www.luogu.org/problem/P1080)\n\n### 解题思路：\n\n贪心\n\n我们只需要将所有大臣左右手上的数的乘积从小到大进行排序，我们就得到了最优答案，但是这个代码要写高精度就是一个麻烦的东西，要好好写，话说近几年不出高精度的题目了，就怕今年要出。\n\nAC代码\n\n```c++\n#include <iostream>\n#include <algorithm>\n#include <cstring>\n#include <cstdio>\nusing namespace std;\nconst int maxn=10005;\nint n;\nstruct mini\n{\n\tint a;\n\tint b;\n}m[maxn];\nbool cmp(mini x,mini y)\n{\n\treturn x.a*x.b<y.a*y.b;\n}\nint s[4005],len;\nint ans[4005],ans_l(0); \nvoid mul(int x)\n{\n\tint t(len);\n\tfor(int i=1;i<=len;++i)\n\ts[i]*=x;\n\tfor(int i=1;i<=len+5;++i)\n\t{\n\t\tif(s[i])\n\t\tt=i;\n\t\ts[i+1]+=s[i]/10;\n\t\ts[i]%=10;\n\t}\n\tlen=t;\n}\nvoid cpy(int c[],int c_l)\n{\n\tfor(int i=1;i<=c_l;++i)\n\tans[i]=c[i];\n\tans_l=c_l;\n}\nvoid out(int s[],int l)\n{\n\tfor(int i=l;i>=1;--i)\n\tcout<<s[i];\n\tcout<<endl; \n}\nvoid div(int x)\n{\n\tint c[4005],l(-1);\n\tfor(int i=1;i<=len;++i)\n\tc[i]=s[i];\n\tfor(int i=len;i>=1;--i)\n\t{\n\t\tc[i-1]+=(c[i]%x)*10;\n\t\tc[i]/=x;\n\t\tif(c[i]&&l==-1)\n\t\tl=i;\n\t}\n\tif(ans_l==l)\n\t{\n\t\tbool flg(0);\n\t\tfor(int i=l;i>=1;--i)\n\t\t{\n\t\t\tif(ans[i]<c[i])\n\t\t\t{\n\t\t\t\tflg=1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\telse break;\n\t\t}\n\t\tif(flg)\n\t\tcpy(c,l);\n\t}\n\tif(ans_l<l)\n\tcpy(c,l);\n}\nint main()\n{\n\tcin>>n;\n\tcin>>m[0].a>>m[0].b;\n\tfor(int i=1;i<=n;++i)\n\tcin>>m[i].a>>m[i].b;\n\tsort(m+1,m+1+n,cmp);\n\ts[1]=1;len=1;\n\tmul(m[0].a);\n\tfor(int i=1;i<=n;++i)\n\tdiv(m[i].b),mul(m[i].a);\n\tout(ans,ans_l);\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 POJ1328 雷达设备","url":"/2019/08/10/【解题报告】-POJ1328-雷达设备/","content":"\n# 【解题报告】 POJ1328 雷达设备\n\n## 题目：[雷达设备](https://www.acwing.com/problem/content/114/)（已翻译）\n\n### 解题思路：\n\n贪心。\n\n这道题只需要将x轴上方的一些建筑物，利用勾股定理算出x轴上一段能管辖它的区间，所以问题就变成了给定n个区间，在x轴上放置最少的点，使每个区间至少包含一个点，但是，特别地，如果建筑物的纵坐标超出了每个雷达管辖的半径，就直接不会继续下去了，因为不管怎么样，那个建筑物都无法被管辖，所以我们只需要每个建筑物算出来的区间的右端点进行排序，然后设置一个数组vis，来记录每个建筑物是否被管辖到，然后就ok了\n\nAC代码\n\n```c++\n#include <iostream>\n#include <algorithm>\n#include <cmath>\nusing namespace std;\nconst int maxn=1005;\nint n,d;\nint ans;\nbool vis[maxn];\nstruct sec\n{\n\tdouble l;\n\tdouble r;\n}b[maxn];\nbool cmp(sec p,sec q)\n{\n\treturn p.r<q.r;\n}\nint main()\n{\n\tcin>>n>>d;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tint x,y;\n\t\tcin>>x>>y;\n\t\ty=abs(y);\n\t\tif(y>d)\n\t\t{\n\t\t\tcout<<\"-1\"<<endl;\n\t\t\treturn 0;\n\t\t}\n\t\tb[i].l=x-sqrt(d*d-y*y);\n\t\tb[i].r=x+sqrt(d*d-y*y);\n\t\tvis[i]=false;\n\t}\n\tsort(b+1,b+1+n,cmp);\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tif(!vis[i])\n\t\t{\n\t\t\tans++;\n\t\t\tvis[i]=true;\n\t\t\tfor(int j=i+1;j<=n;j++)\n\t\t\t{\n\t\t\t\tif(b[j].l<b[i].r)\n\t\t\t\tvis[j]=true;\n\t\t\t}\n\t\t}\n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 POJ3614 防晒","url":"/2019/08/09/【解题报告】-POJ3614-防晒/","content":"\n# 【解题报告】 POJ3614 防晒\n\n## 题目：[防晒](https://www.acwing.com/problem/content/112/)（已翻译）\n\n### 解题思路：\n\n模拟+贪心；\n\n我们这道题是一道好的贪心的例题，我们只要对于每一头奶牛按照它们的minspf进行递减排序，然后在对于每一种防晒霜进行一次扫描，找出符合条件的spf值最大的防晒霜\n\n但是会想到一点，我们需要将每一头奶牛的minspf和maxspf进行一一对应，所以我们可以使用结构体，然后自己在写一个比较函数，就可以让这两个数值一一对应，这样就方便多了\n\nAC代码\n\n```c++\n#include <iostream>\n#include <algorithm>\nusing namespace std;\nconst int maxn=2505;\nint c,l,ans;\nstruct cow\n{\n\tint minspf;\n\tint maxspf;\n}a[maxn];\nstruct sc\n{\n\tint spf;\n\tint cover;\n}b[maxn];\nint cmp(sc a,sc b)\n{\n    if(a.spf==b.spf)\n    return a.cover>b.cover;\n    return a.spf>b.spf;\n}\nint cmp2(cow a,cow b)\n{\n    if(a.minspf==b.minspf)\n    return a.maxspf>b.maxspf;\n    return a.minspf>b.minspf;\n}\nint main()\n{\n\tcin>>c>>l;\n\tfor(int i=1;i<=c;i++)\n\tcin>>a[i].minspf>>a[i].maxspf;\n\tsort(a+1,a+1+c,cmp2);\n\tfor(int i=1;i<=l;i++)\n\tcin>>b[i].spf>>b[i].cover;\n\tsort(b+1,b+1+l,cmp);\n\tfor(int i=1;i<=c;i++)\n\t{\n\t\tfor(int j=1;j<=l;j++)\n\t\t{\n\t\t\tif(b[j].spf>=a[i].minspf&&b[j].spf<=a[i].maxspf&&b[j].cover)\n\t\t\t{\n\t\t\t\tans++;\n\t\t\t\tb[j].cover--;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 天才ACM","url":"/2019/08/09/【解题报告】-天才ACM/","content":"\n# 【解题报告】 天才ACM\n\n## 题目：[天才ACM](https://www.acwing.com/problem/content/111/)\n\n### 解题思路：\n\n倍增算法\n\n设p=1，r=l\n\n求出r—r+p这一区间的校验值，如果校验值小于等于t，则r+=p，p*=2；\n\n否则p/=2;\n\n一直重复，直到p等于0的时候，r就是最终答案\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\nusing namespace std;\nconst int maxn=500005;\nlong long t,tot;\nlong long n,m,k;\nlong long p[maxn];\nlong long f[maxn];\nint l,r,mid,j;\nlong long min(long long a,long long b)\n{\n\treturn a<b? a:b;\n}\nbool check(int l,int r) \n{\n    tot=0;\n    long long sum=0;\n    for(int i=l;i<=r;i++) \n\tf[++tot]=p[i];\n    sort(f+1,f+1+tot);\n    for(int i=1;i<=m;i++) \n\t{\n        if (i>=(tot-i+1)) \n\t\tbreak;\n        sum+=(long long)(f[tot-i+1]-f[i])*(f[tot-i+1]-f[i]);\n        if(sum>k)\n\t\tbreak;\n    }\n    if(sum>k)\n\treturn false;\n    return true;\n}\nbool cmp(int x,int y)\n{\n\treturn p[x]<p[y];\n}\nbool calc()\n{\n\tlong long sum=0;\n\tint ll=1,rr=tot;\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\twhile(ll<rr&&f[ll]>mid)\n\t\tll++;\n\t\twhile(ll<rr&&f[rr]>mid)\n\t\trr--;\n\t\tif(ll>=rr)\n\t\tbreak;\n\t\tsum+=(long long)(p[f[rr]]-p[f[ll]])*(p[f[rr]]-p[f[ll]]);\n\t\tif(sum>k)\n\t\tbreak;\n\t\tll++;\n\t\trr--;\n\t}\n\treturn sum<=k;\n}\nvoid work()\n{\n\t\tcin>>n>>m>>k;\n\t\tfor(int i=1;i<=n;i++)\n\t\tcin>>p[i];\n\t\tint ans=0;\n\t\tfor(int i=1;i<=n;i++)\n\t\t{\n\t\t\tfor(j=1;i+(1<<j)-1<=n;j++)\n\t\t\t{\n\t\t\t\tif(!check(i,i+(1<<j)-1))\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tl=i+(1<<(j-1))-1;\n\t\t\tr=min(i+(1<<j)-1,n);\n\t\t\ttot=0;\n\t\t\tfor(int k=i;k<=r;k++)\n\t\t\tf[++tot]=k;\n\t\t\tsort(f+1,f+tot+1,cmp);\n\t\t\twhile(l<=r)\n\t\t\t{\n\t\t\t\tmid=(l+r)/2;\n\t\t\t\tif(calc())\n\t\t\t\t{\n\t\t\t\t\ti=mid;\n\t\t\t\t\tl=mid+1;\n\t\t\t\t}\n\t\t\t\telse r=mid-1;\n\t\t\t}\n\t\t\tans++;\n\t\t}\n\t\tcout<<ans<<endl;\n}\nint main()\n{\n\tcin>>t;\n\twhile(t--)\n\t{\n\t\twork();\n\t}\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 POJ2299 超快速排序","url":"/2019/08/08/【解题报告】-POJ2299-超快速排序/","content":"\n# 【解题报告】POJ2299 超快速排序\n\n## 题目：[超快速排序](https://www.acwing.com/problem/content/109/)（已翻译）\n\n### 解题思路：\n\n归并排序求逆序对\n\n归并排序使我们众所周知的，我们只要在归并排序中计算每一个子序列中的逆序对数，我们就可以计算出总的逆序对数了，也就是\n$$\ncnt+=mid-i+1\n$$\n然后就完成了这道题\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstring>\nusing namespace std;\nint n;\nlong long cnt;\nconst int maxn=500005;\nlong long a[maxn];\nlong long b[maxn];\nvoid merge_sort(int l,int r)\n{\n    if(r>l)\n    {\n        int mid=(l+r)/2;\n        int i=l; \n        int p=l,q=mid+1;\n        merge_sort(l,mid);\n        merge_sort(mid+1,r);\n        while(p<=mid||q<=r)\n        {\n            if(q>r||(p<=mid&&a[p]<=a[q]))\n            b[i++] = a[p++];\n            else\n            {\n                b[i++]=a[q++];\n                cnt+=mid-p+1;\n            }\n        }\n        for(i=l;i<=r;i++)\n        a[i]=b[i];\n    }\n}\nint main()\n{\n\twhile(cin>>n&&n)\n\t{\n\t\tfor(int i=1;i<=n;i++)\n\t\tcin>>a[i];\n\t\tcnt=0;\n\t\tmerge_sort(1,n);\n\t\tcout<<cnt<<endl;\n\t}\n\treturn 0;\n}\n```\n\n顺便附上归并排序的代码\n\n```c++\n#include <iostream>\nusing namespace std;\nint n,cnt;\nconst int maxn=100005;\nlong long a[maxn];\nlong long b[maxn];\nvoid merge_sort(int l,int r)\n{\n    if(r>l)\n    {\n        int mid=(l+r)/2;\n        int i=l; \n        int p=l,q=mid+1;\n        merge_sort(l,mid);\n        merge_sort(mid+1,r);\n        while(p<=mid||q<=r)\n        {\n            if(q>r||(p<=mid&&a[p]<=a[q]))\n            b[i++] = a[p++];\n            else\n            {\n                b[i++]=a[q++];\n                cnt+=mid-p+1;\n            }\n        }\n        for(i=l;i<=r;i++)\n        a[i]=b[i];\n    }\n}\nint main()\n{\n\tcin>>n;\n\tfor(int i=1;i<=n;i++)\n\tcin>>a[i];\n\tmerge_sort(1,n);\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tcout<<a[i]<<\" \";\n\t}\n\tcout<<endl;\n\treturn 0;\n}\n\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 BZOJ3032 七夕祭","url":"/2019/08/08/【解题报告】-BZOJ3032-七夕祭/","content":"\n# 【解题报告】 BZOJ3032 七夕祭\n\n## 题目：[七夕祭](https://www.acwing.com/problem/content/107/)（翻译过的）\n\nps：话说今天是七夕节，我就正好做到七夕祭\n\n### 解题思路：\n\n看到这道题的题目，可以想到《均分纸牌》和我之前做的《货仓选址》两题，这样经过思考，演算和推理，我们可以得出，需要的最少步数是\n$$\n\\sum\\limits_{i=1}^M\\left|S[i]-s[k]\\right|\n$$\n其中S是A的前缀和，即\n$$\nS[i]=\\sum\\limits_{j=1}^iA[j]\n$$\n所以经过简单地编码，答案就出来了\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cmath>\nusing namespace std;\nconst int maxn=100005;\nlong long a[maxn],b[maxn];\nlong long f[maxn];\nlong long n,m,t;\nlong long x,y;\nlong long calc(long long a[],long long n)\n{\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\ta[i]-=(a[0]/n);\n\t\tf[i]=f[i-1]+a[i];\n\t}\n\tsort(f+1,f+n+1);\n\tlong long mid=(n+1)>>1,ans=0;\n\tfor(int i=1;i<=n;i++)\n\tans+=abs(f[mid]-f[i]);\n\treturn ans;\n}\nint main()\n{\n\tcin>>n>>m>>t;\n\tfor(int i=1;i<=t;i++)\n\t{\n\t\tcin>>x>>y;\n\t\ta[x]++;\n\t\tb[y]++;\n\t}\n\tfor(int i=1;i<=n;i++)\n\ta[0]+=a[i];\n\tfor(int i=1;i<=m;i++)\n\tb[0]+=b[i];\n\tlong long c=a[0]%n,d=b[0]%m;\n\tif(!c&&!d)\n\tcout<<\"both \"<<calc(a,n)+calc(b,m);\n\telse if(!c)\n\tcout<<\"row \"<<calc(a,n);\n\telse if(!d)\n\tcout<<\"column \"<<calc(b,m);\n\telse\n\tcout<<\"impossible\";\n\tcout<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 CH0501 货仓选址","url":"/2019/08/07/【解题报告】-CH0501-货仓选址/","content":"\n# 【解题报告】 CH0501 货仓选址\n\n## 题目：[货仓选址](https://www.acwing.com/problem/content/106/)\n\n### 解题思路：\n\n中位数\n\n中位数是一种美好的数，在数学中经常使用，这道题建立一个数组，读入数据，然后排序一下，假设货仓建在x处，x左边有p个商家，x右边有q个商家，如果p<q,那么把货仓向右移一个位置，距离之和就变小，类似地，p>q，就把货仓向左移一个位置，所以建在中间的时候距离之和最小\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <cmath>\n#include <algorithm>\nusing namespace std;\nconst int maxn=100005;\nint a[maxn],n,ans,pos;\nint main()\n{\n    cin>>n;\n    for(int i=1;i<=n;i++)\n    cin>>a[i];\n    sort(a+1,a+1+n);\n    int pos=a[n/2+1];\n    for(int i=1;i<=n;i++)\n    ans+=abs(a[i]-pos);\n    cout<<ans<<endl;\n    return 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】 CF67C Cinema","url":"/2019/08/06/【解题报告】-CF67C-Cinema/","content":"\n# 【解题报告】 CF67C Cinema\n\n## 题目：https://www.acwing.com/problem/content/105/\n\n### 解题思路：\n\n排序+离散化\n\nm部电影和n个人涉及2×m+n种语言。建立一个数组排序再离散化一下，用1到2×m+1之间的数来算。然后就暴力统计一下，就可以得出答案\n\nAC代码\n\n```c++\n#include <iostream>\n#include <algorithm>\n#include <cstdio>\n#include <cstring>\n#include <set>\nusing namespace std;\nconst int maxn=5*10e5;\nint n,m;\nint a[maxn];\nint p[maxn];\nint w[maxn];\nint s[maxn];\nint q[maxn];\nint k[maxn];\nint ans1,ans2;\nint ans=1;\nvoid disc(int x)//离散化\n{\n\tint c;\n\tsort(p+1,p+x+1);\n\tfor(int i=1;i<=x;i++)\n\t{\n\t\tif(i==1||p[i]!=p[i-1])\n\t\tq[++c]=p[i];\n\t}\n\tq[0]=c;\n}\nint query(int x)//离散化的二分查询\n{\n\tint l=1,r=q[0],mid;\n\twhile(l<r)\n\t{\n\t\tmid=(l+r)>>1;\n\t\tif(q[mid]>=x)\n\t\tr=mid;\n\t\telse\n\t\tl=mid+1;\n\t}\n\treturn l;\n}\nint main()\n{\n\tint c=0;\n\tcin>>n;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tcin>>a[i];\n\t\tp[++c]=a[i];\n\t}\n\tcin>>m;\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\tcin>>w[i];\n\t\tp[++c]=w[i];\n\t}\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\tcin>>s[i];\n\t\tp[++c]=s[i];\n\t}\n\tdisc(c);\n\tfor(int i=1;i<=n;i++)\n\tk[query(a[i])]++;//暴力统计\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\tint x=k[query(w[i])];\n\t\tint y=k[query(s[i])];\n\t\tif(x>ans1||(x==ans1&&y>ans2))\n\t\t{\n\t\t\tans=i;\n\t\t\tans1=x;\n\t\t\tans2=y;\n\t\t}\n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】luogu P2078朋友","url":"/2019/08/06/【解题报告】luoguP2078朋友/","content":"\n# 【解题报告】luogu P2078 朋友\n\n### 题目：[luogu P2078](https://www.luogu.org/problem/P2078)\n\n题目思路：\n\n并查集，C++ STL\n\n有了C++stl容器，我们就high了，map可以处理数组下标为负的情况，然后男女朋友的关系的话，就分别统计每个公司有多少人有关系，取一个最小值就好了\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <map> \nusing namespace std;\nmap<int,int>f;\nint n,m,p,q;\nint fm,fh;\nint max(int a,int b)\n{\n\treturn a<b? a:b;\n} \nint get(int x)\n{\n\treturn f[x]=(x==f[x]? x:get(f[x]));\n}\nvoid merge(int x,int y)\n{\n\tf[get(x)]=get(y);\n}\nint main()\n{\n\tint x,y;\n\tcin>>n>>m>>p>>q;\n\tfor(int i=(-1*m);i<=n;i++)\n\tf[i]=i;\n\tfor(int i=1;i<=p+q;i++)\n\t{\n\t\tcin>>x>>y;\n\t\tmerge(x,y);\n\t}\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tif(get(f[i])==get(1))\n\t\tfm++;\n\t}\n\tfor(int i=(-1*m);i<=-1;i++)\n\t{\n\t\tif(get(f[i])==get(-1))\n\t\tfh++; \n\t}\n\tcout<<max(fm,fh)<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】[HNOI2003]激光炸弹","url":"/2019/08/05/【解题报告】-HNOI2003-激光炸弹/","content":"\n# 【解题报告】 [HNOI2003]激光炸弹\n\n## 题目：[luogu P2280](https://www.luogu.org/problem/P2280)\n\n在这样可爱的夜晚，调试题目恐怕是最爽的选择了\n\n附：https://www.luogu.org/record/list?user=136889\n\n在这么多次失败下我终于成功了\n\n解题思路：\n\n前缀和与拆分\n\n这是一道简单题，但是我却调试了那么多次没调出来，竟然是循环的问题\n\nrp--。。\n\n建立一个二维数组，储存某个区域的目标的数量，然后就使用一个二维前缀和，就可以弄出来了，注意因为时间的原因，在输入的时候也就直接输入了，我那么多次就是因为超时\n\nAC代码\n\n```c++\n#include <iostream>\nusing namespace std;\nconst int maxn=5005;\nint r,n;\nint s[maxn][maxn];\nint ans;\nint x,y,w;\nint max(int a,int b)\n{\n\treturn a>b? a:b;\n}\nint main()\n{\n\tcin>>n>>r;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tcin>>x>>y>>w; \n\t\ts[x+1][y+1]=w;\n\t}\n\tfor(int i=1;i<=5001;i++)\n\t{\n\t\tfor(int j=1;j<=5001;j++)\n\t\ts[i][j]+=s[i-1][j]+s[i][j-1]-s[i-1][j-1];\n\t}\n\tfor(int i=0;i<=5001-r;i++)\n\t{\n\t\tfor(int j=0;j<=5001-r;j++)\n\t\tans=max(ans,s[i][j]-s[i+r][j]-s[i][j+r]+s[i+r][j+r]); \n\t}\n\tcout<<ans<<endl;\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"【解题报告】[NOI2002]银河英雄传说","url":"/2019/08/04/【解题报告】-NOI2002-银河英雄传说/","content":"\n# 【解题报告】[NOI2002]银河英雄传说\n\n## 题目：[luogu P1196](https://www.luogu.org/problem/P1196)\n\n------\n\n### 题意简述\n\n处理M个指令，都为两种如下形式的指令之一\n\n1.M i j 表示让第i好战舰所在列的全部战舰保持原有顺序，接在第j好战舰所在列的尾部。\n\n2.C i j，表示询问第i号战舰与第j号战舰当前是否处于同一列中，如果在同一列中，它们之间间隔了多少艘战舰。如果不在同一列中，输出-1；\n\n### 解题思路\n\n并查集\n\n第二个指令我们要知道i，j两号战舰差多少，维护一个数列d即可，d[x]代表x前面的战舰数量,要查询的时候，我们只要知道i，j两号前面各有多少战舰，然后i前面的减去j前面的绝对值减一就可以了。\n\n而第一个指令就是简单地处理一下size，记录集合大小，也很简单\n\n### AC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\nusing namespace std;\nint f[30005];\nint d[30005];\nint size[30005];\nint t;\nint jdz(int x)\n{\n\treturn x>0? x:(-x);\n}\nvoid init()\n{\n\tfor(int i=1;i<=30000;i++)\n\t{\n\t\tf[i]=i;\n\t\tsize[i]=1;\n\t}\n}\nint get(int x)\n{\n\tif(x==f[x])\n\treturn x;\n\tint root=get(f[x]);\n\td[x]+=d[f[x]];\n\treturn f[x]=root;\n}\nvoid merge(int x,int y)\n{\n\tx=get(x),y=get(y);\n\tf[x]=y;d[x]=size[y];\n\tsize[y]+=size[x];\n}\nint main()\n{\n\tcin>>t;\n\tinit();\n\tfor(int i=1;i<=t;i++)\n\t{\n\t\tchar s[2];\n\t\tint x,y;\n\t\tscanf(\"%s\",s);\n\t\tcin>>x>>y;\n\t\tif(s[0]=='M')\n\t\tmerge(x,y);\n\t\tif(s[0]=='C')\n\t\t{\n\t\t\tif(get(x)==get(y))\n\t\t\tcout<<jdz(d[x]-d[y])-1<<endl;\n\t\t\telse\n\t\t\tcout<<\"-1\"<<endl;\n\t\t}\n\t}\n\treturn 0;\n}\n```\n\n\n\n数据情况https://www.luogu.org/record/22153408\n","categories":["OI时期解题报告"]},{"title":"幂运算","url":"/2019/08/03/幂运算/","content":"\n# 幂运算\n\n众所周知，在NOIP系列竞赛中，会考到许多优化，而这些许多优化是由一个个简单的优化组件而来的，使整个程序的优化尽可能地达到最大，用最少的时间和空间来实现正确代码，而幂运算作为其中的一个基本我今天就来总结一下，如有不足，以后也会加勘误and Update。\n\n### 幂\n\n普遍数学上幂的意义是一个数做自乘的运算，如a的b次方意思是b个a相乘，使表示上更加简便\n\n### 普通幂\n\nC++中普通幂的代码很容易实现，这是实现代码(基本实现代码)\n\n```c++\n#include <iostream>\nusing namespace std;\nint main()\n{\n    int a,b;\n    cin>>a>>b;\n    for(int i=2;i<=b;i++)\n    a*=a;\n    cout<<a<<endl;\n    return 0;\n}\n```\n\n### 快速幂\n\n在普通幂中发现算很大的数的时候回非常地慢，我们就要想怎么优化，怎么优化呢，我们引入一组基本公式\n$$\na^b=a^{\\frac{b}{2}}*a^{\\frac{b}{2}}(b为偶数)\n$$\n\n$$\na^b=a^{\\frac{b}{2}}*a^{\\frac{b}{2}}*a(b为奇数)\n$$\n\n我们发现这样可以利用递归解决子问题来快速地算出a的b次方，加速了运算\n\n代码如下\n\n```c++\n#include <iostream>\nusing namespace std;\nint a,b;\nint quick_pow(int a,int b)\n{\n    if(b==0)\n        return 1;\n    int t=quick_pow(a,b/2);\n    if(b%2==1)\n        return t*t*a;\n    else\n        return t*t;\n}\nint main()\n{\n    cin>>a>>b;\n    cout<<quick_pow(a,b)<<endl;\n    return 0;\n}\n```\n\n或者代码也可以这样\n\n```c++\n#include <iostream>\nusing namespace std;\nint power(int a,int b,int p)\n{\n\tint ans=1%p;\n\twhile(b)\n\t{\n\t\tif(b&1)\n\t\tans=(long long)ans*a%p;\n\t\ta=(long long)a*a%p; \n        b>>=1;\n\t}\n\treturn ans;\n}\nint main()\n{\n\tint a,b,p;\n\tcin>>a>>b>>p;\n\tcout<<power(a,b,p)<<endl;\n\treturn 0;\t\n} \n```\n\n特殊情况下算2的n次幂可以直接\n$$\n1<<n\n$$\n\n### 矩阵快速幂\n\n矩阵快速幂是一个对于矩阵的快速幂\n\n你还不知道什么是矩阵吗？（快速查看：[矩阵](http://www.baidu.com/link?url=Oht4LWR2XZyvOyurx8bBtzOMG9sQ9mw_uIB3vpDv4q5TNoowNK5mTGgpMgMYKjM38ablVMB_zKMS9hKcOJRRC9r1ohtbdNJZAH8qIhf2iaK&wd=&eqid=92e7f3720048392c000000035d443e83)）\n\n矩阵快速幂是对于矩阵乘法的自己多次相乘的快速运算\n\n和普通快速幂差不多，只是把‘*’换成了一个mul函数\n\n代码如下\n\n```c++\nmat mul(mat x,mat y)\n{\n\tmat c;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\tc.m[i][j]=0;\n\t}\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\t{\n\t\t\tfor(int k=1;k<=n;k++)\n\t\t\tc.m[i][j]=(c.m[i][j]%mod+(x.m[i][k]*y.m[k][j])%mod)%mod;\n\t\t}\n\t}\n\treturn c;\n}\n```\n\n因为要返回一个数组，所以为了方便，我们新建了一个结构体，也就是mat\n\n```c++\nstruct mat\n{\n    long long m[1005][1005];//用来存矩阵\n};\n```\n\n这样我们在写一个类似于普通快速幂的函数就成功解决问题了\n\n类普通快速幂函数\n\n```c++\nmat pow(mat a,long long b)\n{\n\tmat ans=e;\n\twhile(b)\n\t{\n\t\tif(b&1)\n\t\tans=mul(ans,a);\n\t\ta=mul(a,a);\n\t\tb>>=1;\n\t}\n\treturn ans;\n}\n```\n\n所以我们就做出来了，但是为了使整个矩阵能保持原样，这个说不清，自己去看单位矩阵，然后这就是一个巧妙的地方\n\n```c++\nfor(int i=1;i<=n;i++)\n\te.m[i][i]=1;\n```\n\n最后，矩阵快速幂的一道模板题发给大家\n\n------\n\n#  lgP3390【模板】矩阵快速幂\n\n提交 25.45k\n\n通过 8.58k\n\n时间限制 1.00s\n\n内存限制 125.00MB\n\n### 题目背景\n\n矩阵快速幂\n\n### 题目描述\n\n给定n*n的矩阵A，求A^k\n\n### 输入格式\n\n第一行，n,k\n\n第2至n+1行，每行n个数，第i+1行第j个数表示矩阵第i行第j列的元素\n\n### 输出格式\n\n输出A^k\n\n共n行，每行n个数，第i行第j个数表示矩阵第i行第j列的元素，每个元素模10^9+7\n\n### 输入输出样例\n\n**输入 #1** \n\n```\n2 1\n1 1\n1 1\n```\n\n**输出 #1** \n\n```\n1 1\n1 1\n```\n\n### 说明/提示\n\nn<=100, k<=10^12, |矩阵元素|<=1000 算法：矩阵快速幂\n\n------\n\nAC代码\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\nusing namespace std;\nconst int maxn=1005;\nconst int mod=1000000007; \nstruct mat\n{\n\tlong long m[maxn][maxn];\n};\nmat a,e;\nlong long n,p;\nmat mul(mat x,mat y)\n{\n\tmat c;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\tc.m[i][j]=0;\n\t}\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\t{\n\t\t\tfor(int k=1;k<=n;k++)\n\t\t\tc.m[i][j]=(c.m[i][j]%mod+(x.m[i][k]*y.m[k][j])%mod)%mod;\n\t\t}\n\t}\n\treturn c;\n}\nmat pow(mat a,long long b)\n{\n\tmat ans=e;\n\twhile(b)\n\t{\n\t\tif(b&1)\n\t\tans=mul(ans,a);\n\t\ta=mul(a,a);\n\t\tb>>=1;\n\t}\n\treturn ans;\n}\nint main()\n{\n\tcin>>n>>p;\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\tcin>>a.m[i][j];\n\t}\n\tfor(int i=1;i<=n;i++)\n\te.m[i][i]=1;a\n\tmat s=pow(a,p);\n\tfor(int i=1;i<=n;i++)\n\t{\n\t\tfor(int j=1;j<=n;j++)\n\t\tcout<<s.m[i][j]<<\" \";\n\t\tcout<<endl;\n\t}\n\treturn 0;\n}\n```\n\nPS:在自己电脑上可能输出不了，但是在洛谷IDE上能正常运行啊\n\n这就奇怪了，请问一下各位为什么\n\n下集预告：单源最短路径的优化或矩阵的简单介绍\n","categories":["学习笔记"]},{"title":"高级（线性）素数筛","url":"/2019/08/02/高级（线性）素数筛/","content":"\n# 高级（线性）素数筛\n\n在不久前，我已经介绍了简单素数筛，所以我们这次来介绍一下高级素数筛，实际上就是线性筛素数，很快地能把素数筛出来，但是我们平常竞赛的时候常用的还是那个简单素数筛，所以我这篇文章就来普及一下加自我练习一下啦！\n\n题目思路：\n\n之前的简单素数筛中的合数会被多个数重复标记，因此造成了冗余，所以我们要想办法在这个方面优化，我们就想到了合数只要被它的最小质因子标记了一遍之后就不再标记了，这就减少了很大一部分的冗余，速度也就变快了。\n\n大概过程如下：\n\n1.从2~n之间枚举\n\n2.如果v[i]=i,那么i就是质数\n\n3.枚举小于等于v[i]的每个质数，让v[i*p]=p，在i的基础上在乘一个质因子p，得到一个新的数。\n\n4.因为p<=v[i]，所以p就是 合数i*p的最小质因子。\n\n|    i    | 2    | 3    | 4    | 5        | 6    | 7           | 8    | 9      | 10   |\n| :-----: | ---- | ---- | ---- | -------- | ---- | ----------- | ---- | ------ | ---- |\n| p<=v[i] | 2    | 2,3  | 2    | 2,3,5    | 2    | 2,3,5,7     | 2    | 2,3    | 2    |\n|   i*p   | 4    | 6,9  | 8    | 10,15,25 | 12   | 14,21,35,49 | 16   | 18,.27 | 20   |\n\nps:以上表格来自《算法竞赛进阶指南》\n\n因此我们就通过这种方法制作出了线性筛素数的程序\n\n代码如下，时间复杂度\n$$\nO(n)\n$$\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\nusing namespace std;\nconst int maxn=10000005;\nint v[maxn],p[maxn];\nvoid primes(int n)\n{\n\tint m=0;\n\tmemset(v,0,sizeof(v));\n\tfor(int i=2;i<=n;i++)\n\t{\n\t\tif(v[i]==0)\n\t\t{\n\t\t\tv[i]=i;\n\t\t\tp[++m]=i;\n\t\t}\n\t\tfor(int j=1;j<=m;j++)\n\t\t{\n\t\t\tif(p[j]>v[i]||p[j]>(n/i))\n\t\t\tbreak;\n\t\t\tv[i*p[j]]=p[j];\n\t\t}\n\t}\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\tcout<<p[i]<<endl;\n\t}\n} \nint main()\n{\n\tint n;\n\tcin>>n;\n\tprimes(n);\n\treturn 0;\n}\n```\n\n------\n\n接下来我们来一道例题练练手\n\n题目：[luogu P3383 【模板】线性筛素数](https://www.luogu.org/problem/P3383)\n\n我帮大家复制了一下题面\n\n### 题目描述\n\n如题，给定一个范围N，你需要处理M个某数字是否为质数的询问（每个数字均在范围1-N内）\n\n### 输入格式\n\n第一行包含两个正整数N、M，分别表示查询的范围和查询的个数。\n\n接下来M行每行包含一个不小于1且不大于N的整数，即询问该数是否为质数。\n\n### 输出格式\n\n输出包含M行，每行为Yes或No，即依次为每一个询问的结果。\n\n### 输入输出样例\n\n**输入 #1** \n\n```\n100 5\n2\n3\n4\n91\n97\n```\n\n**输出 #1** \n\n```\nYes\nYes\nNo\nNo\nYes\n```\n\n### 说明/提示\n\n时空限制：500ms 128M\n\n数据规模：\n\n对于30%的数据：N<=10000，M<=10000\n\n对于100%的数据：N<=10000000，M<=100000\n\n样例说明：\n\nN=100，说明接下来的询问数均不大于100且不小于1。\n\n所以2、3、97为质数，4、91非质数。\n\n故依次输出Yes、Yes、No、No、Yes。\n\n------\n\n题目思路：这道题很简单，就是把刚才的代码稍加改动，判断一下只要v[i]==i，就是素数，否则就不是素数\n\n代码如下\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <algorithm>\n#include <cstring>\nusing namespace std;\nconst int maxn=10000005;\nint v[maxn],p[maxn];//v代表v是不是质数，是的话为1，p为质数\nvoid primes(int n)\n{\n\tint m=0;\n\tmemset(v,0,sizeof(v));\n\tfor(int i=2;i<=n;i++)\n\t{\n\t\tif(v[i]==0)\n\t\t{\n\t\t\tv[i]=i;\n\t\t\tp[++m]=i;\n\t\t}\n\t\tfor(int j=1;j<=m;j++)\n\t\t{\n\t\t\tif(p[j]>v[i]||p[j]>(n/i))\n\t\t\tbreak;\n\t\t\tv[i*p[j]]=p[j];\n\t\t}\n\t}\n} \nint main()\n{\n\tint n,q,r;\n\tcin>>n>>q;\n\tprimes(n);\n\tfor(int i=1;i<=q;i++)\n\t{\n\t\tcin>>r;\n\t\tif(r==0||r==1)\n\t\t{\n\t\t\tcout<<\"No\"<<endl;\n\t\t\tcontinue;\n\t\t}\n\t\tif(v[r]==r)\n\t\tcout<<\"Yes\"<<endl;\n\t\telse\n\t\tcout<<\"No\"<<endl;\n\t}\n\treturn 0;\n}\n```\n\n下集预告:单源最短路径优化版或其他\n","categories":["学习笔记"]},{"title":"Dijkstra（迪杰斯特拉）算法","url":"/2019/07/31/Dijkstra（迪杰斯特拉）算法/","content":"\n# Dijkstra（迪杰斯特拉）算法\n\n晚上是个好时间去刷题，我今天就看了Dijkstra算法，名字倒挺不好读的，所以我进行了深入思考，终于把一个看起来很难的算法，实际上不太简单的算法弄懂了，先来介绍一下Dijkstra\n\n> 迪杰斯特拉算法(Dijkstra)是由荷兰计算机科学家狄克斯特拉于1959 年提出的，因此又叫狄克斯特拉算法。是从一个顶点到其余各顶点的最短路径算法，解决的是有权图中最短路径问题。迪杰斯特拉算法主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止。\n>\n> ——来自《百度百科》\n\n这个荷兰科学家还是比较厉害的。\n\n我们就开始看Dijkstra了。\n\nDijkstra是一个基于贪心的最短路算法，不能处理权值为负的情况，是单源的最短路算法的一种\n\n这个算法的主要过程如下：\n\n> 1.建一个数组d[n]，表示从第n个节点到第1个节点的最短距离，然后初始化d[1]=1，其他的为正无穷。\n>\n> 2.遍历找到一个没有被覆盖的d[x]的最小的节点x，然后标记x\n>\n> 3.尝试x的每个出边(x,y,z)，如果d[y]>d[x]+z,就赋值d[y]=d[x]+z;（z为x到y的距离）；\n>\n> 4.最后重复以上过程，直到所有的点都被标记，就完事儿了\n\n我们就完成了单源最短路径，代码如下：\n\n```c++\n#include <iostream>\n#include <cstdio>\n#include <cstring>\nusing namespace std;\nint a[3010][3010],d[3010],n,m;\nbool v[3010];\nint min(int a,int b)//最小值函数\n{\n\treturn a<b? a:b;\n}\nvoid dijkstra()//单源最短路径\n{\n\tmemset(d,0x3f,sizeof(d));//初始化数组d的元素为正无穷\n\tmemset(v,0,sizeof(v));//初始化数组v为0\n\td[1]=0;//第一个节点到它自己的距离为0\n\tfor(int i=1;i<n;i++)//开始循环\n\t{\n\t\tint x=0;\n\t\tfor(int j=1;j<=n;j++)//内层循环\n\t\t{\n\t\t\tif(!v[j]&&(x==0||d[j]<d[x]))\n\t\t\tx=j;\n\t\t}\n\t\tv[x]=1;\n\t\tfor(int y=1;y<=n;y++)//类似一个动态规划的过程但不是\n\t\td[y]=min(d[y],d[x]+a[x][y]); \n\t}\n}\nint main()\n{\n\tcin>>n>>m;//输入一个n*m的邻接矩阵\n\tmemset(a,0x3f,sizeof(a));//初始化数组a为正无穷\n\tfor(int i=1;i<=n;i++)\n\ta[i][i]=0;//一个点到它自己的距离为0\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\tint x,y,z;\n\t\tcin>>x>>y>>z;//x到y的距离是z\n\t\ta[x][y]=min(a[x][y],z);//x到y的距离要保证是小的\n\t}\n\tdijkstra();//开始Dijkstra\n\tfor(int i=1;i<=n;i++)//输出结果\n\tcout<<d[i]<<endl;\n\treturn 0;//完美撒花！\n}\n```\n\n时间复杂度\n$$\nO(n^2)\n$$\n下期预告：\n\n高级素数筛或者单源最短路径优化版\n","categories":["学习笔记"]},{"title":"简单素数筛","url":"/2019/07/29/简单素数筛/","content":"\n# 简单素数筛\n\n今天，我看了一些数学知识，而数论是比较主要的，而素数筛是其中重要的一个，现在来介绍一下简单素数筛\n\n首先来，普及一下素数的概念\n\n> 若一个正整数无法被除了1和它本身之外的任何自然数整除，则称该数为素数，也称质数（或素数），否则称该正整数为合数。\n\n通过介绍质数（下文统称质数）的概念，我们知道了一种判断质数的方法\n\n**通过枚举**：这种方法的速度太慢了，因此我们需要更快的方法\n\nps：即使从1枚举到根号n也是很慢的\n\n所以我们要引进一种好方法\n\n就是大于二的所有数的倍数都是合数，那就好办了，我们可以从2开始（先初始化一个v[]数组的所有元素为零）2的倍数都是合数，标记v[i*j]为一,3的倍数都是合数，标记为1……\n\n以此类推，我们就可以得出答案了\n\n下面有一道例题\n\n------\n\n### 题目描述\n\n给定一个范围N，你需要处理M个某数字是否为质数的询问（每个数字均在范围1-N内）\n\n### 输入格式\n\n第一行包含两个正整数N、M，分别表示查询的范围和查询的个数。\n\n接下来M行每行包含一个不小于1且不大于N的整数，即询问该数是否为质数。\n\n### 输出格式\n\n输出包含M行，每行为Yes或No，即依次为每一个询问的结果。\n\n### 输入输出样例\n\n**输入 #1** \n\n```\n100 5\n2\n3\n4\n91\n97\n```\n\n**输出 #1** \n\n```\nYes\nYes\nNo\nNo\nYes\n```\n\n### 说明/提示\n\n时空限制：500ms 128M\n\n数据规模：\n\n对于100%的数据：N<=10000，M<=10000\n\n样例说明：\n\nN=100，说明接下来的询问数均不大于100且不小于1。\n\n所以2、3、97为质数，4、91非质数。\n\n故依次输出Yes、Yes、No、No、Yes。\n\n------\n\n这道题就是一道标准的筛素数的题，因为它要访问多个数啊，不做素数筛不可以啊，所以经过努力，我写出了代码\n\n```c++\n#include <iostream>\n#include <cstring>\nusing namespace std;\nint v[10000005];//记录用\nint n,m;\nint p;\nvoid primes(int n)//简单素数筛函数\n{\n\tmemset(v,0,sizeof(v));\n\tfor(int i=2;i<=n;i++)\n\t{\n\t\tif(v[i])\n\t\tcontinue;\n\t\tfor(int j=i;j<=n/i;j++)\n\t\tv[i*j]=1; \n\t}\n}\nint main()\n{\n\tcin>>n>>m;\n\tprimes(n);//调用函数\n\tfor(int i=1;i<=m;i++)\n\t{\n\t\tcin>>p;\n\t\tif(v[p])\n\t\tcout<<\"No\"<<endl;\n\t\telse \n\t\tcout<<\"Yes\"<<endl;\n\t}\n\treturn 0;\n}\n```\n\n这就是简单素数筛，你get到了吗？\n\n下集预告：高级素数筛（或其他的）\n","categories":["学习笔记"]},{"title":"【解题报告】 POJ1958 奇怪的汉诺塔","url":"/2019/07/28/【解题报告】-POJ1958-奇怪的汉诺塔/","content":"\n# 【解题报告】 POJ1958 奇怪的汉诺塔\n\n> 在这样热浪滚滚的暑假，外面晴空高照，在家里刷刷题不妨是最好的选择\n>\n> ——来自wweiyi语录\n\n题目链接（翻译过的）：\n\nhttps://www.acwing.com/problem/content/98/\n\n题意简述：输出四个塔的汉诺塔分别从1个盘子到12个盘子的最少步数\n\n![河内塔.jpg](https://www.acwing.com/media/article/image/2019/01/10/19_acbb764014-河内塔.jpg)\n\n\n\n我们看了题之后，可以知道这道题跟三个塔的汉诺塔一样，用递归，但是我们设先移走i个盘子到第二个塔或第三个塔，然后就转化成三个塔的问题了。\n\n但是问题在于不知道i等于多少，而且直接输出12个值要用递归也会有些慢，所以我们做一下优化，记忆化一下，我们就可以很快的输出了\n\n设三个塔n个盘子的步数为d[n],设四个塔n个盘子的步数为f[n]\n\n动态转移方程如下\n$$\nf_n=min(f_n,2f_i+d_{n-i})\n$$\n其中\n$$\n1\\leq n,i \\leq 12\n$$\n所以我们就解决了这道题目\n\n代码如下\n\n```c++\n#include <iostream>\n#include <cstring>\nusing namespace std;\nint d[15];\nint f[15];\nint min(int a,int b)//最小值函数\n{\n\treturn a<b? a:b;\n}\nint main()\n{\n\td[1]=1;//汉诺三塔边界\n\tfor(int i=2;i<=12;i++)//计算汉诺三塔的数值\n\td[i]=2*d[i-1]+1;\n\tmemset(f,0x3f,sizeof f);//因为要求最小值，所以初始一个极大值\n\tf[1]=1;//汉诺四塔边界\n\tfor(int i=2;i<=12;i++)//动态规划\n\t{\n\t\tfor(int j=1;j<i;j++)\n\t\t{\n\t\t\tf[i]=min(f[i],2*f[j]+d[i-j]);//状态转移方程\n\t\t}\n\t}\n\tfor(int i=1;i<=12;i++)\n\tcout<<f[i]<<endl;//输出\n\treturn 0;\n}\n```\n","categories":["OI时期解题报告"]},{"title":"学习笔记 简单的amodb A%B Problem","url":"/2019/07/28/学习笔记-简单的amodb-A-B-Problem/","content":"\n# 【基础题目】A%B Problem\n\n#### 题目描述\n\n> C--语言是一种C的简化版，它仅有的三种运算符为++，--和==（没有=，+，-，*，/，%，<,>等任何其他运算符），也没有循环及goto语句，除此之外与C相同。使用C--编写一个函数int\n> mod(int a,int b)，计算a除以b的余数。\n>\n> ——来自wweiyi暑假集训\n\n**题意简述**：只用++,--,==运算符和C++的其他功能来完成a%b的功能\n\n提示：允许调用其他函数\n\n这是一道神奇的题目，我想了大半天才想出来，甚至引来了老师的批评，接下来我说一说我的思路\n\n这道题我们需要只用++,--,==和C++的其他功能来实现模运算，这个看起开似乎无法实现，因为不能够使用循环，因此，这道题的阴影加深了。\n\n#### 递归！循环的后裔！\n\n这里我们不能用循环，也就是我们需要使用类似循环的一个过程，递归来解决问题，递归是一种很常用的解决问题的方法，将一个大问题转换成一个子问题也许模运算就可以这样解决了。\n\n#### ++，--，==三运算符！三符成虎！\n\n我们要解决模运算首先要解决减法运算的问题，因为a%b，就是a一直减b，减到a小于b为止，因此，我们可以先解决减法运算。\n\n代码如下\n\n```c++\nint jian(int a,int b)\n{\n    if(b==0)\n        return a;\n    return jian(--a,--b);\n}\n```\n\n这样就解决了减法运算；\n\n但是顺便说一句，加法和乘法运算实际上也很简单，让我来把代码贴出来！\n\n```c++\nint add(int a,int b)\n{\n    if(b==0)\n        return a;\n    return add(++a,--b);\n}\nint cheng(int a,int b)\n{\n    if(b==0)\n        return a;\n    return add(a,cheng(a,--b));\n}\n```\n\n我们可以轻松地解决减法，乘法和加法，但是模运算我们还是不知道，怎么办呢？\n\n#### 判断！事情的转机？\n\n我们再次理清一下思路，我们需要a一直减b，一直减到a小于b为止，也就是mod(a,b)=mod(a-b,b)，这就是神奇的递归式，就可以解决一部分问题，另一部分问题就是判断a小于b，我们可以a，b同时递减，看谁先到0，但由于这个问题的特殊性，当a=b时，需要特殊判断一下，因为一个数mod它的因数等于0，所以我们的思路就出来了，代码也就写出来了。\n\n代码如下\n\n```c++\n#include <iostream>\nusing namespace std;\nint pd(int a,int b)\n{\n\tif(a==b)\n\treturn 0;\n\tif(a==0)\n\treturn 1;\n\tif(b==0)\n\treturn 0;\n\treturn pd(--a,--b);\n}\nint jian(int a,int b)\n{\n\tif(b==0)\n\treturn a;\n\treturn jian(--a,--b);\n}\nint mod(int a,int b)\n{\n\tif(pd(a,b)==1)\n\treturn a;\n\treturn mod(jian(a,b),b);\n}\nint main()\n{\n\tint a,b;\n\tcin>>a>>b;\n\tcout<<mod(a,b)<<endl;\n\treturn 0;\n}\n//wweiyi费时3小时智造\n```\n","categories":["学习笔记"]},{"title":"最大公约数的故事","url":"/2019/07/26/最大公约数的故事/","content":"\n# 最大公约数的故事\n\n> **最大公因数**，也称最大公约数、最大公因子，指两个或多个整数共有约数中最大的一个。a，b的最大公约数记为（a，b），同样的，a，b，c的最大公约数记为（a，b，c），多个整数的最大公约数也有同样的记号。——来自《百度百科》\n\n[^]: 两个数的最大公约数a，b一般用gcd（a，b）来表示，因此，我后文也会用gcd（ ， ）来表示两个数的最大公约数\n\n最大公约数在计算机中也很常用，而对于这种数了解最深的非欧几里得莫属了！\n\n### 欧几里得\n\n> **欧几里得**（英文：Euclid；希腊文：Ευκλειδης，约公元前330年—公元前275年），古希腊人，数学家，被称为“几何之父”。他最著名的著作《几何原本》是欧洲数学的基础，提出五大公设，欧几里得几何，被广泛的认为是历史上最成功的教科书。欧几里得也写了一些关于透视、圆锥曲线、球面几何学及**数论**的作品。——来自《百度百科》\n\n特别是数论方面有很深的造诣，因此我们今天计算机中关于数论的东西有很可观的一部分来自欧几里得，所以我们在这里看一下最大公约数。\n\n### 如何算最大公约数？\n\n#### 质因数分解法\n\n这是一个好问题，最简单的方法是用质因数分解法\n\n例如：gcd（24,21）\n\n质因数分解得：\n\n24=2 * 2 * 2 * 3\n\n21=3 * 7\n\n它们公共的因数有3，因此，它们的最大公约数是3；\n\n这个办法我们需要用到质因数分解，质因数分解代码如下：\n\n```c++\n#include <iostream>\nusing namespace std;\nbool first=true;\nvoid zyz(int n,int p)\n{\n    if(n>1)\n    {\n        if(n%p==0)\n        {\n            if(first)\n            {\n                cout<<p;\n                first=false;\n            }\n            else cout<<\" \"<<p;\n            zyz(n/p,p);\n        }\n        else zyz(n/p+1);\n    }\n}\nint main()\n{\n    int n;\n    cin>>n;\n    zyz(n,2);\n    cout<<endl;\n    return 0;\n}\n//选自信息学奥赛课课通（C++）\n```\n\n#### 更相减损法\n\n但是这样寻找最大公约数太慢了，怎么办呢？\n\n中国古代《九章算术》中有一种方法，称为“更相减损法”\n\n方法是\n$$\ngcd(a,b)=gcd(b,a-b)\n$$\n，这样一直递归下去，当b等于0是，a‘就是a，b的最大公约数。\n\n但这种方法太慢了，有一种更好的方法，我们在下面介绍。\n\n#### 辗转相除法\n\n辗转相除法是欧几里得算法，是求最大公约数最常见的方法，我们来看一看。\n\n我们仔细观察更相减损法，可以发现，gcd（a,b）=gcd(b,a-b)中右边的那一项是一直减b的因此，我们可以联想到计算机中的mod运算(%)，这样效率就大大地加高了。\n\n递推式\n$$\ngcd(a,b)=gcd(b,a模b)\n$$\n所以我们就可以用递归快速求出最大公约数\n\n代码如下\n\n```c++\nint gcd(int a,int b)\n{\n    return b? gcd(b,a%b):a;\n}\n```\n\n这样就完成了求最大公约数\n\n### 惊现！求逆元！\n\n> 如果：a×b≡1 (mod n)\n>\n> 那么：\n>\n>    a×m≡a×k (mod n)\n> => b×a×m≡b×a×k (mod n)\n> => m≡k (mod n)\n>\n> 前提是b存在！\n>\n> b称为a模n的逆元（a也是b模n的逆元）\n>\n> x模n的逆元也记作\n> $$\n> x^{-1}(mod  (n))\n> $$\n> ——来自wweiyi的暑假集训\n\n这样就可以放心地使用同余中的除法运算了，是不是很帅？？？\n\n因为一个数%n的余数只有可能是0~n-1，又因为0不可能是一个数在模n意义下的逆元，所以我们可以枚举求逆元（From 1 to n-1） 所以，我们可以这样求\n\n代码如下：\n\n```c++\n#include <iostream>\nusing namespace std;\nint main()\n{\n    int a,b,n;\n    cin>>a>>n;//表示求a在模n意义下的逆元\n    for(int i=1;i<=n-1;i++)\n    {\n        if((a*i)%n==1)\n        {\n            b=i;break;\n        }\n    }\n    cout<<b<<endl;\n    return 0;\n}\n```\n\nb就是a在模n意义下的乘法逆元，但是我们看下面一个例子：\n\n> 例如：11模7的逆元\n>\n> \t\t\t(11,7) \t  (a,b)\n> \t\t\t\n> \t\t\t(7,4) \t\t(b,a-b)\n> \t\t\t\n> \t\t\t(4,3)         (a-b,2b-a)\n> \t\t\t\n> \t\t\t(3,1)         (2b-a,**2a-3b**)\n> \t\t\t\n> \t\t\t即：2×11-3×7=1\n> \t\t\t\n> \t\t\t2×11≡1 (mod 7)\n> \t\t\t\n> \t\t\t∴11模7的逆元是2\n>\n> ——来自wweiyi暑假集训\n\n有没有发现，这个就是gcd(11,7)的过程，只不过右边用字母代替了它的运算，但实际上是一样的，而到最后一步时，后边加粗的a的系数就是11模7的逆元！是不是很神奇！\n\n所以我们就有了又快又简便的方法来求逆元\n\n代码如下\n\n```c++\n#include<bits/stdc++.h>\nusing namespace std;\nint b,x,y,mod,gcd; \ninline int exgcd(int a,int b,int &x,int &y)\n{\n    if(b==0)\n    {\n        x=1,y=0;\n        return a;\n    }\n    int ret=exgcd(b,a%b,x,y);\n    int t=x;x=y,y=t-(a/b)*y;\n    return ret;\n}\nint main()\n{\n    cin>>b>>mod;\n    gcd=exgcd(b,mod,x,y);\n    if(gcd!=1)printf(\"not exist\\n\");\n    else printf(\"%d\\n\",(x%mod+mod)%mod);\n    return 0;\n}\n/*来自 WJEMail大佬\n网址:\nhttps://www.cnblogs.com/NSDemail0820/p/9910344.html#_label2\n*/\n```\n\n这样我们求出逆元了\n\n### 神奇！求形如ax+by=c的不定方程整数解和形如ax≡1（mod n）的同余方程\n\n> gcd (25,7)\n>\n> (25,7)     (a,b)\n>\n> (7,4)       (b,a-3b)\n>\n> (4,3)       (a-3b,4b-a)\n>\n> (3,1)       (4b-a,2a-7b)\n>\n> (1,0)       (2a-7b,25b-7a)\n>\n> ——来自wweiyi的暑假培训\n\n#### ax+by=c\n\n也就是使用拓展欧几里得：\n\n> **扩展欧几里德算法**是用来在已知a, b求解一组x，y，使它们满足贝祖等式： \n> $$\n> ax+by\n> = gcd(a, b) =d\n> $$\n> （解一定存在，根据数论中的相关定理）。扩展欧几里德常用在求解模线性方程及方程组中。\n>\n> ——来自《百度百科》\n\n有解的条件：gcd(a,b)|c\n\n因此我们就可以解这样一个方程了!\n\n代码如下\n\n```C++\n#include <iostream>\nusing namespace std;\nint exgcd_x(int c,int d,int x1,int y1,int x2,int y2)\n{\n\treturn d?exgcd_x(d,c%d,x2,y2,x1-(c/d)*x2,y1-(c/d)*y2):x1;\n}\nint exgcd_y(int c,int d,int x1,int y1,int x2,int y2)\n{\n\treturn d?exgcd_y(d,c%d,x2,y2,x1-(c/d)*x2,y1-(c/d)*y2):y1;\n}\nint gcd(int c,int d)\n{\n\treturn d?gcd(d,c%d):c;\n}\nint main()\n{\n\tint a,b,n;\n\tint x1=1,y1=0,x2=0,y2=1;\n\tcin>>a>>b>>n;\n\tif(n%gcd(a,b)!=0)\n\t{\n\t\tcout<<\"No solution\"<<endl;\n\t\treturn 0;\n\t}\n\tint x=exgcd_x(a,b,x1,y1,x2,y2);\n\tint y=exgcd_y(a,b,x1,y1,x2,y2);\n\tint g=n/gcd(a,b);\n\tcout<<x*g<<\"a\"<<y*g<<\"b=\"<<n<<endl;\n\treturn 0;\n}\n```\n\n#### ax≡1(mod n)\n\n实际上就是求逆元！\n\n这里就不再多说了!\n\n**总结：这个欧几里得算法和拓展欧几里得算法是很有用的算法，我们会很常用，因此我们要熟悉掌握这个算法，在OI竞赛中帮助我们，今天就到这里了**\n","categories":["学习笔记"]},{"title":"汉诺塔问题","url":"/2019/07/26/汉诺塔问题/","content":"# 汉诺塔问题（Hanoi Tower Problem）\n\n汉诺塔问题家喻户晓，它源于一个印度的神话，内容如下：\n\n在世界中心贝拿勒斯（在印度北部）的圣庙里，一块黄铜板上插着三根宝石针。印度教的主神梵天在创造世界的时候，在其中一根针上从下到上地穿好了由大到小的64片金片，这就是所谓的汉诺塔。不论白天黑夜，总有一个僧侣在按照下面的法则移动这些金片：一次只移动一片，不管在哪根针上，小片必须在大片上面。僧侣们预言，当所有的金片都从梵天穿好的那根针上移到另外一根针上时，世界就将在一声霹雳中消灭，而梵塔、庙宇和众生也都将同归于尽。\n\n这个问题看起来十分可怕，实际上很简单。接下来就解决一个问题来体会这个小小问题中的大大的递归思想！\n\n>有三根柱子，第一根柱子上有n个从下向上越来越小的圆盘。\n\n>目标：使第一根柱子上的n个圆盘按原样摆放在另一个柱子上。\n\n>注：一次移动一个圆盘，不可以出现大的盘子在小的盘子上面的情况。\n\n解：\n首先我们先玩一下这个游戏。\n\n接着我们找出一个规律，要彻底地把这个塔移开，我们必须要移开上方的n-1个盘子，使最下面的盘子可以移动到另一个柱子上，然后在把上方的n-1个盘子移动到最下\n面的盘子（也就是最大的盘子）上就可以解决了。而n-1个盘子的移动方法也一样，因为最下面的大盘子对上面的n-1个盘子的移动毫无影响。\n\n因此，我们可以得出一个递推式：\n$$\nF_n=2F_{n-1}+1\n$$\n$F_n$代表第一个柱子上有n个盘子的情况，所以这个问题得到了解决\n\n代码如下：时间复杂度$O(2^n )$\n```c++\n#include <iostream>\nusing namespace std;\nint main()\n{\n\tint n;\n\tint f[20];\n\tcin>>n;\n\tf[1]=1;\n\tfor(int i=2;i<=n;i++)\n\tf[i]=2*f[i-1]+1;\n\tcout<<f[n]<<endl;\n\treturn 0; \n}\n```\n\n这个问题就简单地这么解决了。（瞎说，还有更简单的方法）\n\n让我们来看一看输出结果：从n=1开始:1 3 7 15 31 63 127 255 511 1023 2047 4095 8191……\n\n我们发现一个规律，所有的$F_n$满足：\n\n$$\nF_n=2^n-1\n$$\n\n这才是最简单的方法！\n\n有些人会说，这个方法是瞎猜的，只是凭运气而已，没事儿，我可以证明一下。\n\n证：\n数学归纳法。\n当 $n=1$ 时，$F(n)=1,2n-1=1$ ;命题显然成立\n假设 $n=k$ 时命题成立，即$F(k)=2k-1$；\n当 $n=k+1$ 时，$F(k+1)=F(k)*2+1$ ;\n因为$F(k)=2^k-1$\n所以 \n$F(k+1)$\n $=2F(k)+1$\n $=(2^k-1)*2+1$\n $=2^{k+1}-1$\n证毕。\n所以这才是这种汉诺塔问题的最优解！\n\n最终代码如下：时间复杂度O(⁡$log_2n$ )\n```c++\n#include <iostream>\nusing namespace std;\nint qp(int a,int b)\n{\n\tif(b==0)\n\treturn 1;\n\tint t=qp(a,b/2);\n\tif(b%2==0)\n\treturn t*t;\n\telse\n\treturn t*t*a;\n}\nint main()\n{\n\tint n;\n\tcin>>n;\n\tcout<<qp(2,n)-1<<endl;\n\treturn 0;\n}\n```\n","categories":["学习笔记"]}]